{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datatable as dt\n",
    "import timer\n",
    "js_path = '~/jane-street-market-prediction/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 3.7233 seconds\n"
     ]
    }
   ],
   "source": [
    "SEED = 123\n",
    "\n",
    "t = timer.Timer()\n",
    "t.start()\n",
    "import datatable as dt\n",
    "data = dt.fread('~/jane-street-market-prediction/train.csv').to_pandas()\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import score_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_score(resp, weight, action):\n",
    "    score = resp * weight * action \n",
    "    mask =  score < 0 \n",
    "    \n",
    "    return [np.mean(score[mask] ), np.std(score[mask] ), (sum(mask)/len(resp))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.00019810473655664196, 0.1124218895745979]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.u_score(data['resp'],data['weight'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02652322642267673, 0.09900784345563303, 0.5892475646216614]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.pos_score(data['resp'],data['weight'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_score(resp, weight, action):\n",
    "    score = resp * weight * action \n",
    "    mask =  score >= 0 \n",
    "    score[mask] \n",
    "    return [np.mean(score[mask] ), np.std(score[mask] )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02652322642267673, 0.09900784345563303]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_score(data['resp'],data['weight'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.010277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.176875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.013513</td>\n",
       "      <td>2.275059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.013743</td>\n",
       "      <td>4.892174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>-0.000408</td>\n",
       "      <td>0.014206</td>\n",
       "      <td>-1.573865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.007903</td>\n",
       "      <td>0.043108</td>\n",
       "      <td>-37.863545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.008935</td>\n",
       "      <td>0.043416</td>\n",
       "      <td>54.084014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.011139</td>\n",
       "      <td>0.046494</td>\n",
       "      <td>41.303268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.002975</td>\n",
       "      <td>0.046962</td>\n",
       "      <td>-14.754876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-0.012382</td>\n",
       "      <td>0.051774</td>\n",
       "      <td>-48.649796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean       std        sum\n",
       "date                               \n",
       "294   0.000354  0.000703   0.010277\n",
       "2     0.000766  0.003554   0.176875\n",
       "147   0.000652  0.013513   2.275059\n",
       "141   0.001496  0.013743   4.892174\n",
       "419  -0.000408  0.014206  -1.573865\n",
       "...        ...       ...        ...\n",
       "110  -0.007903  0.043108 -37.863545\n",
       "19    0.008935  0.043416  54.084014\n",
       "39    0.011139  0.046494  41.303268\n",
       "97   -0.002975  0.046962 -14.754876\n",
       "73   -0.012382  0.051774 -48.649796\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile = pd.pivot_table(data, index='date', values='resp',\n",
    "           aggfunc={'resp':[np.sum, np.std, np.mean]},fill_value=0)\n",
    "profile.sort_values(by='std')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_bins = pd.qcut(profile['std'], 4, labels=False)\n",
    "data['vol_bin'] = 0\n",
    "for vb in np.unique(vol_bins):\n",
    "    mask = vol_bins == vb\n",
    "    date_mask = data['date'].isin(vol_bins.loc[mask].index)\n",
    "    data.loc[date_mask,'vol_bin'] = vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['resp_bin'] = pd.qcut(data['resp'], 10, labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          7\n",
       "1          2\n",
       "2          9\n",
       "3          3\n",
       "4          3\n",
       "          ..\n",
       "2390486    8\n",
       "2390487    3\n",
       "2390488    8\n",
       "2390489    4\n",
       "2390490    4\n",
       "Name: resp_bin, Length: 2390491, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['resp_bin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(data.mean(),inplace=True)\n",
    "\n",
    "data['short'] = ((data['resp'].values) < 0).astype(int)\n",
    "data['action'] = ((data['resp'].values) >= 0).astype(int)\n",
    "\n",
    "\n",
    "features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "f_mean = np.mean(data[features[1:]].values,axis=0)\n",
    "\n",
    "#features.append('weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weight',\n",
       " 'feature_0',\n",
       " 'feature_1',\n",
       " 'feature_2',\n",
       " 'feature_3',\n",
       " 'feature_4',\n",
       " 'feature_5',\n",
       " 'feature_6',\n",
       " 'feature_7',\n",
       " 'feature_8',\n",
       " 'feature_9',\n",
       " 'feature_10',\n",
       " 'feature_11',\n",
       " 'feature_12',\n",
       " 'feature_13',\n",
       " 'feature_14',\n",
       " 'feature_15',\n",
       " 'feature_16',\n",
       " 'feature_17',\n",
       " 'feature_18',\n",
       " 'feature_19',\n",
       " 'feature_20',\n",
       " 'feature_21',\n",
       " 'feature_22',\n",
       " 'feature_23',\n",
       " 'feature_24',\n",
       " 'feature_25',\n",
       " 'feature_26',\n",
       " 'feature_27',\n",
       " 'feature_28',\n",
       " 'feature_29',\n",
       " 'feature_30',\n",
       " 'feature_31',\n",
       " 'feature_32',\n",
       " 'feature_33',\n",
       " 'feature_34',\n",
       " 'feature_35',\n",
       " 'feature_36',\n",
       " 'feature_37',\n",
       " 'feature_38',\n",
       " 'feature_39',\n",
       " 'feature_40',\n",
       " 'feature_41',\n",
       " 'feature_42',\n",
       " 'feature_43',\n",
       " 'feature_44',\n",
       " 'feature_45',\n",
       " 'feature_46',\n",
       " 'feature_47',\n",
       " 'feature_48',\n",
       " 'feature_49',\n",
       " 'feature_50',\n",
       " 'feature_51',\n",
       " 'feature_52',\n",
       " 'feature_53',\n",
       " 'feature_54',\n",
       " 'feature_55',\n",
       " 'feature_56',\n",
       " 'feature_57',\n",
       " 'feature_58',\n",
       " 'feature_59',\n",
       " 'feature_60',\n",
       " 'feature_61',\n",
       " 'feature_62',\n",
       " 'feature_63',\n",
       " 'feature_64',\n",
       " 'feature_65',\n",
       " 'feature_66',\n",
       " 'feature_67',\n",
       " 'feature_68',\n",
       " 'feature_69',\n",
       " 'feature_70',\n",
       " 'feature_71',\n",
       " 'feature_72',\n",
       " 'feature_73',\n",
       " 'feature_74',\n",
       " 'feature_75',\n",
       " 'feature_76',\n",
       " 'feature_77',\n",
       " 'feature_78',\n",
       " 'feature_79',\n",
       " 'feature_80',\n",
       " 'feature_81',\n",
       " 'feature_82',\n",
       " 'feature_83',\n",
       " 'feature_84',\n",
       " 'feature_85',\n",
       " 'feature_86',\n",
       " 'feature_87',\n",
       " 'feature_88',\n",
       " 'feature_89',\n",
       " 'feature_90',\n",
       " 'feature_91',\n",
       " 'feature_92',\n",
       " 'feature_93',\n",
       " 'feature_94',\n",
       " 'feature_95',\n",
       " 'feature_96',\n",
       " 'feature_97',\n",
       " 'feature_98',\n",
       " 'feature_99',\n",
       " 'feature_100',\n",
       " 'feature_101',\n",
       " 'feature_102',\n",
       " 'feature_103',\n",
       " 'feature_104',\n",
       " 'feature_105',\n",
       " 'feature_106',\n",
       " 'feature_107',\n",
       " 'feature_108',\n",
       " 'feature_109',\n",
       " 'feature_110',\n",
       " 'feature_111',\n",
       " 'feature_112',\n",
       " 'feature_113',\n",
       " 'feature_114',\n",
       " 'feature_115',\n",
       " 'feature_116',\n",
       " 'feature_117',\n",
       " 'feature_118',\n",
       " 'feature_119',\n",
       " 'feature_120',\n",
       " 'feature_121',\n",
       " 'feature_122',\n",
       " 'feature_123',\n",
       " 'feature_124',\n",
       " 'feature_125',\n",
       " 'feature_126',\n",
       " 'feature_127',\n",
       " 'feature_128',\n",
       " 'feature_129']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###kfold by date\n",
    "unique_dates = np.unique(data['date'] )\n",
    "np.random.shuffle(unique_dates)\n",
    "bags = 3\n",
    "folds = 3\n",
    "idx = np.linspace(start = 0, stop = len(unique_dates)-1, num = folds + 1, dtype = int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###i = fold number \n",
    "i = 0\n",
    "mask = data['date'].isin(unique_dates[idx[i]:idx[i+1]] )\n",
    "features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "all_features = ['weight']+[c for c in data.columns if \"resp\" in c]+ [c for c in data.columns if \"feature\" in c]\n",
    "target = 'resp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weight',\n",
       " 'feature_0',\n",
       " 'feature_1',\n",
       " 'feature_2',\n",
       " 'feature_3',\n",
       " 'feature_4',\n",
       " 'feature_5',\n",
       " 'feature_6',\n",
       " 'feature_7',\n",
       " 'feature_8',\n",
       " 'feature_9',\n",
       " 'feature_10',\n",
       " 'feature_11',\n",
       " 'feature_12',\n",
       " 'feature_13',\n",
       " 'feature_14',\n",
       " 'feature_15',\n",
       " 'feature_16',\n",
       " 'feature_17',\n",
       " 'feature_18',\n",
       " 'feature_19',\n",
       " 'feature_20',\n",
       " 'feature_21',\n",
       " 'feature_22',\n",
       " 'feature_23',\n",
       " 'feature_24',\n",
       " 'feature_25',\n",
       " 'feature_26',\n",
       " 'feature_27',\n",
       " 'feature_28',\n",
       " 'feature_29',\n",
       " 'feature_30',\n",
       " 'feature_31',\n",
       " 'feature_32',\n",
       " 'feature_33',\n",
       " 'feature_34',\n",
       " 'feature_35',\n",
       " 'feature_36',\n",
       " 'feature_37',\n",
       " 'feature_38',\n",
       " 'feature_39',\n",
       " 'feature_40',\n",
       " 'feature_41',\n",
       " 'feature_42',\n",
       " 'feature_43',\n",
       " 'feature_44',\n",
       " 'feature_45',\n",
       " 'feature_46',\n",
       " 'feature_47',\n",
       " 'feature_48',\n",
       " 'feature_49',\n",
       " 'feature_50',\n",
       " 'feature_51',\n",
       " 'feature_52',\n",
       " 'feature_53',\n",
       " 'feature_54',\n",
       " 'feature_55',\n",
       " 'feature_56',\n",
       " 'feature_57',\n",
       " 'feature_58',\n",
       " 'feature_59',\n",
       " 'feature_60',\n",
       " 'feature_61',\n",
       " 'feature_62',\n",
       " 'feature_63',\n",
       " 'feature_64',\n",
       " 'feature_65',\n",
       " 'feature_66',\n",
       " 'feature_67',\n",
       " 'feature_68',\n",
       " 'feature_69',\n",
       " 'feature_70',\n",
       " 'feature_71',\n",
       " 'feature_72',\n",
       " 'feature_73',\n",
       " 'feature_74',\n",
       " 'feature_75',\n",
       " 'feature_76',\n",
       " 'feature_77',\n",
       " 'feature_78',\n",
       " 'feature_79',\n",
       " 'feature_80',\n",
       " 'feature_81',\n",
       " 'feature_82',\n",
       " 'feature_83',\n",
       " 'feature_84',\n",
       " 'feature_85',\n",
       " 'feature_86',\n",
       " 'feature_87',\n",
       " 'feature_88',\n",
       " 'feature_89',\n",
       " 'feature_90',\n",
       " 'feature_91',\n",
       " 'feature_92',\n",
       " 'feature_93',\n",
       " 'feature_94',\n",
       " 'feature_95',\n",
       " 'feature_96',\n",
       " 'feature_97',\n",
       " 'feature_98',\n",
       " 'feature_99',\n",
       " 'feature_100',\n",
       " 'feature_101',\n",
       " 'feature_102',\n",
       " 'feature_103',\n",
       " 'feature_104',\n",
       " 'feature_105',\n",
       " 'feature_106',\n",
       " 'feature_107',\n",
       " 'feature_108',\n",
       " 'feature_109',\n",
       " 'feature_110',\n",
       " 'feature_111',\n",
       " 'feature_112',\n",
       " 'feature_113',\n",
       " 'feature_114',\n",
       " 'feature_115',\n",
       " 'feature_116',\n",
       " 'feature_117',\n",
       " 'feature_118',\n",
       " 'feature_119',\n",
       " 'feature_120',\n",
       " 'feature_121',\n",
       " 'feature_122',\n",
       " 'feature_123',\n",
       " 'feature_124',\n",
       " 'feature_125',\n",
       " 'feature_126',\n",
       " 'feature_127',\n",
       " 'feature_128',\n",
       " 'feature_129',\n",
       " 'resp_1',\n",
       " 'resp_2',\n",
       " 'resp_3',\n",
       " 'resp_4',\n",
       " 'resp',\n",
       " 'resp_bin']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "mask = data['date'].isin(unique_dates[idx[i]:idx[i+1]] )\n",
    "\n",
    "train = data[~mask]\n",
    "test = data[mask]\n",
    "\n",
    "features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "target = 'resp'\n",
    "X_train = train.loc[:, features]\n",
    "y_train = (train.loc[:, target])\n",
    "X_test = test.loc[:, features]\n",
    "y_test= (test.loc[:, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ret_kfold' from '/home/x99e/gitgood/legendary-robot/kaggle_jane_street/ret_kfold.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ret_kfold\n",
    "import imp\n",
    "imp.reload(ret_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ret_kfold.ret_fold_mask(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, test_weight, test_resp = ret_kfold.kfold_split(data,mask,features,target)\n",
    "y_train_binary = (y_train > 0 ) * 1\n",
    "y_test_binary = (y_test > 0 ) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          0\n",
       "2          1\n",
       "3          0\n",
       "4          0\n",
       "          ..\n",
       "2335939    0\n",
       "2335940    0\n",
       "2335941    1\n",
       "2335942    0\n",
       "2335943    1\n",
       "Name: resp, Length: 469940, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resp_MLP():\n",
    "    hidden_units = 300\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(int(hidden_units/3), activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~/jane-street-market-prediction/models/resp_model.h5'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_path+'models/resp_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 8ms/step - loss: 0.6578\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 8ms/step - loss: 0.1240\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 8ms/step - loss: 0.0324\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 8ms/step - loss: 0.0105\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 8ms/step - loss: 0.0032\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0012\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 8.3761e-04\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.5424e-04\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4184e-04\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.3998e-04\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4244e-04\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 8ms/step - loss: 7.4261e-04\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 8ms/step - loss: 7.4171e-04\n",
      "14686/14686 [==============================] - 25s 2ms/step - loss: 6.5598e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0006559767061844468"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = 'mse'\n",
    "model = resp_MLP()\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "modelcheckpoint  = tf.keras.callbacks.ModelCheckpoint(js_path+'models/resp_model.h5', monitor='loss', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn)\n",
    "model.fit(x_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1, callbacks= [earlystopping])\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14686/14686 [==============================] - 25s 2ms/step - loss: 6.5598e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0006559767061844468"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[194.42339828374926]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(x_test)\n",
    "\n",
    "pred_action = (preds > 0) * 1\n",
    "score_func.u(test_weight, test_resp,np.array(pred_action).reshape(len(y_test),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_MLP():\n",
    "    hidden_units = 300\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(int(hidden_units/3), activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = (y_train > 0 ) * 1\n",
    "y_test_binary = (y_test > 0 ) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5587       0\n",
       "5588       0\n",
       "5589       1\n",
       "5590       0\n",
       "5591       1\n",
       "          ..\n",
       "2390486    1\n",
       "2390487    0\n",
       "2390488    1\n",
       "2390489    0\n",
       "2390490    0\n",
       "Name: resp, Length: 1920551, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 9ms/step - loss: 5.7276 - accuracy: 0.5022\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 5.6894 - accuracy: 0.5003\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 4.6014 - accuracy: 0.4986\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 4.2170 - accuracy: 0.5029\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 4.4137 - accuracy: 0.5009\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.3037 - accuracy: 0.5010\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.7840 - accuracy: 0.5008\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.3438 - accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.3341 - accuracy: 0.5002\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1989 - accuracy: 0.5001\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2180 - accuracy: 0.5009\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2818 - accuracy: 0.5014\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1354 - accuracy: 0.5011\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.0044 - accuracy: 0.5018\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.9443 - accuracy: 0.4988\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.8994 - accuracy: 0.5011\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.8791 - accuracy: 0.4995\n",
      "Epoch 18/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.8442 - accuracy: 0.5008\n",
      "Epoch 19/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.8227 - accuracy: 0.5013\n",
      "Epoch 20/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.8135 - accuracy: 0.5024\n",
      "Epoch 21/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.8062 - accuracy: 0.5009\n",
      "Epoch 22/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.8107 - accuracy: 0.5024\n",
      "Epoch 23/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.8496 - accuracy: 0.5028\n",
      "Epoch 24/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.8441 - accuracy: 0.5008\n",
      "14686/14686 [==============================] - 30s 2ms/step - loss: 0.8801 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8800592422485352, 0.0]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "\n",
    "model = binary_MLP()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['AUC'])\n",
    "model.fit(x_train, y_train_binary, batch_size= 10000, epochs=50 ,verbose  = 1 ,callbacks= [earlystopping])\n",
    "model.evaluate(x_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "pred_train = probability_model(x_train.values)\n",
    "pred_test = probability_model(x_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_bins = [] \n",
    "for n_pred in pred_test:\n",
    "    pred_bins.append(np.argmax(n_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5a034a6bfb4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_test' is not defined"
     ]
    }
   ],
   "source": [
    "pred_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "385/385 [==============================] - 5s 7ms/step - loss: 0.7071 - AUC: 0.5112\n",
      "Epoch 2/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6922 - AUC: 0.5246\n",
      "Epoch 3/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6914 - AUC: 0.5302\n",
      "Epoch 4/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6911 - AUC: 0.5324\n",
      "Epoch 5/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6907 - AUC: 0.5347\n",
      "Epoch 6/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6906 - AUC: 0.5357\n",
      "Epoch 7/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6903 - AUC: 0.5372\n",
      "Epoch 8/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6900 - AUC: 0.5387\n",
      "Epoch 9/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6898 - AUC: 0.5395\n",
      "Epoch 10/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6895 - AUC: 0.5409\n",
      "Epoch 11/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6893 - AUC: 0.5418\n",
      "Epoch 12/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6890 - AUC: 0.5431\n",
      "Epoch 13/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6888 - AUC: 0.5442\n",
      "Epoch 14/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6884 - AUC: 0.5453\n",
      "Epoch 15/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6882 - AUC: 0.5461\n",
      "Epoch 16/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6880 - AUC: 0.5464\n",
      "Epoch 17/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6877 - AUC: 0.5479\n",
      "Epoch 18/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6876 - AUC: 0.5484\n",
      "Epoch 19/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6872 - AUC: 0.5495\n",
      "Epoch 20/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6870 - AUC: 0.5498\n",
      "Epoch 21/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6868 - AUC: 0.5505\n",
      "Epoch 22/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6866 - AUC: 0.5511\n",
      "Epoch 23/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6864 - AUC: 0.5511\n",
      "Epoch 24/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6863 - AUC: 0.5513\n",
      "Epoch 25/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6859 - AUC: 0.5525\n",
      "Epoch 26/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6858 - AUC: 0.5532\n",
      "Epoch 27/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6857 - AUC: 0.5536\n",
      "Epoch 28/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6853 - AUC: 0.5552\n",
      "Epoch 29/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6853 - AUC: 0.5543\n",
      "Epoch 30/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6852 - AUC: 0.5545\n",
      "Epoch 31/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6850 - AUC: 0.5557\n",
      "Epoch 32/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6846 - AUC: 0.5561\n",
      "Epoch 33/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6846 - AUC: 0.5566\n",
      "Epoch 34/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6845 - AUC: 0.5561\n",
      "Epoch 35/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6843 - AUC: 0.5576\n",
      "Epoch 36/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6843 - AUC: 0.5574\n",
      "Epoch 37/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6842 - AUC: 0.5578\n",
      "Epoch 38/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6840 - AUC: 0.5578\n",
      "Epoch 39/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6839 - AUC: 0.5580\n",
      "Epoch 40/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6836 - AUC: 0.5594\n",
      "Epoch 41/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6834 - AUC: 0.5600\n",
      "Epoch 42/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6835 - AUC: 0.5599\n",
      "Epoch 43/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6834 - AUC: 0.5593\n",
      "Epoch 44/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6835 - AUC: 0.5583\n",
      "Epoch 45/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6832 - AUC: 0.5600\n",
      "Epoch 46/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6833 - AUC: 0.5597\n",
      "Epoch 47/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6833 - AUC: 0.5592\n",
      "Epoch 48/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6832 - AUC: 0.5594\n",
      "Epoch 49/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6829 - AUC: 0.5606\n",
      "Epoch 50/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6829 - AUC: 0.5602\n",
      "Epoch 51/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6827 - AUC: 0.5612\n",
      "Epoch 52/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6826 - AUC: 0.5616\n",
      "Epoch 53/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6828 - AUC: 0.5604\n",
      "Epoch 54/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6824 - AUC: 0.5617\n",
      "Epoch 55/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6825 - AUC: 0.5617\n",
      "Epoch 56/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6823 - AUC: 0.5620\n",
      "Epoch 57/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6825 - AUC: 0.5612\n",
      "Epoch 58/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6823 - AUC: 0.5622\n",
      "Epoch 59/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6822 - AUC: 0.5622\n",
      "Epoch 60/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6820 - AUC: 0.5627\n",
      "Epoch 61/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6821 - AUC: 0.5623\n",
      "Epoch 62/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6819 - AUC: 0.5631\n",
      "Epoch 63/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6819 - AUC: 0.5622\n",
      "Epoch 64/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6818 - AUC: 0.5635\n",
      "Epoch 65/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6820 - AUC: 0.5619\n",
      "Epoch 66/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6817 - AUC: 0.5640\n",
      "Epoch 67/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6818 - AUC: 0.5636\n",
      "Epoch 68/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6816 - AUC: 0.5636\n",
      "Epoch 69/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6817 - AUC: 0.5635\n",
      "Epoch 70/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6813 - AUC: 0.5649\n",
      "Epoch 71/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6815 - AUC: 0.5645\n",
      "Epoch 72/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6815 - AUC: 0.5636\n",
      "Epoch 73/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6814 - AUC: 0.5644\n",
      "Epoch 74/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6810 - AUC: 0.5649\n",
      "Epoch 75/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6810 - AUC: 0.5656\n",
      "Epoch 76/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6811 - AUC: 0.5652\n",
      "Epoch 77/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6812 - AUC: 0.5637\n",
      "Epoch 78/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6811 - AUC: 0.5650\n",
      "Epoch 79/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6810 - AUC: 0.5651\n",
      "Epoch 80/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6808 - AUC: 0.5658\n",
      "Epoch 81/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6811 - AUC: 0.5654\n",
      "Epoch 82/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6810 - AUC: 0.5651\n",
      "Epoch 83/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6808 - AUC: 0.5654\n",
      "Epoch 84/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6807 - AUC: 0.5655\n",
      "Epoch 85/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6809 - AUC: 0.5660\n",
      "Epoch 86/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6809 - AUC: 0.5656\n",
      "Epoch 87/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6806 - AUC: 0.5657\n",
      "Epoch 88/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6806 - AUC: 0.5670\n",
      "Epoch 89/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6805 - AUC: 0.5666\n",
      "Epoch 90/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6806 - AUC: 0.5668\n",
      "Epoch 91/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6808 - AUC: 0.5658\n",
      "Epoch 92/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6804 - AUC: 0.5670\n",
      "Epoch 93/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6806 - AUC: 0.5664\n",
      "Epoch 94/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6802 - AUC: 0.5668\n",
      "Epoch 95/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6803 - AUC: 0.5669\n",
      "Epoch 96/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6805 - AUC: 0.5669\n",
      "Epoch 97/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6802 - AUC: 0.5672\n",
      "Epoch 98/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6803 - AUC: 0.5666\n",
      "Epoch 99/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6799 - AUC: 0.5678\n",
      "Epoch 100/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6802 - AUC: 0.5674\n",
      "Epoch 101/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6800 - AUC: 0.5683\n",
      "Epoch 102/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6801 - AUC: 0.5673\n",
      "Epoch 103/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6799 - AUC: 0.5681\n",
      "Epoch 104/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6800 - AUC: 0.5675\n",
      "Epoch 105/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6801 - AUC: 0.5670\n",
      "Epoch 106/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6800 - AUC: 0.5674\n",
      "Epoch 107/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6800 - AUC: 0.5689\n",
      "Epoch 108/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6799 - AUC: 0.5681\n",
      "Epoch 109/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6798 - AUC: 0.5686\n",
      "Epoch 110/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6797 - AUC: 0.5687\n",
      "Epoch 111/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6798 - AUC: 0.5681\n",
      "Epoch 112/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6797 - AUC: 0.5686\n",
      "Epoch 113/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6797 - AUC: 0.5681\n",
      "Epoch 114/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6796 - AUC: 0.5688\n",
      "Epoch 115/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6798 - AUC: 0.5683\n",
      "Epoch 116/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6796 - AUC: 0.5689\n",
      "Epoch 117/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6795 - AUC: 0.5691\n",
      "Epoch 118/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6796 - AUC: 0.5691\n",
      "Epoch 119/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6796 - AUC: 0.5685\n",
      "Epoch 120/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6795 - AUC: 0.5690\n",
      "Epoch 121/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6795 - AUC: 0.5690\n",
      "Epoch 122/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6796 - AUC: 0.5693\n",
      "Epoch 123/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6797 - AUC: 0.5686\n",
      "Epoch 124/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6795 - AUC: 0.5689\n",
      "Epoch 125/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6789 - AUC: 0.5709\n",
      "Epoch 126/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6793 - AUC: 0.5699\n",
      "Epoch 127/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6795 - AUC: 0.5690\n",
      "Epoch 128/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6794 - AUC: 0.5699\n",
      "Epoch 129/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6791 - AUC: 0.5705\n",
      "Epoch 130/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6792 - AUC: 0.5700\n",
      "Epoch 131/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6795 - AUC: 0.5696\n",
      "Epoch 132/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6793 - AUC: 0.5698\n",
      "Epoch 133/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6791 - AUC: 0.5699\n",
      "Epoch 134/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6793 - AUC: 0.5693\n",
      "Epoch 135/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6791 - AUC: 0.5699\n",
      "Epoch 136/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6790 - AUC: 0.5697\n",
      "Epoch 137/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6791 - AUC: 0.5701\n",
      "Epoch 138/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6792 - AUC: 0.5698\n",
      "Epoch 139/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6790 - AUC: 0.5701\n",
      "Epoch 140/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6788 - AUC: 0.5708\n",
      "Epoch 141/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6790 - AUC: 0.5704\n",
      "Epoch 142/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6789 - AUC: 0.5708\n",
      "Epoch 143/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6788 - AUC: 0.5702\n",
      "Epoch 144/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6790 - AUC: 0.5702\n",
      "Epoch 145/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6788 - AUC: 0.5707\n",
      "Epoch 146/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6789 - AUC: 0.5712\n",
      "Epoch 147/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6789 - AUC: 0.5699\n",
      "Epoch 148/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6786 - AUC: 0.5712\n",
      "Epoch 149/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6787 - AUC: 0.5709\n",
      "Epoch 150/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6786 - AUC: 0.5711\n",
      "Epoch 151/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6786 - AUC: 0.5720\n",
      "Epoch 152/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6787 - AUC: 0.5712\n",
      "Epoch 153/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6786 - AUC: 0.5719\n",
      "Epoch 154/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6787 - AUC: 0.5712\n",
      "Epoch 155/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6788 - AUC: 0.5712\n",
      "Epoch 156/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6787 - AUC: 0.5707\n",
      "Epoch 157/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6787 - AUC: 0.5712\n",
      "Epoch 158/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6786 - AUC: 0.5710\n",
      "Epoch 159/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6787 - AUC: 0.5711\n",
      "Epoch 160/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6785 - AUC: 0.5717\n",
      "Epoch 161/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6784 - AUC: 0.5719\n",
      "Epoch 162/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6784 - AUC: 0.5716\n",
      "Epoch 163/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6784 - AUC: 0.5721\n",
      "Epoch 164/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6787 - AUC: 0.5713\n",
      "Epoch 165/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6784 - AUC: 0.5722\n",
      "Epoch 166/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6783 - AUC: 0.5722\n",
      "Epoch 167/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6784 - AUC: 0.5718\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6785 - AUC: 0.5718\n",
      "Epoch 169/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6783 - AUC: 0.5720\n",
      "Epoch 170/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6782 - AUC: 0.5722\n",
      "Epoch 171/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6785 - AUC: 0.5714\n",
      "Epoch 172/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6782 - AUC: 0.5729\n",
      "Epoch 173/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6785 - AUC: 0.5720\n",
      "Epoch 174/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6785 - AUC: 0.5715\n",
      "Epoch 175/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6783 - AUC: 0.5721\n",
      "Epoch 176/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6784 - AUC: 0.5718\n",
      "Epoch 177/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6783 - AUC: 0.5720\n",
      "Epoch 178/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6782 - AUC: 0.5730\n",
      "Epoch 179/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6779 - AUC: 0.5733\n",
      "Epoch 180/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6780 - AUC: 0.5722\n",
      "Epoch 181/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6781 - AUC: 0.5727\n",
      "Epoch 182/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6782 - AUC: 0.5730\n",
      "Epoch 183/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6780 - AUC: 0.5737\n",
      "Epoch 184/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6779 - AUC: 0.5733\n",
      "Epoch 185/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6779 - AUC: 0.5734\n",
      "Epoch 186/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6782 - AUC: 0.5721\n",
      "Epoch 187/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6777 - AUC: 0.5744\n",
      "Epoch 188/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6778 - AUC: 0.5731\n",
      "Epoch 189/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6781 - AUC: 0.5727\n",
      "Epoch 190/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6779 - AUC: 0.5728\n",
      "Epoch 191/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6779 - AUC: 0.5736\n",
      "Epoch 192/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6781 - AUC: 0.5728\n",
      "Epoch 193/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6782 - AUC: 0.5729\n",
      "Epoch 194/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6779 - AUC: 0.5729\n",
      "Epoch 195/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6779 - AUC: 0.5740\n",
      "Epoch 196/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6782 - AUC: 0.5725\n",
      "Epoch 197/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6779 - AUC: 0.5735\n",
      "Epoch 198/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6780 - AUC: 0.5738\n",
      "Epoch 199/200\n",
      "385/385 [==============================] - 3s 7ms/step - loss: 0.6782 - AUC: 0.5718\n",
      "Epoch 200/200\n",
      "385/385 [==============================] - 3s 8ms/step - loss: 0.6778 - AUC: 0.5736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1ae8306040>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_mlp(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns,))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(num_labels)(x)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
    "        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "batch_size = 5000\n",
    "hidden_units = [150, 150, 150]\n",
    "dropout_rates = [0.2, 0.2, 0.2, 0.2]\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "clf = create_mlp(\n",
    "    len(features), 1, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "\n",
    "clf.fit(x_train, y_train_binary, epochs=200, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469940, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469940,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test).reshape(len(y_test),).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5587       9.821427\n",
       "5588       0.838150\n",
       "5589       0.115654\n",
       "5590       0.000000\n",
       "5591       0.000000\n",
       "             ...   \n",
       "2390486    0.000000\n",
       "2390487    0.000000\n",
       "2390488    0.000000\n",
       "2390489    0.283405\n",
       "2390490    0.000000\n",
       "Name: weight, Length: 1920551, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w = data[mask].loc[:,'weight']\n",
    "test_resp = data[mask].loc[:,'resp']\n",
    "test_action = (test_resp > 0) * 1\n",
    "test_size= len(test_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.872746</td>\n",
       "      <td>-2.191242</td>\n",
       "      <td>-0.474163</td>\n",
       "      <td>-0.323046</td>\n",
       "      <td>0.014688</td>\n",
       "      <td>-0.002484</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>1.168391</td>\n",
       "      <td>8.313583</td>\n",
       "      <td>1.782433</td>\n",
       "      <td>14.018213</td>\n",
       "      <td>2.653056</td>\n",
       "      <td>12.600292</td>\n",
       "      <td>2.301488</td>\n",
       "      <td>11.445807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.673515</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.349537</td>\n",
       "      <td>-1.704709</td>\n",
       "      <td>0.068058</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.193794</td>\n",
       "      <td>0.138212</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>-1.178850</td>\n",
       "      <td>1.777472</td>\n",
       "      <td>-0.915458</td>\n",
       "      <td>2.831612</td>\n",
       "      <td>-1.417010</td>\n",
       "      <td>2.297459</td>\n",
       "      <td>-1.304614</td>\n",
       "      <td>1.898684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.812780</td>\n",
       "      <td>-0.256156</td>\n",
       "      <td>0.806463</td>\n",
       "      <td>0.400221</td>\n",
       "      <td>-0.614188</td>\n",
       "      <td>-0.354800</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>6.115747</td>\n",
       "      <td>9.667908</td>\n",
       "      <td>5.542871</td>\n",
       "      <td>11.671595</td>\n",
       "      <td>7.281757</td>\n",
       "      <td>10.060014</td>\n",
       "      <td>6.638248</td>\n",
       "      <td>9.427299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.174378</td>\n",
       "      <td>0.344640</td>\n",
       "      <td>0.066872</td>\n",
       "      <td>0.009357</td>\n",
       "      <td>-1.006373</td>\n",
       "      <td>-0.676458</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>2.838853</td>\n",
       "      <td>0.499251</td>\n",
       "      <td>3.033732</td>\n",
       "      <td>1.513488</td>\n",
       "      <td>4.397532</td>\n",
       "      <td>1.266037</td>\n",
       "      <td>3.856384</td>\n",
       "      <td>1.013469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138531</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>-0.161518</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.195006</td>\n",
       "      <td>-0.143780</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.344850</td>\n",
       "      <td>4.101145</td>\n",
       "      <td>0.614252</td>\n",
       "      <td>6.623456</td>\n",
       "      <td>0.800129</td>\n",
       "      <td>5.233243</td>\n",
       "      <td>0.362636</td>\n",
       "      <td>3.926633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335939</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.953012</td>\n",
       "      <td>-1.825455</td>\n",
       "      <td>1.167282</td>\n",
       "      <td>1.407730</td>\n",
       "      <td>0.604182</td>\n",
       "      <td>0.745740</td>\n",
       "      <td>0.533140</td>\n",
       "      <td>0.860828</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.821456</td>\n",
       "      <td>-2.384486</td>\n",
       "      <td>-1.125181</td>\n",
       "      <td>0.374372</td>\n",
       "      <td>-1.678955</td>\n",
       "      <td>-2.763350</td>\n",
       "      <td>-2.406755</td>\n",
       "      <td>-1.314956</td>\n",
       "      <td>-1.421907</td>\n",
       "      <td>0.109313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335940</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.093580</td>\n",
       "      <td>-0.308872</td>\n",
       "      <td>-1.175020</td>\n",
       "      <td>-1.802869</td>\n",
       "      <td>-0.266547</td>\n",
       "      <td>-0.411329</td>\n",
       "      <td>-0.622232</td>\n",
       "      <td>-0.924464</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.044894</td>\n",
       "      <td>-0.937643</td>\n",
       "      <td>-1.864301</td>\n",
       "      <td>2.394599</td>\n",
       "      <td>-2.370054</td>\n",
       "      <td>-0.312073</td>\n",
       "      <td>-3.317327</td>\n",
       "      <td>1.023902</td>\n",
       "      <td>-2.057109</td>\n",
       "      <td>2.593925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335941</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.140352</td>\n",
       "      <td>-0.681436</td>\n",
       "      <td>-0.155998</td>\n",
       "      <td>-0.173413</td>\n",
       "      <td>1.272839</td>\n",
       "      <td>1.825249</td>\n",
       "      <td>0.102961</td>\n",
       "      <td>0.315304</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.472322</td>\n",
       "      <td>-2.130481</td>\n",
       "      <td>-1.143322</td>\n",
       "      <td>-0.328897</td>\n",
       "      <td>-1.453351</td>\n",
       "      <td>-2.812128</td>\n",
       "      <td>-2.248055</td>\n",
       "      <td>-1.705126</td>\n",
       "      <td>-1.632931</td>\n",
       "      <td>-0.999637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335942</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.911317</td>\n",
       "      <td>0.753140</td>\n",
       "      <td>-1.752795</td>\n",
       "      <td>-1.096010</td>\n",
       "      <td>1.310088</td>\n",
       "      <td>0.788032</td>\n",
       "      <td>-0.760948</td>\n",
       "      <td>-0.698112</td>\n",
       "      <td>...</td>\n",
       "      <td>1.673762</td>\n",
       "      <td>-1.070076</td>\n",
       "      <td>2.878417</td>\n",
       "      <td>0.354457</td>\n",
       "      <td>1.798903</td>\n",
       "      <td>-0.709118</td>\n",
       "      <td>3.122721</td>\n",
       "      <td>-0.039724</td>\n",
       "      <td>3.135445</td>\n",
       "      <td>0.229055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335943</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.832427</td>\n",
       "      <td>-0.034935</td>\n",
       "      <td>0.448466</td>\n",
       "      <td>0.228038</td>\n",
       "      <td>1.056665</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>0.445886</td>\n",
       "      <td>0.172523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.619985</td>\n",
       "      <td>-2.173323</td>\n",
       "      <td>-0.610736</td>\n",
       "      <td>-1.429112</td>\n",
       "      <td>-0.746617</td>\n",
       "      <td>-2.959842</td>\n",
       "      <td>-1.269910</td>\n",
       "      <td>-2.238614</td>\n",
       "      <td>-1.191447</td>\n",
       "      <td>-2.057664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469940 rows Ã— 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            weight  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0         0.000000          1  -1.872746  -2.191242  -0.474163  -0.323046   \n",
       "1        16.673515         -1  -1.349537  -1.704709   0.068058   0.028432   \n",
       "2         0.000000         -1   0.812780  -0.256156   0.806463   0.400221   \n",
       "3         0.000000         -1   1.174378   0.344640   0.066872   0.009357   \n",
       "4         0.138531          1  -3.172026  -3.093182  -0.161518  -0.128149   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "2335939   0.000000          1  -1.953012  -1.825455   1.167282   1.407730   \n",
       "2335940   0.000000         -1  -1.093580  -0.308872  -1.175020  -1.802869   \n",
       "2335941   0.000000         -1  -1.140352  -0.681436  -0.155998  -0.173413   \n",
       "2335942   0.000000         -1   1.911317   0.753140  -1.752795  -1.096010   \n",
       "2335943   0.000000         -1   0.832427  -0.034935   0.448466   0.228038   \n",
       "\n",
       "         feature_5  feature_6  feature_7  feature_8  ...  feature_120  \\\n",
       "0         0.014688  -0.002484   0.051777   0.026828  ...     0.335127   \n",
       "1         0.193794   0.138212   0.051777   0.026828  ...     0.335127   \n",
       "2        -0.614188  -0.354800   0.051777   0.026828  ...     0.335127   \n",
       "3        -1.006373  -0.676458   0.051777   0.026828  ...     0.335127   \n",
       "4        -0.195006  -0.143780   0.051777   0.026828  ...     0.335127   \n",
       "...            ...        ...        ...        ...  ...          ...   \n",
       "2335939   0.604182   0.745740   0.533140   0.860828  ...    -1.821456   \n",
       "2335940  -0.266547  -0.411329  -0.622232  -0.924464  ...    -3.044894   \n",
       "2335941   1.272839   1.825249   0.102961   0.315304  ...    -1.472322   \n",
       "2335942   1.310088   0.788032  -0.760948  -0.698112  ...     1.673762   \n",
       "2335943   1.056665   0.671095   0.445886   0.172523  ...    -0.619985   \n",
       "\n",
       "         feature_121  feature_122  feature_123  feature_124  feature_125  \\\n",
       "0           0.268776     1.168391     8.313583     1.782433    14.018213   \n",
       "1           0.268776    -1.178850     1.777472    -0.915458     2.831612   \n",
       "2           0.268776     6.115747     9.667908     5.542871    11.671595   \n",
       "3           0.268776     2.838853     0.499251     3.033732     1.513488   \n",
       "4           0.268776     0.344850     4.101145     0.614252     6.623456   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "2335939    -2.384486    -1.125181     0.374372    -1.678955    -2.763350   \n",
       "2335940    -0.937643    -1.864301     2.394599    -2.370054    -0.312073   \n",
       "2335941    -2.130481    -1.143322    -0.328897    -1.453351    -2.812128   \n",
       "2335942    -1.070076     2.878417     0.354457     1.798903    -0.709118   \n",
       "2335943    -2.173323    -0.610736    -1.429112    -0.746617    -2.959842   \n",
       "\n",
       "         feature_126  feature_127  feature_128  feature_129  \n",
       "0           2.653056    12.600292     2.301488    11.445807  \n",
       "1          -1.417010     2.297459    -1.304614     1.898684  \n",
       "2           7.281757    10.060014     6.638248     9.427299  \n",
       "3           4.397532     1.266037     3.856384     1.013469  \n",
       "4           0.800129     5.233243     0.362636     3.926633  \n",
       "...              ...          ...          ...          ...  \n",
       "2335939    -2.406755    -1.314956    -1.421907     0.109313  \n",
       "2335940    -3.317327     1.023902    -2.057109     2.593925  \n",
       "2335941    -2.248055    -1.705126    -1.632931    -0.999637  \n",
       "2335942     3.122721    -0.039724     3.135445     0.229055  \n",
       "2335943    -1.269910    -2.238614    -1.191447    -2.057664  \n",
       "\n",
       "[469940 rows x 131 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[194.42339828374926]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pred_action = (preds > 0) * 1\n",
    "score_func.u(x_test['weight'],test_resp,np.array(pred_action).reshape(len(y_test),)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[194.42339828374926]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.u(test_weight, test_resp,np.array(pred_action).reshape(len(y_test),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_threshold(thd):\n",
    "    pred_action = (preds > thd) * 1\n",
    "    score =  score_func.u(test_w,test_resp,np.array(pred_action).reshape(len(test_w),)) \n",
    "    return np.array(score).astype('double') * -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "sresult = optimize.minimize_scalar(pred_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00090106])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sresult.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'score_func' from '/home/x99e/gitgood/legendary-robot/kaggle_jane_street/score_func.py'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(score_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469940,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0.,  0., ...,  0., -0.,  0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.array(test_resp) * np.array(test_w) * np.array(pred_action).reshape(len(y_test),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12979.074287617968]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.u(test_w,test_resp,test_action) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[309.93456452215366]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.u(test_w,test_resp,np.array(pred_action).reshape(len(y_test),)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008834356337043228, 0.042206384155710534, 0.7557011468372646]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.pos_score(test_w,test_resp,np.array(pred_action).reshape(len(y_test),)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-99.07358774971269]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.u(test_w,test_resp,np.random.randint(0,2,size=len(test_resp))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04021211768006526, 0.12885324404099371, 0.2043161805375402]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.neg_score(test_w,test_resp,np.random.randint(0,2,size=len(test_resp))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010026132783883944, 0.06097105191575509, 0.7955002667632561]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.pos_score(test_w,test_resp,np.random.randint(0,2,size=len(test_resp))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,2,size=len(test_resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_MLP2():\n",
    "    hidden_units = 300\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(int(hidden_units/3), activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(10)\n",
    "    ])    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_MLP():\n",
    "    hidden_units = 300\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(int(hidden_units/3), activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 9ms/step - loss: 2.3094 - accuracy: 0.1647\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1406 - accuracy: 0.1870\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1252 - accuracy: 0.1897\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1177 - accuracy: 0.1923\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1123 - accuracy: 0.1937\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1088 - accuracy: 0.1944\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1060 - accuracy: 0.1955\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1021 - accuracy: 0.1965\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1001 - accuracy: 0.1974\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0973 - accuracy: 0.1983\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0951 - accuracy: 0.1994\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0925 - accuracy: 0.2006\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0906 - accuracy: 0.2014\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0889 - accuracy: 0.2025\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0863 - accuracy: 0.2030\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0844 - accuracy: 0.2043\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0827 - accuracy: 0.2052\n",
      "Epoch 18/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0799 - accuracy: 0.2063\n",
      "Epoch 19/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0789 - accuracy: 0.2068\n",
      "Epoch 20/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0766 - accuracy: 0.2076\n",
      "Epoch 21/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0753 - accuracy: 0.2083\n",
      "Epoch 22/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0745 - accuracy: 0.2086\n",
      "Epoch 23/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0715 - accuracy: 0.2097\n",
      "Epoch 24/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0705 - accuracy: 0.2100\n",
      "Epoch 25/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0692 - accuracy: 0.2111\n",
      "Epoch 26/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0673 - accuracy: 0.2120\n",
      "Epoch 27/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0650 - accuracy: 0.2127\n",
      "Epoch 28/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0648 - accuracy: 0.2135\n",
      "Epoch 29/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0630 - accuracy: 0.2134\n",
      "Epoch 30/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0622 - accuracy: 0.2139\n",
      "Epoch 31/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0619 - accuracy: 0.2146\n",
      "Epoch 32/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0597 - accuracy: 0.2149\n",
      "Epoch 33/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0583 - accuracy: 0.2157\n",
      "Epoch 34/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0576 - accuracy: 0.2161\n",
      "Epoch 35/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0578 - accuracy: 0.2170\n",
      "Epoch 36/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0556 - accuracy: 0.2168\n",
      "Epoch 37/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0545 - accuracy: 0.2178\n",
      "Epoch 38/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0534 - accuracy: 0.2184\n",
      "Epoch 39/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0525 - accuracy: 0.2185\n",
      "Epoch 40/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0523 - accuracy: 0.2183\n",
      "Epoch 41/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0511 - accuracy: 0.2187\n",
      "Epoch 42/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0507 - accuracy: 0.2191\n",
      "Epoch 43/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0505 - accuracy: 0.2195\n",
      "Epoch 44/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0498 - accuracy: 0.2193\n",
      "Epoch 45/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0491 - accuracy: 0.2201\n",
      "Epoch 46/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0472 - accuracy: 0.2208\n",
      "Epoch 47/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0461 - accuracy: 0.2208\n",
      "Epoch 48/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0469 - accuracy: 0.2212\n",
      "Epoch 49/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0471 - accuracy: 0.2214\n",
      "Epoch 50/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0453 - accuracy: 0.2220\n",
      "14679/14679 [==============================] - 32s 2ms/step - loss: 2.1491 - accuracy: 0.1843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.149144172668457, 0.18434061110019684]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model = seq_MLP()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1 )\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "189/189 [==============================] - 3s 8ms/step - loss: 23.4657\n",
      "Epoch 2/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 9.4292\n",
      "Epoch 3/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.9414\n",
      "Epoch 4/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.6872\n",
      "Epoch 5/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.5498\n",
      "Epoch 6/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.4191\n",
      "Epoch 7/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.3277\n",
      "Epoch 8/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.2679\n",
      "Epoch 9/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.2077\n",
      "Epoch 10/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.1614\n",
      "Epoch 11/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.1431\n",
      "Epoch 12/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.1040\n",
      "Epoch 13/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.0801\n",
      "Epoch 14/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.0586\n",
      "Epoch 15/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.0352\n",
      "Epoch 16/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.0039\n",
      "Epoch 17/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.9842\n",
      "Epoch 18/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.9579\n",
      "Epoch 19/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.9409\n",
      "Epoch 20/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.9120\n",
      "Epoch 21/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.8861\n",
      "Epoch 22/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.8603\n",
      "Epoch 23/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.8478\n",
      "Epoch 24/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.8287\n",
      "Epoch 25/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.7968\n",
      "Epoch 26/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.7647\n",
      "Epoch 27/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.7497\n",
      "Epoch 28/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.7276\n",
      "Epoch 29/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.7106\n",
      "Epoch 30/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.6949\n",
      "Epoch 31/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.6671\n",
      "Epoch 32/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.6438\n",
      "Epoch 33/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.6229\n",
      "Epoch 34/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.6157\n",
      "Epoch 35/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5973\n",
      "Epoch 36/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5827\n",
      "Epoch 37/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5633\n",
      "Epoch 38/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5486\n",
      "Epoch 39/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5347\n",
      "Epoch 40/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.5219\n",
      "Epoch 41/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5152\n",
      "Epoch 42/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4972\n",
      "Epoch 43/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.4787\n",
      "Epoch 44/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4594\n",
      "Epoch 45/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4645\n",
      "Epoch 46/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4450\n",
      "Epoch 47/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4280\n",
      "Epoch 48/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4332\n",
      "Epoch 49/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4114\n",
      "Epoch 50/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.4053\n",
      "15905/15905 [==============================] - 29s 2ms/step - loss: 8.8867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.88668441772461"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = seq_MLP()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "model.fit(X_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1 )\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14620/14620 [==============================] - 31s 2ms/step - loss: 1.5392 - accuracy: 0.2910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5392255783081055, 0.2909947633743286]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "pred_train = probability_model(X_train.values)\n",
    "pred_test = probability_model(X_test.values)\n",
    "\n",
    "pred_bins = [] \n",
    "for n_pred in pred_test:\n",
    "    pred_bins.append(np.argmax(n_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.295488\n",
      "Precision: 0.323100\n",
      "Recall: 0.297252\n",
      "F1 score: 0.292774\n"
     ]
    }
   ],
   "source": [
    "skl_pred_score(pred_bins,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:40:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:40:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'max_depth': 10,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3}  # the number of classes that exist in this datset\n",
    "num_round = 20  # the number of training iterations\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "#print \"Numpy array precision:\", precision_score(y_test, best_preds, average='macro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45103186727225814"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(best_preds)==y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9448906290315314"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((np.array(best_preds)-y_test)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "predictions = probability_model(X_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bins = [] \n",
    "for preds in predictions:\n",
    "    pred_bins.append(np.argmax(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = (pred_bins - data['vol_bin']) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8112875042434053"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mean() ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbm = xgb.XGBClassifier(max_depth=5, n_estimators=300, learning_rate=0.05).fit(X_train, y_train)\n",
    "#gbm_pred = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [300,130]\n",
    "dropout_rates = [0.2,0.2,0.2]\n",
    "\n",
    "num_labels = len(np.unique(y_train))\n",
    "num_labels\n",
    "i = 0\n",
    "inp = tf.keras.layers.Input(shape=(X_train.shape[1]))\n",
    "x = tf.keras.layers.BatchNormalization()(inp)\n",
    "x = tf.keras.layers.Dropout(0.2)(x) \n",
    "\n",
    "\n",
    "x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
    "x = tf.keras.layers.Dropout(dropout_rates[i])(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(num_labels)(x)\n",
    "\n",
    "\n",
    "out = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "k_model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 131)]             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 131)               524       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 131)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 300)               39600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                3010      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 44,334\n",
      "Trainable params: 43,472\n",
      "Non-trainable params: 862\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "191/191 [==============================] - 2s 6ms/step - loss: 2.1827 - accuracy: 0.1855\n",
      "Epoch 2/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1812 - accuracy: 0.1865\n",
      "Epoch 3/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1797 - accuracy: 0.1869\n",
      "Epoch 4/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1782 - accuracy: 0.1879\n",
      "Epoch 5/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1773 - accuracy: 0.1880\n",
      "Epoch 6/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1770 - accuracy: 0.1887\n",
      "Epoch 7/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1760 - accuracy: 0.1893\n",
      "Epoch 8/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1755 - accuracy: 0.1889\n",
      "Epoch 9/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1748 - accuracy: 0.1900\n",
      "Epoch 10/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1741 - accuracy: 0.1895\n",
      "Epoch 11/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1740 - accuracy: 0.1902\n",
      "Epoch 12/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1727 - accuracy: 0.1909\n",
      "Epoch 13/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1722 - accuracy: 0.1910\n",
      "Epoch 14/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1728 - accuracy: 0.1901\n",
      "Epoch 15/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1721 - accuracy: 0.1907\n",
      "Epoch 16/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1719 - accuracy: 0.1908\n",
      "Epoch 17/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1705 - accuracy: 0.1912\n",
      "Epoch 18/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1706 - accuracy: 0.1916\n",
      "Epoch 19/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1695 - accuracy: 0.1928\n",
      "Epoch 20/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1706 - accuracy: 0.1918\n",
      "Epoch 21/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1703 - accuracy: 0.1918\n",
      "Epoch 22/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1696 - accuracy: 0.1916\n",
      "Epoch 23/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1693 - accuracy: 0.1921\n",
      "Epoch 24/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1689 - accuracy: 0.1926\n",
      "Epoch 25/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1682 - accuracy: 0.1920\n",
      "Epoch 26/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1686 - accuracy: 0.1928\n",
      "Epoch 27/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1681 - accuracy: 0.1922\n",
      "Epoch 28/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1678 - accuracy: 0.1922\n",
      "Epoch 29/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1671 - accuracy: 0.1925\n",
      "Epoch 30/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1669 - accuracy: 0.1933\n",
      "Epoch 31/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1663 - accuracy: 0.1935\n",
      "Epoch 32/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1673 - accuracy: 0.1928\n",
      "Epoch 33/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1661 - accuracy: 0.1931\n",
      "Epoch 34/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1666 - accuracy: 0.1937\n",
      "Epoch 35/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1663 - accuracy: 0.1939\n",
      "Epoch 36/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1652 - accuracy: 0.1938\n",
      "Epoch 37/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1661 - accuracy: 0.1935\n",
      "Epoch 38/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1644 - accuracy: 0.1941\n",
      "Epoch 39/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1648 - accuracy: 0.1944\n",
      "Epoch 40/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1653 - accuracy: 0.1935\n",
      "Epoch 41/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1645 - accuracy: 0.1942\n",
      "Epoch 42/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1641 - accuracy: 0.1948\n",
      "Epoch 43/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1638 - accuracy: 0.1948\n",
      "Epoch 44/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1646 - accuracy: 0.1946\n",
      "Epoch 45/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1647 - accuracy: 0.1944\n",
      "Epoch 46/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1633 - accuracy: 0.1945\n",
      "Epoch 47/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1644 - accuracy: 0.1940\n",
      "Epoch 48/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1639 - accuracy: 0.1949\n",
      "Epoch 49/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1638 - accuracy: 0.1937\n",
      "Epoch 50/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1635 - accuracy: 0.1948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f27600a0550>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_model.summary()\n",
    "\n",
    "k_model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "k_model.fit(X_train, y_train, batch_size= 10000, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_bin_classifier(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x= tf.keras.layers.Dropout(dropout_rates[0])(x) \n",
    "    #x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dense(hidden_units[0])(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x) #(x)\n",
    "    \n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "        x = tf.keras.layers.Concatenate(axis=1)([x, inp])\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(num_labels)(x)\n",
    "    \n",
    "    out = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        #optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vb_simple(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns))\n",
    "    \n",
    "    for _ in range(3):\n",
    "        x = tf.keras.layers.Dense(hidden_units[0], activation='relu')(inp)\n",
    "        #x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "\n",
    "\n",
    "\n",
    "    x= tf.keras.layers.Dense(num_labels)(x)\n",
    "    \n",
    "    out = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        #optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "num_cats = len(np.unique(y_train))\n",
    "batch_size = 5000\n",
    "hidden_units = [300, 300, 300]\n",
    "dropout_rates = [0.2, 0.2, 0.2, 0.2]\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.5179 - accuracy: 0.1255\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.2758 - accuracy: 0.1520\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.2349 - accuracy: 0.1631\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.2173 - accuracy: 0.1696\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.2069 - accuracy: 0.1742\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1981 - accuracy: 0.1784\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1913 - accuracy: 0.1819\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1857 - accuracy: 0.1851\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1800 - accuracy: 0.1879\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1747 - accuracy: 0.1907\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1710 - accuracy: 0.1927\n",
      "Epoch 12/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1668 - accuracy: 0.1946\n",
      "Epoch 13/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1631 - accuracy: 0.1966\n",
      "Epoch 14/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1578 - accuracy: 0.1987\n",
      "Epoch 15/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1549 - accuracy: 0.2003\n",
      "Epoch 16/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1518 - accuracy: 0.2020\n",
      "Epoch 17/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1493 - accuracy: 0.2027\n",
      "Epoch 18/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1467 - accuracy: 0.2039\n",
      "Epoch 19/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1444 - accuracy: 0.2056\n",
      "Epoch 20/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1418 - accuracy: 0.2071\n",
      "Epoch 21/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1396 - accuracy: 0.2075\n",
      "Epoch 22/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1366 - accuracy: 0.2091\n",
      "Epoch 23/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1353 - accuracy: 0.2098\n",
      "Epoch 24/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1326 - accuracy: 0.2108\n",
      "Epoch 25/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1310 - accuracy: 0.2109\n",
      "Epoch 26/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1295 - accuracy: 0.2121\n",
      "Epoch 27/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1279 - accuracy: 0.2131\n",
      "Epoch 28/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1260 - accuracy: 0.2136\n",
      "Epoch 29/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1252 - accuracy: 0.2142\n",
      "Epoch 30/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1232 - accuracy: 0.2149\n",
      "Epoch 31/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1227 - accuracy: 0.2152\n",
      "Epoch 32/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1213 - accuracy: 0.2160\n",
      "Epoch 33/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1186 - accuracy: 0.2173\n",
      "Epoch 34/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1180 - accuracy: 0.2176\n",
      "Epoch 35/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1177 - accuracy: 0.2179\n",
      "Epoch 36/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1147 - accuracy: 0.2191\n",
      "Epoch 37/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1150 - accuracy: 0.2194\n",
      "Epoch 38/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1144 - accuracy: 0.2192\n",
      "Epoch 39/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1133 - accuracy: 0.2196\n",
      "Epoch 40/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1120 - accuracy: 0.2200\n",
      "Epoch 41/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1109 - accuracy: 0.2203\n",
      "Epoch 42/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1107 - accuracy: 0.2209\n",
      "Epoch 43/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1087 - accuracy: 0.2212\n",
      "Epoch 44/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1087 - accuracy: 0.2219\n",
      "Epoch 45/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1080 - accuracy: 0.2226\n",
      "Epoch 46/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1074 - accuracy: 0.2222\n",
      "Epoch 47/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1062 - accuracy: 0.2228\n",
      "Epoch 48/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1055 - accuracy: 0.2233\n",
      "Epoch 49/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1043 - accuracy: 0.2232\n",
      "Epoch 50/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1044 - accuracy: 0.2236\n",
      "Epoch 51/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1033 - accuracy: 0.2245\n",
      "Epoch 52/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1028 - accuracy: 0.2247\n",
      "Epoch 53/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1024 - accuracy: 0.2245\n",
      "Epoch 54/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1014 - accuracy: 0.2255\n",
      "Epoch 55/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1004 - accuracy: 0.2256\n",
      "Epoch 56/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0992 - accuracy: 0.2258\n",
      "Epoch 57/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1001 - accuracy: 0.2256\n",
      "Epoch 58/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0991 - accuracy: 0.2260\n",
      "Epoch 59/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0980 - accuracy: 0.2270\n",
      "Epoch 60/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0982 - accuracy: 0.2269\n",
      "Epoch 61/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0974 - accuracy: 0.2269\n",
      "Epoch 62/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0970 - accuracy: 0.2276\n",
      "Epoch 63/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0961 - accuracy: 0.2272\n",
      "Epoch 64/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0958 - accuracy: 0.2281\n",
      "Epoch 65/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0952 - accuracy: 0.2284\n",
      "Epoch 66/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0950 - accuracy: 0.2284\n",
      "Epoch 67/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0945 - accuracy: 0.2285\n",
      "Epoch 68/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0931 - accuracy: 0.2291\n",
      "Epoch 69/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0936 - accuracy: 0.2283\n",
      "Epoch 70/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0932 - accuracy: 0.2295\n",
      "Epoch 71/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0927 - accuracy: 0.2295\n",
      "Epoch 72/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0924 - accuracy: 0.2295\n",
      "Epoch 73/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0912 - accuracy: 0.2299\n",
      "Epoch 74/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0918 - accuracy: 0.2298\n",
      "Epoch 75/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0905 - accuracy: 0.2304\n",
      "Epoch 76/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0910 - accuracy: 0.2306\n",
      "Epoch 77/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0902 - accuracy: 0.2307\n",
      "Epoch 78/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0891 - accuracy: 0.2310\n",
      "Epoch 79/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0894 - accuracy: 0.2311\n",
      "Epoch 80/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0899 - accuracy: 0.2310\n",
      "Epoch 81/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0896 - accuracy: 0.2304\n",
      "Epoch 82/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0879 - accuracy: 0.2315\n",
      "Epoch 83/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0883 - accuracy: 0.2311\n",
      "Epoch 84/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0872 - accuracy: 0.2315\n",
      "Epoch 85/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0872 - accuracy: 0.2314\n",
      "Epoch 86/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0879 - accuracy: 0.2312\n",
      "Epoch 87/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0863 - accuracy: 0.2318\n",
      "Epoch 88/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0859 - accuracy: 0.2325\n",
      "Epoch 89/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0854 - accuracy: 0.2328\n",
      "Epoch 90/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0854 - accuracy: 0.2328\n",
      "Epoch 91/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0849 - accuracy: 0.2325\n",
      "Epoch 92/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0850 - accuracy: 0.2327\n",
      "Epoch 93/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0846 - accuracy: 0.2327\n",
      "Epoch 94/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0844 - accuracy: 0.2326\n",
      "Epoch 95/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0835 - accuracy: 0.2333\n",
      "Epoch 96/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0832 - accuracy: 0.2335\n",
      "Epoch 97/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0831 - accuracy: 0.2334\n",
      "Epoch 98/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0832 - accuracy: 0.2335\n",
      "Epoch 99/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0824 - accuracy: 0.2337\n",
      "Epoch 100/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0820 - accuracy: 0.2341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f273c4f0370>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel = vb_simple(\n",
    "    inp_shape,num_cats, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "fmodel.fit(X_train, y_train, epochs=100, batch_size=20000)\n",
    "fmodel.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15261/15261 [==============================] - 27s 2ms/step - loss: 2.3678 - accuracy: 0.1339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.367835283279419, 0.13387653231620789]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "96/96 [==============================] - 5s 30ms/step - loss: 2.5185 - accuracy: 0.1269\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.2631 - accuracy: 0.1497\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.2391 - accuracy: 0.1569\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.2205 - accuracy: 0.1645\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.2012 - accuracy: 0.1742\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.1814 - accuracy: 0.1838\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.1649 - accuracy: 0.1916\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.1492 - accuracy: 0.1990\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.1348 - accuracy: 0.2050\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.1222 - accuracy: 0.2109\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.1105 - accuracy: 0.2160\n",
      "Epoch 12/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.0989 - accuracy: 0.2206\n",
      "Epoch 13/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0894 - accuracy: 0.2252\n",
      "Epoch 14/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.0786 - accuracy: 0.2298\n",
      "Epoch 15/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.0699 - accuracy: 0.2331\n",
      "Epoch 16/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.0608 - accuracy: 0.2371\n",
      "Epoch 17/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0526 - accuracy: 0.2406\n",
      "Epoch 18/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.0441 - accuracy: 0.2445\n",
      "Epoch 19/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0382 - accuracy: 0.2466\n",
      "Epoch 20/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0325 - accuracy: 0.2494\n",
      "Epoch 21/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0257 - accuracy: 0.2520\n",
      "Epoch 22/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.0216 - accuracy: 0.2537\n",
      "Epoch 23/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.0162 - accuracy: 0.2563\n",
      "Epoch 24/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0109 - accuracy: 0.2587\n",
      "Epoch 25/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.0070 - accuracy: 0.2599\n",
      "Epoch 26/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.0036 - accuracy: 0.2615\n",
      "Epoch 27/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.0003 - accuracy: 0.2629\n",
      "Epoch 28/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9960 - accuracy: 0.2646\n",
      "Epoch 29/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9921 - accuracy: 0.2658\n",
      "Epoch 30/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9899 - accuracy: 0.2667\n",
      "Epoch 31/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9870 - accuracy: 0.2681\n",
      "Epoch 32/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9862 - accuracy: 0.2687\n",
      "Epoch 33/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9824 - accuracy: 0.2697\n",
      "Epoch 34/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9798 - accuracy: 0.2714\n",
      "Epoch 35/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9781 - accuracy: 0.2722\n",
      "Epoch 36/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9765 - accuracy: 0.2730\n",
      "Epoch 37/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9742 - accuracy: 0.2735\n",
      "Epoch 38/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9714 - accuracy: 0.2747\n",
      "Epoch 39/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9694 - accuracy: 0.2752\n",
      "Epoch 40/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9673 - accuracy: 0.2767\n",
      "Epoch 41/100\n",
      "96/96 [==============================] - 3s 32ms/step - loss: 1.9672 - accuracy: 0.2764\n",
      "Epoch 42/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9634 - accuracy: 0.2785\n",
      "Epoch 43/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9641 - accuracy: 0.2780\n",
      "Epoch 44/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9628 - accuracy: 0.2781\n",
      "Epoch 45/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9609 - accuracy: 0.2792\n",
      "Epoch 46/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9604 - accuracy: 0.2799\n",
      "Epoch 47/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9592 - accuracy: 0.2799\n",
      "Epoch 48/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9571 - accuracy: 0.2810\n",
      "Epoch 49/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9553 - accuracy: 0.2810\n",
      "Epoch 50/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9542 - accuracy: 0.2823\n",
      "Epoch 51/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9542 - accuracy: 0.2819\n",
      "Epoch 52/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9522 - accuracy: 0.2835\n",
      "Epoch 53/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9502 - accuracy: 0.2839\n",
      "Epoch 54/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9492 - accuracy: 0.2846\n",
      "Epoch 55/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9486 - accuracy: 0.2845\n",
      "Epoch 56/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9481 - accuracy: 0.2845\n",
      "Epoch 57/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9473 - accuracy: 0.2846\n",
      "Epoch 58/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9461 - accuracy: 0.2857\n",
      "Epoch 59/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9451 - accuracy: 0.2862\n",
      "Epoch 60/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9423 - accuracy: 0.2868\n",
      "Epoch 61/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9422 - accuracy: 0.2869\n",
      "Epoch 62/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9433 - accuracy: 0.2862\n",
      "Epoch 63/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9417 - accuracy: 0.2873\n",
      "Epoch 64/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9404 - accuracy: 0.2878\n",
      "Epoch 65/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9409 - accuracy: 0.2874\n",
      "Epoch 66/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9391 - accuracy: 0.2882\n",
      "Epoch 67/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9379 - accuracy: 0.2887\n",
      "Epoch 68/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9371 - accuracy: 0.2887\n",
      "Epoch 69/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9368 - accuracy: 0.2895\n",
      "Epoch 70/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9371 - accuracy: 0.2893\n",
      "Epoch 71/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9364 - accuracy: 0.2893\n",
      "Epoch 72/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9343 - accuracy: 0.2905\n",
      "Epoch 73/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9343 - accuracy: 0.2904\n",
      "Epoch 74/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9338 - accuracy: 0.2901\n",
      "Epoch 75/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9327 - accuracy: 0.2912\n",
      "Epoch 76/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9332 - accuracy: 0.2904\n",
      "Epoch 77/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9324 - accuracy: 0.2916\n",
      "Epoch 78/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9306 - accuracy: 0.2923\n",
      "Epoch 79/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9291 - accuracy: 0.2930\n",
      "Epoch 80/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9300 - accuracy: 0.2914\n",
      "Epoch 81/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9289 - accuracy: 0.2923\n",
      "Epoch 82/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9275 - accuracy: 0.2928\n",
      "Epoch 83/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9277 - accuracy: 0.2928\n",
      "Epoch 84/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9277 - accuracy: 0.2929\n",
      "Epoch 85/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9279 - accuracy: 0.2930\n",
      "Epoch 86/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9265 - accuracy: 0.2934\n",
      "Epoch 87/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9253 - accuracy: 0.2939\n",
      "Epoch 88/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9260 - accuracy: 0.2932\n",
      "Epoch 89/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9252 - accuracy: 0.2938\n",
      "Epoch 90/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9254 - accuracy: 0.2937\n",
      "Epoch 91/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9242 - accuracy: 0.2946\n",
      "Epoch 92/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9224 - accuracy: 0.2952\n",
      "Epoch 93/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9235 - accuracy: 0.2937\n",
      "Epoch 94/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9222 - accuracy: 0.2949\n",
      "Epoch 95/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9231 - accuracy: 0.2950\n",
      "Epoch 96/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9207 - accuracy: 0.2962\n",
      "Epoch 97/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9217 - accuracy: 0.2957\n",
      "Epoch 98/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9206 - accuracy: 0.2960\n",
      "Epoch 99/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9196 - accuracy: 0.2961\n",
      "Epoch 100/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9201 - accuracy: 0.2961\n",
      "15261/15261 [==============================] - 41s 3ms/step - loss: 2.5122 - accuracy: 0.1327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5122361183166504, 0.1327277421951294]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "fmodel = vol_bin_classifier(\n",
    "    inp_shape,num_cats, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "fmodel.fit(X_train, y_train, epochs=100, batch_size=20000)\n",
    "fmodel.evaluate(X_test, y_test)\n",
    "#fmodel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1491 - accuracy: 0.1953\n",
      "Epoch 2/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1471 - accuracy: 0.1965\n",
      "Epoch 3/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1447 - accuracy: 0.1977\n",
      "Epoch 4/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1430 - accuracy: 0.1985\n",
      "Epoch 5/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1413 - accuracy: 0.1993\n",
      "Epoch 6/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1392 - accuracy: 0.2001\n",
      "Epoch 7/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1381 - accuracy: 0.2007\n",
      "Epoch 8/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1361 - accuracy: 0.2017\n",
      "Epoch 9/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1347 - accuracy: 0.2025\n",
      "Epoch 10/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1334 - accuracy: 0.2029\n",
      "Epoch 11/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1317 - accuracy: 0.2038\n",
      "Epoch 12/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1310 - accuracy: 0.2040\n",
      "Epoch 13/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1299 - accuracy: 0.2047\n",
      "Epoch 14/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1278 - accuracy: 0.2057\n",
      "Epoch 15/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1270 - accuracy: 0.2058\n",
      "Epoch 16/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1264 - accuracy: 0.2062\n",
      "Epoch 17/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1255 - accuracy: 0.2064\n",
      "Epoch 18/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1248 - accuracy: 0.2070\n",
      "Epoch 19/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1235 - accuracy: 0.2076\n",
      "Epoch 20/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1226 - accuracy: 0.2081\n",
      "Epoch 21/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1217 - accuracy: 0.2084\n",
      "Epoch 22/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1215 - accuracy: 0.2087\n",
      "Epoch 23/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1205 - accuracy: 0.2088\n",
      "Epoch 24/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1197 - accuracy: 0.2091\n",
      "Epoch 25/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1189 - accuracy: 0.2096\n",
      "Epoch 26/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1180 - accuracy: 0.2099\n",
      "Epoch 27/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1174 - accuracy: 0.2107\n",
      "Epoch 28/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1167 - accuracy: 0.2106\n",
      "Epoch 29/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1164 - accuracy: 0.2109\n",
      "Epoch 30/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1149 - accuracy: 0.2115\n",
      "Epoch 31/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1150 - accuracy: 0.2113\n",
      "Epoch 32/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1141 - accuracy: 0.2120\n",
      "Epoch 33/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1136 - accuracy: 0.2121\n",
      "Epoch 34/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1133 - accuracy: 0.2121\n",
      "Epoch 35/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1127 - accuracy: 0.2122\n",
      "Epoch 36/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1121 - accuracy: 0.2127\n",
      "Epoch 37/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1115 - accuracy: 0.2127\n",
      "Epoch 38/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1111 - accuracy: 0.2132\n",
      "Epoch 39/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1103 - accuracy: 0.2135\n",
      "Epoch 40/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1097 - accuracy: 0.2137\n",
      "Epoch 41/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1099 - accuracy: 0.2136\n",
      "Epoch 42/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1088 - accuracy: 0.2142\n",
      "Epoch 43/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1094 - accuracy: 0.2138\n",
      "Epoch 44/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1084 - accuracy: 0.2144\n",
      "Epoch 45/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1078 - accuracy: 0.2144\n",
      "Epoch 46/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1076 - accuracy: 0.2149\n",
      "Epoch 47/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1077 - accuracy: 0.2146\n",
      "Epoch 48/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1062 - accuracy: 0.2153\n",
      "Epoch 49/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1067 - accuracy: 0.2153\n",
      "Epoch 50/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1061 - accuracy: 0.2152\n",
      "Epoch 51/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1055 - accuracy: 0.2158\n",
      "Epoch 52/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1052 - accuracy: 0.2155\n",
      "Epoch 53/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1050 - accuracy: 0.2160\n",
      "Epoch 54/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.1046 - accuracy: 0.2162\n",
      "Epoch 55/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1042 - accuracy: 0.2161\n",
      "Epoch 56/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1039 - accuracy: 0.2162\n",
      "Epoch 57/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1032 - accuracy: 0.2167\n",
      "Epoch 58/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1029 - accuracy: 0.2164\n",
      "Epoch 59/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1022 - accuracy: 0.2171\n",
      "Epoch 60/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1021 - accuracy: 0.2173\n",
      "Epoch 61/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1022 - accuracy: 0.2175\n",
      "Epoch 62/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1017 - accuracy: 0.2173\n",
      "Epoch 63/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.1012 - accuracy: 0.2174\n",
      "Epoch 64/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1011 - accuracy: 0.2175\n",
      "Epoch 65/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1004 - accuracy: 0.2183\n",
      "Epoch 66/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1005 - accuracy: 0.2178\n",
      "Epoch 67/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1000 - accuracy: 0.2181\n",
      "Epoch 68/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0996 - accuracy: 0.2180\n",
      "Epoch 69/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0994 - accuracy: 0.2182\n",
      "Epoch 70/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0993 - accuracy: 0.2186\n",
      "Epoch 71/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0987 - accuracy: 0.2188\n",
      "Epoch 72/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0991 - accuracy: 0.2186\n",
      "Epoch 73/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0985 - accuracy: 0.2189\n",
      "Epoch 74/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0979 - accuracy: 0.2193\n",
      "Epoch 75/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0979 - accuracy: 0.2189\n",
      "Epoch 76/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0975 - accuracy: 0.2191\n",
      "Epoch 77/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0974 - accuracy: 0.2194\n",
      "Epoch 78/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0972 - accuracy: 0.2194\n",
      "Epoch 79/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0969 - accuracy: 0.2191\n",
      "Epoch 80/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0962 - accuracy: 0.2199\n",
      "Epoch 81/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0960 - accuracy: 0.2200\n",
      "Epoch 82/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0960 - accuracy: 0.2197\n",
      "Epoch 83/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0954 - accuracy: 0.2200\n",
      "Epoch 84/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0948 - accuracy: 0.2204\n",
      "Epoch 85/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0953 - accuracy: 0.2202\n",
      "Epoch 86/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0952 - accuracy: 0.2203\n",
      "Epoch 87/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0950 - accuracy: 0.2206\n",
      "Epoch 88/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0944 - accuracy: 0.2206\n",
      "Epoch 89/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0950 - accuracy: 0.2202\n",
      "Epoch 90/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0939 - accuracy: 0.2210\n",
      "Epoch 91/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0937 - accuracy: 0.2207\n",
      "Epoch 92/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0937 - accuracy: 0.2206\n",
      "Epoch 93/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0938 - accuracy: 0.2209\n",
      "Epoch 94/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0930 - accuracy: 0.2211\n",
      "Epoch 95/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0926 - accuracy: 0.2215\n",
      "Epoch 96/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0925 - accuracy: 0.2215\n",
      "Epoch 97/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0925 - accuracy: 0.2212\n",
      "Epoch 98/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0920 - accuracy: 0.2216\n",
      "Epoch 99/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0919 - accuracy: 0.2216\n",
      "Epoch 100/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0921 - accuracy: 0.2213\n",
      "Epoch 101/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0921 - accuracy: 0.2213\n",
      "Epoch 102/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0909 - accuracy: 0.2218\n",
      "Epoch 103/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0911 - accuracy: 0.2220\n",
      "Epoch 104/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0912 - accuracy: 0.2221\n",
      "Epoch 105/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0909 - accuracy: 0.2224\n",
      "Epoch 106/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0907 - accuracy: 0.2223\n",
      "Epoch 107/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0903 - accuracy: 0.2224\n",
      "Epoch 108/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0899 - accuracy: 0.2225\n",
      "Epoch 109/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0900 - accuracy: 0.2226\n",
      "Epoch 110/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0897 - accuracy: 0.2227\n",
      "Epoch 111/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0893 - accuracy: 0.2226\n",
      "Epoch 112/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0896 - accuracy: 0.2228\n",
      "Epoch 113/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0891 - accuracy: 0.2231\n",
      "Epoch 114/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0883 - accuracy: 0.2233\n",
      "Epoch 115/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0892 - accuracy: 0.2228\n",
      "Epoch 116/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0888 - accuracy: 0.2238\n",
      "Epoch 117/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0884 - accuracy: 0.2233\n",
      "Epoch 118/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0880 - accuracy: 0.2234\n",
      "Epoch 119/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0878 - accuracy: 0.2236\n",
      "Epoch 120/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0879 - accuracy: 0.2231\n",
      "Epoch 121/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0873 - accuracy: 0.2235\n",
      "Epoch 122/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0877 - accuracy: 0.2236\n",
      "Epoch 123/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0876 - accuracy: 0.2238\n",
      "Epoch 124/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0874 - accuracy: 0.2235\n",
      "Epoch 125/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0869 - accuracy: 0.2240\n",
      "Epoch 126/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0868 - accuracy: 0.2239\n",
      "Epoch 127/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0867 - accuracy: 0.2238\n",
      "Epoch 128/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0866 - accuracy: 0.2240\n",
      "Epoch 129/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0867 - accuracy: 0.2241\n",
      "Epoch 130/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0866 - accuracy: 0.2241\n",
      "Epoch 131/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0867 - accuracy: 0.2238\n",
      "Epoch 132/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0860 - accuracy: 0.2243\n",
      "Epoch 133/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0859 - accuracy: 0.2244\n",
      "Epoch 134/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0851 - accuracy: 0.2246\n",
      "Epoch 135/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0858 - accuracy: 0.2240\n",
      "Epoch 136/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0849 - accuracy: 0.2247\n",
      "Epoch 137/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0849 - accuracy: 0.2249\n",
      "Epoch 138/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0851 - accuracy: 0.2249\n",
      "Epoch 139/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0847 - accuracy: 0.2249\n",
      "Epoch 140/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0847 - accuracy: 0.2250\n",
      "Epoch 141/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0849 - accuracy: 0.2249\n",
      "Epoch 142/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0847 - accuracy: 0.2250\n",
      "Epoch 143/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0839 - accuracy: 0.2257\n",
      "Epoch 144/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0848 - accuracy: 0.2248\n",
      "Epoch 145/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0841 - accuracy: 0.2255\n",
      "Epoch 146/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0840 - accuracy: 0.2250\n",
      "Epoch 147/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0836 - accuracy: 0.2253\n",
      "Epoch 148/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0836 - accuracy: 0.2251\n",
      "Epoch 149/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0839 - accuracy: 0.2252\n",
      "Epoch 150/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0836 - accuracy: 0.2252\n",
      "Epoch 151/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0831 - accuracy: 0.2256\n",
      "Epoch 152/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0835 - accuracy: 0.2255\n",
      "Epoch 153/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0828 - accuracy: 0.2255\n",
      "Epoch 154/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0832 - accuracy: 0.2255\n",
      "Epoch 155/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0828 - accuracy: 0.2257\n",
      "Epoch 156/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0826 - accuracy: 0.2258\n",
      "Epoch 157/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0827 - accuracy: 0.2255\n",
      "Epoch 158/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0825 - accuracy: 0.2259\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0821 - accuracy: 0.2257\n",
      "Epoch 160/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0820 - accuracy: 0.2262\n",
      "Epoch 161/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0823 - accuracy: 0.2260\n",
      "Epoch 162/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0818 - accuracy: 0.2262\n",
      "Epoch 163/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0819 - accuracy: 0.2260\n",
      "Epoch 164/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0818 - accuracy: 0.2261\n",
      "Epoch 165/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0817 - accuracy: 0.2262\n",
      "Epoch 166/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0812 - accuracy: 0.2265\n",
      "Epoch 167/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0811 - accuracy: 0.2267\n",
      "Epoch 168/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0812 - accuracy: 0.2265\n",
      "Epoch 169/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0812 - accuracy: 0.2265\n",
      "Epoch 170/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0810 - accuracy: 0.2264\n",
      "Epoch 171/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0808 - accuracy: 0.2265\n",
      "Epoch 172/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0809 - accuracy: 0.2265\n",
      "Epoch 173/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0809 - accuracy: 0.2267\n",
      "Epoch 174/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0805 - accuracy: 0.2265\n",
      "Epoch 175/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0805 - accuracy: 0.2271\n",
      "Epoch 176/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0800 - accuracy: 0.2268\n",
      "Epoch 177/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0805 - accuracy: 0.2267\n",
      "Epoch 178/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0800 - accuracy: 0.2268\n",
      "Epoch 179/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0803 - accuracy: 0.2267\n",
      "Epoch 180/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0796 - accuracy: 0.2271\n",
      "Epoch 181/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0800 - accuracy: 0.2266\n",
      "Epoch 182/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0796 - accuracy: 0.2270\n",
      "Epoch 183/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0798 - accuracy: 0.2271\n",
      "Epoch 184/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0796 - accuracy: 0.2270\n",
      "Epoch 185/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0794 - accuracy: 0.2274\n",
      "Epoch 186/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0787 - accuracy: 0.2277\n",
      "Epoch 187/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0792 - accuracy: 0.2275\n",
      "Epoch 188/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0789 - accuracy: 0.2273\n",
      "Epoch 189/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0792 - accuracy: 0.2273\n",
      "Epoch 190/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0786 - accuracy: 0.2277\n",
      "Epoch 191/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0786 - accuracy: 0.2272\n",
      "Epoch 192/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0788 - accuracy: 0.2275\n",
      "Epoch 193/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0785 - accuracy: 0.2273\n",
      "Epoch 194/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0784 - accuracy: 0.2278\n",
      "Epoch 195/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0782 - accuracy: 0.2276\n",
      "Epoch 196/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0781 - accuracy: 0.2279\n",
      "Epoch 197/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0782 - accuracy: 0.2276\n",
      "Epoch 198/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0776 - accuracy: 0.2280\n",
      "Epoch 199/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0781 - accuracy: 0.2277\n",
      "Epoch 200/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0783 - accuracy: 0.2281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f70cc6f13a0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel.fit(X_train, y_train, epochs=20, batch_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from time import time\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.0000 seconds\n"
     ]
    }
   ],
   "source": [
    "import timer\n",
    "t = timer.Timer()\n",
    "t.start()\n",
    "\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimerError",
     "evalue": "Timer is not running. Use .start() to start it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimerError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-83ca768b201b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/gitgood/legendary-robot/kaggle_jane_street/timer.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Timer is running. Use .stop() to stop it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimerError\u001b[0m: Timer is not running. Use .start() to start it"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:02:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Finished training model\n",
      "Elapsed time: 100.4053 seconds\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "NAN_VALUE = -9999\n",
    "#features = [c for c in data.columns if 'feature' in c]\n",
    "#target = 'vol_bin'\n",
    "\n",
    "# Split into features X and target Y\n",
    "#X = data.loc[:, features].fillna(NAN_VALUE)\n",
    "#Y = (data.loc[:, target] > 0).astype(int)\n",
    "\n",
    "# Clear memory\n",
    "#del data\n",
    "#gc.collect()\n",
    "\n",
    "# Train model\n",
    "t.start()\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators = 500,\n",
    "    tree_method='gpu_hist',\n",
    "    nthread=multiprocessing.cpu_count()-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "print('Finished training model')\n",
    "\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softprob',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'gpu_id': 0,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': '',\n",
       " 'learning_rate': 0.300000012,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': '()',\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 23,\n",
       " 'num_parallel_tree': 1,\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': 1,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'validate_parameters': 1,\n",
       " 'verbosity': None,\n",
       " 'nthread': 23}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22440      1\n",
       "22441      1\n",
       "22442      1\n",
       "22443      1\n",
       "22444      1\n",
       "          ..\n",
       "2390486    2\n",
       "2390487    2\n",
       "2390488    2\n",
       "2390489    2\n",
       "2390490    2\n",
       "Name: vol_bin, Length: 479681, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.318647\n",
      "Precision: 0.327637\n",
      "Recall: 0.312099\n",
      "F1 score: 0.312950\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_pred, y_test, average = 'macro')\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_pred, y_test,average = 'macro')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_pred, y_test,average = 'macro')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skl_pred_score(y_pred, y_test):\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_pred, y_test, average = 'macro')\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_pred, y_test,average = 'macro')\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_pred, y_test,average = 'macro')\n",
    "    print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.1406872 , -5.133367  ,  0.33794835, -1.5648116 , -1.4330602 ,\n",
       "        -1.9282924 ,  0.29217353,  1.996077  ,  1.6452407 ,  1.560955  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "predictions = probability_model(X_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = probability_model(X_train[0:2].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([2.0905882e-03, 2.8500901e-04, 6.7767411e-02, 1.0107967e-02,\n",
       "       1.1531389e-02, 7.0275711e-03, 6.4735338e-02, 3.5574403e-01,\n",
       "       2.5047842e-01, 2.3023234e-01], dtype=float32)>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          7\n",
       "1          7\n",
       "2          7\n",
       "3          7\n",
       "4          7\n",
       "          ..\n",
       "2390486    6\n",
       "2390487    6\n",
       "2390488    6\n",
       "2390489    6\n",
       "2390490    6\n",
       "Name: vol_bin, Length: 2390491, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['vol_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def utility_score(df,action_vec):\n",
    "    \"\"\"Calculate utility score of a dataframe according to formulas defined at\n",
    "    https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    p = df['weight']  * df['resp'] * action_vec\n",
    "    df['p'] = p\n",
    "    p_i = df.set_index('date')['p'].groupby('date').sum()\n",
    "    t = (p_i.sum() / sqrt((p_i**2).sum())) * (sqrt(250 / p_i.index.size))\n",
    "    return min(max(t, 0), 6) * p_i.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = train[['date', 'resp','weight']]\n",
    "pred_df['actual'] = ((train['resp'].values) > 0).astype(int)\n",
    "preds = clf(X_train.values, training=False)\n",
    "pred_df['preds'] = preds.numpy()\n",
    "pred_df['action'] = (pred_df['preds'] >= 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score(pred_df, pred_df['action']) / utility_score(pred_df, pred_df['actual'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 131)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0271607 , -0.78438675,  0.7532629 ,  0.9322199 ,  0.18562594,\n",
       "         0.58452153,  0.14282605,  0.5791917 ,  0.18594626, -0.60336185]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07061819, 0.0331175 , 0.15411688, 0.18431908, 0.08736322,\n",
       "        0.13018675, 0.08370297, 0.12949471, 0.08739121, 0.0396895 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0387852"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 1.3377 - accuracy: 0.6045\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3775 - accuracy: 0.8934\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2971 - accuracy: 0.9158\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9301\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9397\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1895 - accuracy: 0.9448\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1760 - accuracy: 0.9504\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.9561\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1450 - accuracy: 0.9590\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1346 - accuracy: 0.9616\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1231 - accuracy: 0.9637\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1180 - accuracy: 0.9654\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1112 - accuracy: 0.9667\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1048 - accuracy: 0.9693\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9721\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9724\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.9725\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9752\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0817 - accuracy: 0.9753\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7137a24a00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size= 1000, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0815 - accuracy: 0.9747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08148473501205444, 0.9746999740600586]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.3066444 ,  -7.0519342 ,   0.6971853 ,   3.110398  ,\n",
       "         -9.046184  ,  -2.5873153 , -10.296194  ,  10.261124  ,\n",
       "         -2.562631  ,  -0.04820664]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "predictions = probability_model(x_test[:1])\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = probability_model(x_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390491, 131)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resLSTM(i_shape, multiplier = 1):\n",
    "    \n",
    "    inputs =  Input(shape = i_shape)\n",
    "    n_timesteps, n_features = i_shape[0], i_shape[1]\n",
    "\n",
    "    node_factor = int(n_features * multiplier)\n",
    "\n",
    "    ######### resnet LSTM model \n",
    "    stackLSTM0 = LSTM(node_factor * 10, activation='relu', input_shape = i_shape, return_sequences=True)(inputs)\n",
    "    stackLSTM0 = BatchNormalization()(stackLSTM0)\n",
    "    merge0 = concatenate([stackLSTM0, inputs])\n",
    "    \n",
    "    \n",
    "    stackLSTM1 = LSTM(node_factor * 5, activation='relu', return_sequences=True)(merge0)\n",
    "    stackLSTM1 = BatchNormalization()(stackLSTM1)\n",
    "    merge1 = concatenate([stackLSTM1, inputs])\n",
    "    \n",
    "        \n",
    "    stackLSTM2 = LSTM(node_factor * 2, activation='relu', return_sequences=True)(merge1)\n",
    "    stackLSTM2 = BatchNormalization()(stackLSTM2)\n",
    "    merge2 = concatenate([stackLSTM2, inputs])\n",
    "    \n",
    "    \n",
    "    resLSTM = LSTM(node_factor * 10, activation='relu', return_sequences=False)(merge2)\n",
    "    resLSTM = BatchNormalization()(resLSTM)\n",
    "    resLSTM = Dropout(rate = 0.3)(resLSTM)\n",
    "    # resLSTM = Flatten()(resLSTM)\n",
    "    resLSTM = Dense(node_factor * 10, activation='relu')(resLSTM)\n",
    "    resLSTM = Dropout(rate = 0.3)(resLSTM)\n",
    "    resLSTM = BatchNormalization()(resLSTM)\n",
    "    \n",
    "    \n",
    "\n",
    "    # output\n",
    "    output = Dense(node_factor * 1, activation='relu',kernel_regularizer='l1')(resLSTM)\n",
    "    # output = BatchNormalization()(output)\n",
    "    output = Dense(node_factor * 1, activation='relu')(output)\n",
    "    output = Dense(1)(output)\n",
    "    model = Model(inputs=inputs, outputs=output, name = 'resnet')\n",
    "    # summarize layers\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "    return model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle_tf] *",
   "language": "python",
   "name": "conda-env-kaggle_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
