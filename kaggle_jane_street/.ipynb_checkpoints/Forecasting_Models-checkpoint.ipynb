{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datatable as dt\n",
    "import timer\n",
    "js_path = '~/jane-street-market-prediction/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def skl_pred_score(y_pred, y_test):\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_pred, y_test, average = 'macro')\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_pred, y_test,average = 'macro')\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_pred, y_test,average = 'macro')\n",
    "    print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'score_func' from '/home/x99e/gitgood/legendary-robot/kaggle_jane_street/score_func.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp \n",
    "imp.reload(ret_kfold)\n",
    "imp.reload(score_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 3.5406 seconds\n"
     ]
    }
   ],
   "source": [
    "SEED = 123\n",
    "\n",
    "t = timer.Timer()\n",
    "t.start()\n",
    "import datatable as dt\n",
    "data = dt.fread('~/jane-street-market-prediction/train.csv').to_pandas()\n",
    "\n",
    "\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['action'] = ((data['resp'].values) > 0).astype(int)\n",
    "\n",
    "data.fillna(data.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['wresp'] = (data['resp'] * data['weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([data['resp'].shift(1),data['resp'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(data)\n",
    "scaled = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import score_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">action</th>\n",
       "      <th colspan=\"3\" halign=\"left\">resp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.439043</td>\n",
       "      <td>0.496334</td>\n",
       "      <td>1725</td>\n",
       "      <td>-0.012382</td>\n",
       "      <td>0.051774</td>\n",
       "      <td>-48.649796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.515610</td>\n",
       "      <td>0.499785</td>\n",
       "      <td>4426</td>\n",
       "      <td>-0.005658</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>-48.571445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.492734</td>\n",
       "      <td>0.499989</td>\n",
       "      <td>2916</td>\n",
       "      <td>-0.006934</td>\n",
       "      <td>0.034525</td>\n",
       "      <td>-41.033821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.477562</td>\n",
       "      <td>0.499548</td>\n",
       "      <td>2288</td>\n",
       "      <td>-0.007903</td>\n",
       "      <td>0.043108</td>\n",
       "      <td>-37.863545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>0.497708</td>\n",
       "      <td>0.500037</td>\n",
       "      <td>2932</td>\n",
       "      <td>-0.005707</td>\n",
       "      <td>0.037951</td>\n",
       "      <td>-33.619636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.506528</td>\n",
       "      <td>0.499989</td>\n",
       "      <td>3957</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.038605</td>\n",
       "      <td>41.996270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.567339</td>\n",
       "      <td>0.495506</td>\n",
       "      <td>2279</td>\n",
       "      <td>0.011341</td>\n",
       "      <td>0.042243</td>\n",
       "      <td>45.558622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.522672</td>\n",
       "      <td>0.499518</td>\n",
       "      <td>4092</td>\n",
       "      <td>0.006027</td>\n",
       "      <td>0.041523</td>\n",
       "      <td>47.181906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.496778</td>\n",
       "      <td>0.500031</td>\n",
       "      <td>3007</td>\n",
       "      <td>0.008935</td>\n",
       "      <td>0.043416</td>\n",
       "      <td>54.084014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.505469</td>\n",
       "      <td>0.499995</td>\n",
       "      <td>5037</td>\n",
       "      <td>0.005816</td>\n",
       "      <td>0.034035</td>\n",
       "      <td>57.957487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        action                      resp                     \n",
       "          mean       std   sum      mean       std        sum\n",
       "date                                                         \n",
       "73    0.439043  0.496334  1725 -0.012382  0.051774 -48.649796\n",
       "346   0.515610  0.499785  4426 -0.005658  0.036100 -48.571445\n",
       "282   0.492734  0.499989  2916 -0.006934  0.034525 -41.033821\n",
       "110   0.477562  0.499548  2288 -0.007903  0.043108 -37.863545\n",
       "451   0.497708  0.500037  2932 -0.005707  0.037951 -33.619636\n",
       "...        ...       ...   ...       ...       ...        ...\n",
       "276   0.506528  0.499989  3957  0.005376  0.038605  41.996270\n",
       "83    0.567339  0.495506  2279  0.011341  0.042243  45.558622\n",
       "488   0.522672  0.499518  4092  0.006027  0.041523  47.181906\n",
       "19    0.496778  0.500031  3007  0.008935  0.043416  54.084014\n",
       "5     0.505469  0.499995  5037  0.005816  0.034035  57.957487\n",
       "\n",
       "[500 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile = pd.pivot_table(data, index='date', values=['resp','action'],\n",
    "           aggfunc={'resp':[np.sum, np.std, np.mean], \n",
    "                   'action':[np.sum, np.std, np.mean]},fill_value=0)\n",
    "profile.sort_values(by=[('resp','sum')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action  mean       0.504164\n",
       "        std        0.499419\n",
       "        sum     2409.948000\n",
       "resp    mean       0.000377\n",
       "        std        0.025171\n",
       "        sum        1.952129\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = profile.iloc[:, profile.columns.get_level_values(1)=='mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.columns = plot_df.columns.get_level_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='action', ylabel='resp'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df.plot.scatter(y='resp', x='action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>action</th>\n",
       "      <th>wresp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14988</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.315568</td>\n",
       "      <td>-2.427847</td>\n",
       "      <td>...</td>\n",
       "      <td>4.710628</td>\n",
       "      <td>-0.633676</td>\n",
       "      <td>-0.833933</td>\n",
       "      <td>-0.158933</td>\n",
       "      <td>0.970844</td>\n",
       "      <td>1.427628</td>\n",
       "      <td>3.740462</td>\n",
       "      <td>14988</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14989</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>1</td>\n",
       "      <td>1.286473</td>\n",
       "      <td>0.815372</td>\n",
       "      <td>...</td>\n",
       "      <td>9.871553</td>\n",
       "      <td>0.256014</td>\n",
       "      <td>2.508071</td>\n",
       "      <td>1.825720</td>\n",
       "      <td>4.346613</td>\n",
       "      <td>3.966233</td>\n",
       "      <td>8.214060</td>\n",
       "      <td>14989</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14990</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001381</td>\n",
       "      <td>-0.001381</td>\n",
       "      <td>-0.001381</td>\n",
       "      <td>-0.001381</td>\n",
       "      <td>-0.001381</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.533538</td>\n",
       "      <td>-2.570111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229737</td>\n",
       "      <td>-1.420274</td>\n",
       "      <td>-3.004160</td>\n",
       "      <td>-1.725308</td>\n",
       "      <td>-1.176840</td>\n",
       "      <td>-0.882615</td>\n",
       "      <td>0.232439</td>\n",
       "      <td>14990</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14991</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.723242</td>\n",
       "      <td>-1.263938</td>\n",
       "      <td>...</td>\n",
       "      <td>2.654421</td>\n",
       "      <td>-1.046138</td>\n",
       "      <td>-1.562771</td>\n",
       "      <td>-1.220793</td>\n",
       "      <td>-0.172505</td>\n",
       "      <td>-0.329597</td>\n",
       "      <td>1.377567</td>\n",
       "      <td>14991</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>1</td>\n",
       "      <td>1.691772</td>\n",
       "      <td>1.088833</td>\n",
       "      <td>...</td>\n",
       "      <td>2.650584</td>\n",
       "      <td>-1.045037</td>\n",
       "      <td>-1.562759</td>\n",
       "      <td>-1.218922</td>\n",
       "      <td>-0.172518</td>\n",
       "      <td>-0.327931</td>\n",
       "      <td>1.377065</td>\n",
       "      <td>14992</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15214</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.213240</td>\n",
       "      <td>-1.557117</td>\n",
       "      <td>...</td>\n",
       "      <td>2.551488</td>\n",
       "      <td>0.525934</td>\n",
       "      <td>1.242721</td>\n",
       "      <td>1.977483</td>\n",
       "      <td>2.563083</td>\n",
       "      <td>1.857149</td>\n",
       "      <td>2.424928</td>\n",
       "      <td>15214</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15215</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.413328</td>\n",
       "      <td>-0.642504</td>\n",
       "      <td>...</td>\n",
       "      <td>22.159397</td>\n",
       "      <td>-0.101824</td>\n",
       "      <td>3.804838</td>\n",
       "      <td>1.780150</td>\n",
       "      <td>7.504901</td>\n",
       "      <td>4.702145</td>\n",
       "      <td>15.376130</td>\n",
       "      <td>15215</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15216</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.378947</td>\n",
       "      <td>-1.702976</td>\n",
       "      <td>...</td>\n",
       "      <td>2.685696</td>\n",
       "      <td>0.527251</td>\n",
       "      <td>1.245219</td>\n",
       "      <td>1.981606</td>\n",
       "      <td>2.567519</td>\n",
       "      <td>1.876328</td>\n",
       "      <td>2.450874</td>\n",
       "      <td>15216</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15217</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.324708</td>\n",
       "      <td>-1.089962</td>\n",
       "      <td>...</td>\n",
       "      <td>11.789678</td>\n",
       "      <td>0.310616</td>\n",
       "      <td>2.660067</td>\n",
       "      <td>3.052869</td>\n",
       "      <td>6.399390</td>\n",
       "      <td>5.396259</td>\n",
       "      <td>10.972647</td>\n",
       "      <td>15217</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15218</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.652183</td>\n",
       "      <td>-1.857245</td>\n",
       "      <td>...</td>\n",
       "      <td>22.254007</td>\n",
       "      <td>-0.098530</td>\n",
       "      <td>3.816995</td>\n",
       "      <td>1.793470</td>\n",
       "      <td>7.534632</td>\n",
       "      <td>4.730957</td>\n",
       "      <td>15.457871</td>\n",
       "      <td>15218</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  weight    resp_1    resp_2    resp_3    resp_4      resp  \\\n",
       "14988     2     0.0  0.000336  0.000336  0.000336  0.000336  0.000336   \n",
       "14989     2     0.0  0.001779  0.001779  0.001779  0.001779  0.001779   \n",
       "14990     2     0.0 -0.001381 -0.001381 -0.001381 -0.001381 -0.001381   \n",
       "14991     2     0.0  0.001658  0.001658  0.001658  0.001658  0.001658   \n",
       "14992     2     0.0  0.002666  0.002666  0.002666  0.002666  0.002666   \n",
       "...     ...     ...       ...       ...       ...       ...       ...   \n",
       "15214     2     0.0  0.003275  0.003275  0.003275  0.003275  0.003275   \n",
       "15215     2     0.0  0.002855  0.002855  0.002855  0.002855  0.002855   \n",
       "15216     2     0.0  0.003480  0.003480  0.003480  0.003480  0.003480   \n",
       "15217     2     0.0  0.002564  0.002564  0.002564  0.002564  0.002564   \n",
       "15218     2     0.0  0.003097  0.003097  0.003097  0.003097  0.003097   \n",
       "\n",
       "       feature_0  feature_1  feature_2  ...  feature_123  feature_124  \\\n",
       "14988          1  -2.315568  -2.427847  ...     4.710628    -0.633676   \n",
       "14989          1   1.286473   0.815372  ...     9.871553     0.256014   \n",
       "14990          1  -2.533538  -2.570111  ...     0.229737    -1.420274   \n",
       "14991          1  -0.723242  -1.263938  ...     2.654421    -1.046138   \n",
       "14992          1   1.691772   1.088833  ...     2.650584    -1.045037   \n",
       "...          ...        ...        ...  ...          ...          ...   \n",
       "15214          1  -1.213240  -1.557117  ...     2.551488     0.525934   \n",
       "15215          1  -0.413328  -0.642504  ...    22.159397    -0.101824   \n",
       "15216          1  -1.378947  -1.702976  ...     2.685696     0.527251   \n",
       "15217          1  -0.324708  -1.089962  ...    11.789678     0.310616   \n",
       "15218          1  -1.652183  -1.857245  ...    22.254007    -0.098530   \n",
       "\n",
       "       feature_125  feature_126  feature_127  feature_128  feature_129  ts_id  \\\n",
       "14988    -0.833933    -0.158933     0.970844     1.427628     3.740462  14988   \n",
       "14989     2.508071     1.825720     4.346613     3.966233     8.214060  14989   \n",
       "14990    -3.004160    -1.725308    -1.176840    -0.882615     0.232439  14990   \n",
       "14991    -1.562771    -1.220793    -0.172505    -0.329597     1.377567  14991   \n",
       "14992    -1.562759    -1.218922    -0.172518    -0.327931     1.377065  14992   \n",
       "...            ...          ...          ...          ...          ...    ...   \n",
       "15214     1.242721     1.977483     2.563083     1.857149     2.424928  15214   \n",
       "15215     3.804838     1.780150     7.504901     4.702145    15.376130  15215   \n",
       "15216     1.245219     1.981606     2.567519     1.876328     2.450874  15216   \n",
       "15217     2.660067     3.052869     6.399390     5.396259    10.972647  15217   \n",
       "15218     3.816995     1.793470     7.534632     4.730957    15.457871  15218   \n",
       "\n",
       "       action  wresp  \n",
       "14988       1    0.0  \n",
       "14989       1    0.0  \n",
       "14990       0   -0.0  \n",
       "14991       1    0.0  \n",
       "14992       1    0.0  \n",
       "...       ...    ...  \n",
       "15214       1    0.0  \n",
       "15215       1    0.0  \n",
       "15216       1    0.0  \n",
       "15217       1    0.0  \n",
       "15218       1    0.0  \n",
       "\n",
       "[231 rows x 140 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['date'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_profile = pd.pivot_table(data, index='date', values='resp',\n",
    "           aggfunc={'resp':[np.sum, np.std, np.mean]},fill_value=0)\n",
    "\n",
    "\n",
    "vol_bins = pd.qcut(vol_profile['std'], 4, labels=False)\n",
    "data['vol_bin'] = 0\n",
    "for vb in np.unique(vol_bins):\n",
    "    mask = vol_bins == vb\n",
    "    date_mask = data['date'].isin(vol_bins.loc[mask].index)\n",
    "    data.loc[date_mask,'vol_bin'] = vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['resp_bin'] = pd.qcut(data['resp'], 10, labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>action</th>\n",
       "      <th>wresp</th>\n",
       "      <th>vol_bin</th>\n",
       "      <th>resp_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>0.008773</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.872746</td>\n",
       "      <td>-2.191242</td>\n",
       "      <td>...</td>\n",
       "      <td>14.018213</td>\n",
       "      <td>2.653056</td>\n",
       "      <td>12.600292</td>\n",
       "      <td>2.301488</td>\n",
       "      <td>11.445807</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>16.673515</td>\n",
       "      <td>-0.002828</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>-0.007319</td>\n",
       "      <td>-0.011114</td>\n",
       "      <td>-0.009792</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.349537</td>\n",
       "      <td>-1.704709</td>\n",
       "      <td>...</td>\n",
       "      <td>2.831612</td>\n",
       "      <td>-1.417010</td>\n",
       "      <td>2.297459</td>\n",
       "      <td>-1.304614</td>\n",
       "      <td>1.898684</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.163262</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.027607</td>\n",
       "      <td>0.033406</td>\n",
       "      <td>0.034380</td>\n",
       "      <td>0.023970</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.812780</td>\n",
       "      <td>-0.256156</td>\n",
       "      <td>...</td>\n",
       "      <td>11.671595</td>\n",
       "      <td>7.281757</td>\n",
       "      <td>10.060014</td>\n",
       "      <td>6.638248</td>\n",
       "      <td>9.427299</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004730</td>\n",
       "      <td>-0.003273</td>\n",
       "      <td>-0.000461</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.003200</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.174378</td>\n",
       "      <td>0.344640</td>\n",
       "      <td>...</td>\n",
       "      <td>1.513488</td>\n",
       "      <td>4.397532</td>\n",
       "      <td>1.266037</td>\n",
       "      <td>3.856384</td>\n",
       "      <td>1.013469</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.138531</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>-0.001215</td>\n",
       "      <td>-0.006219</td>\n",
       "      <td>-0.002604</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>...</td>\n",
       "      <td>6.623456</td>\n",
       "      <td>0.800129</td>\n",
       "      <td>5.233243</td>\n",
       "      <td>0.362636</td>\n",
       "      <td>3.926633</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000361</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date     weight    resp_1    resp_2    resp_3    resp_4      resp  \\\n",
       "0     0   0.000000  0.009916  0.014079  0.008773  0.001390  0.006270   \n",
       "1     0  16.673515 -0.002828 -0.003226 -0.007319 -0.011114 -0.009792   \n",
       "2     0   0.000000  0.025134  0.027607  0.033406  0.034380  0.023970   \n",
       "3     0   0.000000 -0.004730 -0.003273 -0.000461 -0.000476 -0.003200   \n",
       "4     0   0.138531  0.001252  0.002165 -0.001215 -0.006219 -0.002604   \n",
       "\n",
       "   feature_0  feature_1  feature_2  ...  feature_125  feature_126  \\\n",
       "0          1  -1.872746  -2.191242  ...    14.018213     2.653056   \n",
       "1         -1  -1.349537  -1.704709  ...     2.831612    -1.417010   \n",
       "2         -1   0.812780  -0.256156  ...    11.671595     7.281757   \n",
       "3         -1   1.174378   0.344640  ...     1.513488     4.397532   \n",
       "4          1  -3.172026  -3.093182  ...     6.623456     0.800129   \n",
       "\n",
       "   feature_127  feature_128  feature_129  ts_id  action     wresp  vol_bin  \\\n",
       "0    12.600292     2.301488    11.445807      0       1  0.000000        2   \n",
       "1     2.297459    -1.304614     1.898684      1       0 -0.163262        2   \n",
       "2    10.060014     6.638248     9.427299      2       1  0.000000        2   \n",
       "3     1.266037     3.856384     1.013469      3       0 -0.000000        2   \n",
       "4     5.233243     0.362636     3.926633      4       0 -0.000361        2   \n",
       "\n",
       "   resp_bin  \n",
       "0         7  \n",
       "1         2  \n",
       "2         9  \n",
       "3         3  \n",
       "4         3  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, train_wresp, test_wresp = ret_kfold.kfold_split(data,mask,features,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "all_features = ['weight']+[c for c in data.columns if \"resp\" in c]+ [c for c in data.columns if \"feature\" in c]\n",
    "target = 'action'\n",
    "\n",
    "fold = 0 \n",
    "features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "\n",
    "target = 'resp'\n",
    "mask = ret_kfold.ret_fold_mask(fold)\n",
    "\n",
    "x_train, y_train, x_test, y_test, train_wresp, test_wresp = ret_kfold.kfold_split(data,mask,features,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resp_MLP():\n",
    "    hidden_units = 300\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(int(hidden_units/3), activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(int(hidden_units/6), activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(int(hidden_units/10), activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~/jane-street-market-prediction/models/resp_model.h5'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_path+'models/resp_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[293.1346256948761, 2857.4221440759834, -2564.2875183811075]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(score_func.u2_wresp(test_wresp,np.array(pred_action).reshape(len(pred_action),)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 1111\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_zero(x_train, y_train, x_test, y_test, train_wresp, test_wresp ):\n",
    "    train_mask = x_train['weight']== 0 \n",
    "    test_mask = x_test['weight']== 0 \n",
    "    \n",
    "    \n",
    "    return (x_train[train_mask], y_train[train_mask], \n",
    "            x_test[test_mask], y_test[test_mask], \n",
    "            train_wresp[train_mask], test_wresp[test_mask]    )\n",
    "\n",
    "\n",
    "def weight_nonzero(x_train, y_train, x_test, y_test, train_wresp, test_wresp ):\n",
    "    train_mask = x_train['weight']== 0 \n",
    "    test_mask = x_test['weight']== 0 \n",
    "    \n",
    "    \n",
    "    return (x_train[~train_mask], y_train[~train_mask], \n",
    "            x_test[~test_mask], y_test[~test_mask], \n",
    "            train_wresp[~train_mask], test_wresp[~test_mask]    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, train_wresp, test_wresp  = ret_kfold.kfold_split(data,mask,features,target)\n",
    "x_train, y_train, x_test, y_test, train_wresp, test_wresp = weight_nonzero(x_train, y_train, x_test, y_test, train_wresp, test_wresp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1592907, 131)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 [==============================] - 1s 8ms/step - loss: 0.8792\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.3214\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.2269\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.1860\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.1433\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.1085\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0916\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0729\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0558\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0481\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0381\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0281\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0222\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0173\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0136\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0113\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0089\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0070\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0050\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0042\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0032\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0024\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0018\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0013\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 9.3946e-04\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 6.7850e-04\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 5.2301e-04\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 3.7598e-04\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 2.5299e-04\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.8107e-04\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.3486e-04\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 9.4437e-05\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 6.5770e-05\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 4.5747e-05\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 2.9605e-05\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 2.0140e-05\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.3233e-05\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 9.3476e-06\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 6.3252e-06\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 4.0160e-06\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 2.6413e-06\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.5651e-06\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 9.9681e-07\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 6.2577e-07\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 3.6541e-07\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 2.2213e-07\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.2854e-07\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 7.3858e-08\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 4.8403e-08\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 3.1005e-08\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 1s 7ms/step - loss: 0.8764\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3430\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2398\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1882\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.1529\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.1117\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0887\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0735\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0616\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0494\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0414\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0330\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0261\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0218\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0178\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0128\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0111\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0092\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0061\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0051\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0044\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0028\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0022\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0017\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0014\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 9.2110e-04\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 6.9498e-04\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 5.1766e-04\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 3.9620e-04\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.9970e-04\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.1491e-04\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.5634e-04\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.0697e-04\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 7.5864e-05\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 5.1043e-05\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 4.1522e-05\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.6492e-05\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.5506e-05\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.1452e-05\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 7.9158e-06\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 5.0039e-06\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 3.2640e-06\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.1851e-06\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.4031e-06\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 8.6600e-07\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 5.3666e-07\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 3.7543e-07\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.1201e-07\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.3215e-07\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 8.1215e-08\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 8ms/step - loss: 0.8684\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.3121\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.2246\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.1670\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.1328\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.1084\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0825\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0638\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0537\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0416\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0331\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0254\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0189\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0151\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0123\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0090\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0075\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0056\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0042\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0031\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0022\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0017\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0012\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 8.3662e-04\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 6.2778e-04\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 4.3186e-04\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 3.2857e-04\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2.3141e-04\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.6224e-04\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.1033e-04\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 7.8461e-05\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 4.8868e-05\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 3.3161e-05\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2.2461e-05\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.4260e-05\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 9.4832e-06\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 5.9915e-06\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 3.8434e-06\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2.5598e-06\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.6746e-06\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 9.2009e-07\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 5.5890e-07\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 3.2471e-07\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2.0150e-07\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.1622e-07\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 6.8235e-08\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 4.0226e-08\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2.1669e-08\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.2815e-08\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 6.3614e-09\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 1s 8ms/step - loss: 0.8430\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.3265\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.2252\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.1989\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.1377\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.1093\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0858\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0698\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0544\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0466\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0364\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0292\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0244\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0186\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0138\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0114\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0085\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0067\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0055\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0040\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0031\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0025\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0018\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0014\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0010\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 7.5228e-04\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 5.5930e-04\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 4.1013e-04\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 2.8143e-04\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 2.1288e-04\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.4956e-04\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.1374e-04\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 7.0769e-05\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 4.7467e-05\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 3.4125e-05\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 2.3696e-05\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.4051e-05\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.0117e-05\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 6.1485e-06\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 4.2130e-06\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 2.6111e-06\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.7876e-06\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.0709e-06\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 6.6526e-07\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 4.1974e-07\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 2.4452e-07\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1.4596e-07\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 8.8662e-08\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 4.9379e-08\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 3.1619e-08\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 8ms/step - loss: 0.8775\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.3204\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.2203\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.1755\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.1384\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.1051\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0827\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0676\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0563\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0408\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0336\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0252\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0198\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0153\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0120\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0096\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0074\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0054\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0043\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0031\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0024\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0017\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0014\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 9.3876e-04\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 7.0336e-04\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 5.2227e-04\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 3.4497e-04\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2.5140e-04\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.7443e-04\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.2217e-04\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 8.8929e-05\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 5.9026e-05\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 3.8084e-05\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2.5663e-05\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.7248e-05\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.2521e-05\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 7.6596e-06\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 4.5856e-06\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2.8171e-06\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.8911e-06\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.0598e-06\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 6.6497e-07\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 4.3749e-07\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2.3737e-07\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.4752e-07\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 8.0172e-08\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 4.7490e-08\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2.6067e-08\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1.3801e-08\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 8.1687e-09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEED = 1111\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "wresp_scores = []\n",
    "fold = 0 \n",
    "for fold in range(0,5):\n",
    "    \n",
    "    SEED = 1111\n",
    "\n",
    "    tf.random.set_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "    target = 'wresp'\n",
    "\n",
    "    mask = ret_kfold.ret_fold_mask(fold)\n",
    "\n",
    "    x_train, y_train, x_test, y_test, train_wresp, test_wresp  = ret_kfold.kfold_split(data,mask,features,target)\n",
    "    \n",
    "    x_train, y_train, x_test, y_test, train_wresp, test_wresp = weight_zero(x_train, y_train, x_test, y_test, train_wresp, test_wresp )\n",
    "    \n",
    "    \n",
    "    scaler.fit(x_train)\n",
    "    #x_train = scaler.transform(x_train)\n",
    "    #x_test = scaler.transform(x_test)\n",
    "\n",
    "    loss_fn = 'mse'\n",
    "    model = resp_MLP()\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "    modelcheckpoint  = tf.keras.callbacks.ModelCheckpoint(js_path+'models/resp_model.h5', \n",
    "                                                          monitor='loss', mode='auto', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_fn)\n",
    "    model.fit(x_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1, callbacks= [earlystopping])\n",
    "    #model.fit(x_train, y_train, batch_size= 15000, epochs=50 ,verbose  = 1)\n",
    "    preds = model(x_test.values).numpy()\n",
    "    pred_action = (preds > 0) * 1\n",
    "    wresp_scores.append(score_func.u2_wresp(test_wresp,np.array(pred_action).reshape(len(pred_action),)))\n",
    "#model.evaluate(x_test, y_test)\n",
    "wresp_scores\n",
    "weight_zero_model = model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "160/160 [==============================] - 2s 8ms/step - loss: 0.5120\n",
      "Epoch 2/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.1111\n",
      "Epoch 3/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0454\n",
      "Epoch 4/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0241\n",
      "Epoch 5/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0180\n",
      "Epoch 6/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0155\n",
      "Epoch 7/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0156\n",
      "Epoch 8/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0153\n",
      "Epoch 9/50\n",
      "160/160 [==============================] - 1s 7ms/step - loss: 0.0154\n",
      "Epoch 10/50\n",
      "160/160 [==============================] - 1s 7ms/step - loss: 0.0152\n",
      "Epoch 11/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0151\n",
      "Epoch 12/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0149\n",
      "Epoch 13/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0156\n",
      "Epoch 14/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0157\n",
      "Epoch 15/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0158\n",
      "Epoch 16/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0152\n",
      "Epoch 17/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0158\n",
      "Epoch 18/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0150\n",
      "Epoch 19/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0154\n",
      "Epoch 1/50\n",
      "158/158 [==============================] - 2s 8ms/step - loss: 0.5156\n",
      "Epoch 2/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.1122\n",
      "Epoch 3/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0501\n",
      "Epoch 4/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0259\n",
      "Epoch 5/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0183\n",
      "Epoch 6/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0156\n",
      "Epoch 7/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0155\n",
      "Epoch 8/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0151\n",
      "Epoch 9/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0155\n",
      "Epoch 10/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0152\n",
      "Epoch 11/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0153\n",
      "Epoch 12/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0150\n",
      "Epoch 13/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0152\n",
      "Epoch 14/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0152\n",
      "Epoch 15/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0153\n",
      "Epoch 16/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0147\n",
      "Epoch 1/50\n",
      "160/160 [==============================] - 2s 8ms/step - loss: 0.5198\n",
      "Epoch 2/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.1115\n",
      "Epoch 3/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0465\n",
      "Epoch 4/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0242\n",
      "Epoch 5/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0177\n",
      "Epoch 6/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0155\n",
      "Epoch 7/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0156\n",
      "Epoch 8/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0150\n",
      "Epoch 9/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0155\n",
      "Epoch 10/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0154\n",
      "Epoch 11/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0151\n",
      "Epoch 12/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0155\n",
      "Epoch 13/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0151\n",
      "Epoch 14/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0151\n",
      "Epoch 15/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0149\n",
      "Epoch 1/50\n",
      "158/158 [==============================] - 2s 7ms/step - loss: 0.5165\n",
      "Epoch 2/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.1173\n",
      "Epoch 3/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0481\n",
      "Epoch 4/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0251\n",
      "Epoch 5/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0178\n",
      "Epoch 6/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0158\n",
      "Epoch 7/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0146\n",
      "Epoch 8/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0148\n",
      "Epoch 9/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0145\n",
      "Epoch 10/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0143\n",
      "Epoch 11/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0146\n",
      "Epoch 12/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0148\n",
      "Epoch 13/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0143\n",
      "Epoch 14/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0147\n",
      "Epoch 15/50\n",
      "158/158 [==============================] - 1s 8ms/step - loss: 0.0145\n",
      "Epoch 1/50\n",
      "160/160 [==============================] - 2s 7ms/step - loss: 0.5387\n",
      "Epoch 2/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.1200\n",
      "Epoch 3/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0507\n",
      "Epoch 4/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0273\n",
      "Epoch 5/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0197\n",
      "Epoch 6/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0167\n",
      "Epoch 7/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0161\n",
      "Epoch 8/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0162\n",
      "Epoch 9/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0158\n",
      "Epoch 10/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0161\n",
      "Epoch 11/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0161\n",
      "Epoch 12/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0155\n",
      "Epoch 13/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0153\n",
      "Epoch 14/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0156\n",
      "Epoch 15/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0156\n",
      "Epoch 16/50\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0156\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEED = 1111\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "wresp_scores = []\n",
    "fold = 0 \n",
    "for fold in range(0,5):\n",
    "    \n",
    "    SEED = 1111\n",
    "\n",
    "    tf.random.set_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "    target = 'wresp'\n",
    "\n",
    "    mask = ret_kfold.ret_fold_mask(fold)\n",
    "\n",
    "    x_train, y_train, x_test, y_test, train_wresp, test_wresp  = ret_kfold.kfold_split(data,mask,features,target)\n",
    "    \n",
    "    x_train, y_train, x_test, y_test, train_wresp, test_wresp = weight_nonzero(x_train, y_train, x_test, y_test, train_wresp, test_wresp )\n",
    "    \n",
    "    \n",
    "    scaler.fit(x_train)\n",
    "    #x_train = scaler.transform(x_train)\n",
    "    #x_test = scaler.transform(x_test)\n",
    "\n",
    "    loss_fn = 'mse'\n",
    "    model = resp_MLP()\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "    modelcheckpoint  = tf.keras.callbacks.ModelCheckpoint(js_path+'models/resp_model.h5', \n",
    "                                                          monitor='loss', mode='auto', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_fn)\n",
    "    model.fit(x_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1, callbacks= [earlystopping])\n",
    "    #model.fit(x_train, y_train, batch_size= 15000, epochs=50 ,verbose  = 1)\n",
    "    preds = model(x_test.values).numpy()\n",
    "    pred_action = (preds > 0) * 1\n",
    "    wresp_scores.append(score_func.u2_wresp(test_wresp,np.array(pred_action).reshape(len(pred_action),)))\n",
    "#model.evaluate(x_test, y_test)\n",
    "wresp_scores\n",
    "weight_nonzero_model = model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 9ms/step - loss: 0.6574\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.1247\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0416\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0136\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0046\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0017\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 9.6281e-04\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.8410e-04\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.5212e-04\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4087e-04\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4142e-04\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4101e-04\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4044e-04\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.3864e-04\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.3900e-04\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4238e-04\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4404e-04\n",
      "[203.02326936778, 4395.213404270615, -4192.190134902836]\n",
      "Epoch 1/50\n",
      "160/160 [==============================] - 4s 9ms/step - loss: 0.5604\n",
      "Epoch 2/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.1165\n",
      "Epoch 3/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.0459\n",
      "Epoch 4/50\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.0192\n",
      "Epoch 5/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.0078\n",
      "Epoch 6/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.0034\n",
      "Epoch 7/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.0017\n",
      "Epoch 8/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.0011\n",
      "Epoch 9/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 8.9330e-04\n",
      "Epoch 10/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.3508e-04\n",
      "Epoch 11/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.1574e-04\n",
      "Epoch 12/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.1512e-04\n",
      "Epoch 13/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.1287e-04\n",
      "Epoch 14/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.0923e-04\n",
      "Epoch 15/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.1069e-04\n",
      "Epoch 16/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.0808e-04\n",
      "Epoch 17/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.0849e-04\n",
      "Epoch 18/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.0728e-04\n",
      "Epoch 19/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.1351e-04\n",
      "[68.55051603034079, 5282.798329510266, -5214.247813479927]\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 2s 9ms/step - loss: 1.1811\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.4546\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.3358\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.2657\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.2153\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.1787\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.1497\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.1185\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.1011\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0786\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0686\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0554\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0452\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0395\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0325\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0277\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0215\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0179\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0151\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0126\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0104\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0089\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0073\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0062\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0051\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0043\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0036\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0030\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0024\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0021\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0017\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0015\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0013\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0011\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 9.8311e-04\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 8.8106e-04\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 7.5822e-04\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 6.8184e-04\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 6.1691e-04\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 5.7535e-04\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 5.3184e-04\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 5.0669e-04\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 4.7651e-04\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 4.5830e-04\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.4933e-04\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 4.3264e-04\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.2828e-04\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.1862e-04\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.1571e-04\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 4.1027e-04\n",
      "[0.0, 0.0, 0.0]\n",
      "Epoch 1/50\n",
      "190/190 [==============================] - 3s 9ms/step - loss: 0.6584\n",
      "Epoch 2/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.1253\n",
      "Epoch 3/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.0416\n",
      "Epoch 4/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.0137\n",
      "Epoch 5/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.0047\n",
      "Epoch 6/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.0018\n",
      "Epoch 7/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 9.5181e-04\n",
      "Epoch 8/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 7.6039e-04\n",
      "Epoch 9/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 7.1761e-04\n",
      "Epoch 10/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 7.1313e-04\n",
      "Epoch 11/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 7.0984e-04\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - 2s 10ms/step - loss: 7.0986e-04\n",
      "Epoch 13/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 7.1612e-04\n",
      "Epoch 14/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 7.1285e-04\n",
      "Epoch 15/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 7.1346e-04\n",
      "Epoch 16/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 7.0655e-04\n",
      "Epoch 17/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 7.1354e-04\n",
      "Epoch 18/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 7.0793e-04\n",
      "[175.00178921782043, 3957.037803136689, -3782.0360139188683]\n",
      "Epoch 1/50\n",
      "158/158 [==============================] - 3s 9ms/step - loss: 0.5627\n",
      "Epoch 2/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.1171\n",
      "Epoch 3/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.0463\n",
      "Epoch 4/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.0192\n",
      "Epoch 5/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.0079\n",
      "Epoch 6/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.0035\n",
      "Epoch 7/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.0017\n",
      "Epoch 8/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.0011\n",
      "Epoch 9/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 8.6745e-04\n",
      "Epoch 10/50\n",
      "158/158 [==============================] - 2s 9ms/step - loss: 7.9946e-04\n",
      "Epoch 11/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.7635e-04\n",
      "Epoch 12/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.8195e-04\n",
      "Epoch 13/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.7772e-04\n",
      "Epoch 14/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.8044e-04\n",
      "Epoch 15/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.8152e-04\n",
      "Epoch 16/50\n",
      "158/158 [==============================] - 2s 9ms/step - loss: 7.7073e-04\n",
      "Epoch 17/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.7734e-04\n",
      "Epoch 18/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.7519e-04\n",
      "Epoch 19/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 7.7422e-04\n",
      "Epoch 20/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.7600e-04\n",
      "Epoch 21/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.7746e-04\n",
      "Epoch 22/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.7699e-04\n",
      "Epoch 23/50\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 7.7299e-04\n",
      "[203.31726801414854, 5798.287310940376, -5594.970042926228]\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 2s 9ms/step - loss: 1.1734\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.4565\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.3334\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.2722\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.2196\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.1780\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.1475\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.1238\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.1062\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0886\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0771\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0622\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0515\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0424\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0370\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0304\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0246\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0207\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0175\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0150\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0126\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0103\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0083\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0072\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0063\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0052\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0043\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0036\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0031\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0026\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0021\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0018\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0015\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0014\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0012\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0010\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 9.0262e-04\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 7.9116e-04\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 7.1684e-04\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 6.5195e-04\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 5.9231e-04\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 5.3606e-04\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 5.1039e-04\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.8205e-04\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.6063e-04\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.4461e-04\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.3025e-04\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.1775e-04\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.0852e-04\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 3.9963e-04\n",
      "[0.0, 0.0, 0.0]\n",
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 10ms/step - loss: 0.6476\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.1243\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.0419\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.0138\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.0046\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.0017\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 9.5648e-04\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.6814e-04\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.3332e-04\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.3012e-04\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.2420e-04\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.2734e-04\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.3098e-04\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.2963e-04\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.2874e-04\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.2294e-04\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 2s 10ms/step - loss: 7.3025e-04\n",
      "[309.02084762997606, 5094.141398110318, -4785.12055048034]\n",
      "Epoch 1/50\n",
      "160/160 [==============================] - 3s 9ms/step - loss: 0.5637\n",
      "Epoch 2/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.1163\n",
      "Epoch 3/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.0464\n",
      "Epoch 4/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.0187\n",
      "Epoch 5/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.0078\n",
      "Epoch 6/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.0034\n",
      "Epoch 7/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.0017\n",
      "Epoch 8/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.0011\n",
      "Epoch 9/50\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 8.7914e-04\n",
      "Epoch 10/50\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 8.1455e-04\n",
      "Epoch 11/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 7.9908e-04\n",
      "Epoch 12/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 7.9475e-04\n",
      "Epoch 13/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 7.9346e-04\n",
      "Epoch 14/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 7.9707e-04\n",
      "Epoch 15/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 7.9685e-04\n",
      "Epoch 16/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 7.9413e-04\n",
      "Epoch 17/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 7.9256e-04\n",
      "Epoch 18/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 7.9170e-04\n",
      "Epoch 19/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 7.9557e-04\n",
      "[69.11762737754024, 5307.613526294911, -5238.495898917372]\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 2s 9ms/step - loss: 1.1746\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.4433\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.3371\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.2508\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.2152\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1651\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1335\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1144\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0923\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0729\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0609\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.0506\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0415\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0361\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0282\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0234\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0187\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0163\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0125\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0115\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0087\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0079\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0059\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.0050\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0040\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0034\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0027\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0024\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0019\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0017\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0013\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0012\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0011\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 9.1729e-04\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 7.9736e-04\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 7.2848e-04\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 6.7173e-04\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 5.8419e-04\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 5.4216e-04\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 5.0024e-04\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.9045e-04\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 4.5736e-04\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 4.4671e-04\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.3212e-04\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.2389e-04\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.1767e-04\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.1224e-04\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.0655e-04\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.0876e-04\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.0133e-04\n",
      "[0.0, 0.0, 0.0]\n",
      "Epoch 1/50\n",
      "191/191 [==============================] - 3s 9ms/step - loss: 0.6461\n",
      "Epoch 2/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.1240\n",
      "Epoch 3/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.0406\n",
      "Epoch 4/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.0133\n",
      "Epoch 5/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.0045\n",
      "Epoch 6/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.0017\n",
      "Epoch 7/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 9.5132e-04\n",
      "Epoch 8/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 7.5915e-04\n",
      "Epoch 9/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 7.1517e-04\n",
      "Epoch 10/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 7.1219e-04\n",
      "Epoch 11/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 7.1223e-04\n",
      "Epoch 12/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 7.1349e-04\n",
      "Epoch 13/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 7.1723e-04\n",
      "Epoch 14/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 7.1259e-04\n",
      "Epoch 15/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 7.1380e-04\n",
      "Epoch 16/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 7.1534e-04\n",
      "Epoch 17/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 7.1371e-04\n",
      "[-458.8500374734168, 6647.87688879749, -7106.726926270907]\n",
      "Epoch 1/50\n",
      "158/158 [==============================] - 3s 9ms/step - loss: 0.5601\n",
      "Epoch 2/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 0.1162\n",
      "Epoch 3/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 0.0464\n",
      "Epoch 4/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 0.0189\n",
      "Epoch 5/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 6/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 0.0034\n",
      "Epoch 7/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 0.0017\n",
      "Epoch 8/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 0.0011\n",
      "Epoch 9/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 8.7283e-04\n",
      "Epoch 10/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 8.0784e-04\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 9ms/step - loss: 7.8356e-04\n",
      "Epoch 12/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 7.8092e-04\n",
      "Epoch 13/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 7.7847e-04\n",
      "Epoch 14/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 7.7750e-04\n",
      "Epoch 15/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 7.7809e-04\n",
      "Epoch 16/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 7.7916e-04\n",
      "Epoch 17/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 7.7907e-04\n",
      "Epoch 18/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 7.7925e-04\n",
      "Epoch 19/50\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 7.7869e-04\n",
      "[-504.4254386359492, 7583.244524022247, -8087.669962658196]\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 2s 10ms/step - loss: 1.2001\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.4450\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.3302\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.2652\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.2194\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.1754\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.1412\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.1158\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0942\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0774\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0658\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0517\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0441\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0364\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0302\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0258\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0211\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0172\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0146\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0117\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0097\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0086\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0068\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0056\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0050\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0043\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0034\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0028\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0024\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0022\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0017\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0016\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0012\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 0.0010\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 9.1807e-04\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 8.5389e-04\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 7.6541e-04\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 6.8148e-04\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 6.2240e-04\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 5.7182e-04\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 5.2159e-04\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.8338e-04\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.7831e-04\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.6103e-04\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.3878e-04\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.2354e-04\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 4.2228e-04\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.1064e-04\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.1102e-04\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.0774e-04\n",
      "[0.0, 0.0, 0.0]\n",
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 9ms/step - loss: 0.6381\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.1190\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0386\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.0126\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.0043\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0016\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 9.2524e-04\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.6973e-04\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.4038e-04\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.3182e-04\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.3049e-04\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.3362e-04\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.3542e-04\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.2993e-04\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.2994e-04\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 7.3368e-04\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.3399e-04\n",
      "[174.52478904126633, 4576.088527609438, -4401.563738568172]\n",
      "Epoch 1/50\n",
      "160/160 [==============================] - 3s 9ms/step - loss: 0.5603\n",
      "Epoch 2/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.1151\n",
      "Epoch 3/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.0453\n",
      "Epoch 4/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.0178\n",
      "Epoch 5/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.0073\n",
      "Epoch 6/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.0031\n",
      "Epoch 7/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.0016\n",
      "Epoch 8/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.0011\n",
      "Epoch 9/50\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 8.6523e-04\n",
      "Epoch 10/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.1751e-04\n",
      "Epoch 11/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.0472e-04\n",
      "Epoch 12/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.0495e-04\n",
      "Epoch 13/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 7.9603e-04\n",
      "Epoch 14/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.0289e-04\n",
      "Epoch 15/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.0905e-04\n",
      "Epoch 16/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.0155e-04\n",
      "Epoch 17/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 8.0149e-04\n",
      "Epoch 18/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 7.9968e-04\n",
      "Epoch 19/50\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 7.9780e-04\n",
      "[89.45350299413185, 5541.736625954531, -5452.2831229604]\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 2s 9ms/step - loss: 1.1753\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 9ms/step - loss: 0.4772\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.3389\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.2614\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.2175\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1699\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1416\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1143\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0954\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0804\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0670\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0538\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0458\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0356\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0319\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0263\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0205\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0172\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0147\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0125\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0097\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0085\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0070\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0060\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0050\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0039\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0035\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0029\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0023\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0020\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0017\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.0014\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0012\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0011\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 9.2962e-04\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 8.1008e-04\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 7.0748e-04\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 6.6328e-04\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 5.8190e-04\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 5.5960e-04\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 5.1266e-04\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.9143e-04\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.7288e-04\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.4742e-04\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.3857e-04\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 4.2893e-04\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.2342e-04\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.2202e-04\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.1764e-04\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 4.1387e-04\n",
      "[0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "SEED = 1111\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "modelcheckpoint  = tf.keras.callbacks.ModelCheckpoint(js_path+'models/resp_model.h5', \n",
    "                                                          monitor='loss', mode='auto', verbose=1, save_best_only=True)\n",
    "loss_fn = 'mse' \n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "full_scores = []\n",
    "nonzero_scores = []\n",
    "zero_scores = []\n",
    "\n",
    "full_models= []\n",
    "nonzero_models = []\n",
    "zero_models= []\n",
    "\n",
    "fold = 0 \n",
    "for fold in range(0,5):\n",
    "    \n",
    "    SEED = 1111\n",
    "\n",
    "    tf.random.set_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "    target = 'resp'\n",
    "\n",
    "    mask = ret_kfold.ret_fold_mask(fold)\n",
    "\n",
    "    x_train, y_train, x_test, y_test, train_wresp, test_wresp  = ret_kfold.kfold_split(data,mask,features,target)\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    \n",
    "    weight_full_model = resp_MLP()\n",
    "    weight_full_model.compile(optimizer='adam',\n",
    "                  loss=loss_fn)\n",
    "    weight_full_model.fit(x_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1, callbacks= [earlystopping])\n",
    "    #model.fit(x_train, y_train, batch_size= 15000, epochs=50 ,verbose  = 1)\n",
    "    preds = weight_full_model(x_test).numpy()\n",
    "    pred_action = (preds > 0) * 1\n",
    "    full_scores.append(score_func.u2_wresp(test_wresp,np.array(pred_action).reshape(len(pred_action),)))\n",
    "    #weight_full_model = model \n",
    "    full_models.append(weight_full_model)\n",
    "    print(full_scores[-1])\n",
    "    \n",
    "    ##########\n",
    "    x_train, y_train, x_test, y_test, train_wresp, test_wresp  = ret_kfold.kfold_split(data,mask,features,target)\n",
    "    x_train, y_train, x_test, y_test, train_wresp, test_wresp = weight_nonzero(x_train, y_train, x_test, y_test, train_wresp, test_wresp )\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "    weight_nonzero_model = resp_MLP()\n",
    "    weight_nonzero_model.compile(optimizer='adam',\n",
    "                  loss=loss_fn)\n",
    "    weight_nonzero_model.fit(x_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1, callbacks= [earlystopping])\n",
    "    #model.fit(x_train, y_train, batch_size= 15000, epochs=50 ,verbose  = 1)\n",
    "    preds = weight_nonzero_model(x_test).numpy()\n",
    "    pred_action = (preds > 0) * 1\n",
    "    nonzero_scores.append(score_func.u2_wresp(test_wresp,np.array(pred_action).reshape(len(pred_action),)))\n",
    "    #weight_full_model = model \n",
    "    nonzero_models.append(weight_nonzero_model)\n",
    "    print(nonzero_scores[-1])\n",
    "    \n",
    "    \n",
    "    ################\n",
    "    x_train, y_train, x_test, y_test, train_wresp, test_wresp  = ret_kfold.kfold_split(data,mask,features,target)\n",
    "    x_train, y_train, x_test, y_test, train_wresp, test_wresp = weight_zero(x_train, y_train, x_test, y_test, train_wresp, test_wresp )\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "    weight_zero_model = resp_MLP()\n",
    "    weight_zero_model.compile(optimizer='adam',\n",
    "                  loss=loss_fn)\n",
    "    weight_zero_model.fit(x_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1, callbacks= [earlystopping])\n",
    "    #model.fit(x_train, y_train, batch_size= 15000, epochs=50 ,verbose  = 1)\n",
    "    preds = weight_zero_model(x_test).numpy()\n",
    "    pred_action = (preds > 0) * 1\n",
    "    zero_scores.append(score_func.u2_wresp(test_wresp,np.array(pred_action).reshape(len(pred_action),)))\n",
    "    #weight_full_model = model \n",
    "    zero_models.append(weight_zero_model)\n",
    "    print(zero_scores[-1])\n",
    "    #scaler.fit(x_train)\n",
    "    #x_train = scaler.transform(x_train)\n",
    "    #x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "#model.evaluate(x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920551, 131)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, train_wresp, test_wresp  = ret_kfold.kfold_split(data,mask,features,target)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1592907, 131)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, train_wresp, test_wresp = weight_nonzero(x_train, y_train, x_test, y_test, train_wresp, test_wresp )\n",
    "x_train.shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327644, 131)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, train_wresp, test_wresp  = ret_kfold.kfold_split(data,mask,features,target)\n",
    "x_train, y_train, x_test, y_test, train_wresp, test_wresp = weight_zero(x_train, y_train, x_test, y_test, train_wresp, test_wresp )\n",
    "x_train.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[217.28000377239164, 4921.05688513385, -4703.776881361458],\n",
       " [139.45278309529087, 6929.485767545234, -6790.032984449946],\n",
       " [454.8642593372436, 4588.238800758754, -4133.374541421511],\n",
       " [-485.73971723238515, 6728.079052523933, -7213.818769756319],\n",
       " [152.21957829662477, 5456.8803641337545, -5304.66078583713]]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[62.65496140570919, 3084.0122076917432, -3021.357246286035],\n",
       " [241.47115516672565, 3615.05600674479, -3373.5848515780626],\n",
       " [96.50665591946134, 5904.108867539636, -5807.602211620175],\n",
       " [-233.82760591907646, 5616.491719555278, -5850.319325474352],\n",
       " [51.81806022077859, 4335.165689152318, -4283.347628931539]]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, train_wresp, test_wresp  = ret_kfold.kfold_split(data,mask,features,target)\n",
    "#x_train, y_train, x_test, y_test, train_wresp, test_wresp = weight_nonzero(x_train, y_train, x_test, y_test, train_wresp, test_wresp )\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "i = 4\n",
    "pred1 = full_models[i](x_test).numpy()\n",
    "\n",
    "pred2 = nonzero_models[i](x_test).numpy()\n",
    "\n",
    "pred3 = zero_models[i](x_test).numpy()\n",
    "y_action = (y_test > 0 )  * 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46.13354849699238, 2507.251201325604, -2461.1176528286114]\n",
      "Accuracy: 0.503660\n",
      "Precision: 0.504743\n",
      "Recall: 0.505564\n",
      "F1 score: 0.485220\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pred_ave= np.min([pred1,pred2,pred3],axis =0)\n",
    "pred_action = (pred_ave > 0) * 1\n",
    "print(score_func.u2_wresp(test_wresp,np.array(pred_action).reshape(len(pred_action),)))\n",
    "print(skl_pred_score(pred_action, y_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115.0976375431605, 6254.38385632313, -6139.286218779969]\n",
      "Accuracy: 0.504742\n",
      "Precision: 0.503093\n",
      "Recall: 0.504703\n",
      "F1 score: 0.457396\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pred_ave= np.max([pred1,pred2,pred3],axis =0)\n",
    "pred_action = (pred_ave > 0) * 1\n",
    "print(score_func.u2_wresp(test_wresp,np.array(pred_action).reshape(len(pred_action),)))\n",
    "print(skl_pred_score(pred_action, y_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122.66770087874197, 4527.584250002748, -4404.9165491240055]\n",
      "Accuracy: 0.505898\n",
      "Precision: 0.505562\n",
      "Recall: 0.505642\n",
      "F1 score: 0.503960\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pred_ave= np.mean([pred1,pred2,pred3],axis =0)\n",
    "pred_action = (pred_ave > 0) * 1\n",
    "print(score_func.u2_wresp(test_wresp,np.array(pred_action).reshape(len(pred_action),)))\n",
    "print(skl_pred_score(pred_action, y_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.506341\n",
      "Precision: 0.504348\n",
      "Recall: 0.507872\n",
      "F1 score: 0.442864\n"
     ]
    }
   ],
   "source": [
    "y_action = (y_test > 0 )  * 1 \n",
    "skl_pred_score(pred_action, y_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = weight_zero_model(x_test.values).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = weight_nonzero_model(x_test.values).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = weight_full_model(x_test.values).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resp_reg_loss(y_true, y_pred):\n",
    "    loss = (y_true - y_pred) ** 2\n",
    "    mask = (y_true > 0 ) * (y_pred < 0)\n",
    "    \n",
    "    signed_loss = loss * mask\n",
    "    mask = (y_true < 0 ) * (y_pred > 0)\n",
    "    signed_loss = loss * mask + signed_loss\n",
    "    return -1 * (loss + signed_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resp_reg_loss(y_true, y_pred):\n",
    "    loss = (y_true - y_pred) ** 2\n",
    "    mask = ((np.array(y_true).reshape(-1,1)  > 0 ) * (np.array(y_pred).reshape(-1,1)  < 0)) \n",
    "    signed_loss = loss * mask\n",
    "    mask = ((np.array(y_true).reshape(-1,1)  > 0 ) * (np.array(y_pred).reshape(-1,1)  < 0)) \n",
    "    signed_loss = loss * mask + signed_loss\n",
    "    return -1 * (loss + signed_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.22 TiB for an array with shape (409459, 409459) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-0dfeab07eedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresp_reg_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-266-8a919f15b54b>\u001b[0m in \u001b[0;36mresp_reg_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresp_reg_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msigned_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__sub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rsub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   4957\u001b[0m         \u001b[0mlvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4958\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4959\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4961\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;31m# error: \"None\" not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.22 TiB for an array with shape (409459, 409459) and data type float64"
     ]
    }
   ],
   "source": [
    "resp_reg_loss(y_test, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0287465 ],\n",
       "       [ 0.10153913],\n",
       "       [-0.00660517],\n",
       "       ...,\n",
       "       [-0.00746873],\n",
       "       [-0.00848901],\n",
       "       [-0.00590622]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test).reshape(-1,1) - np.array(pred1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.22 TiB for an array with shape (409459, 409459) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-269-f13c47e25d68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__sub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rsub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   4957\u001b[0m         \u001b[0mlvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4958\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4959\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4961\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;31m# error: \"None\" not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.22 TiB for an array with shape (409459, 409459) and data type float64"
     ]
    }
   ],
   "source": [
    "loss = (y_test - pred1) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = ((np.array(y_test).reshape(-1,1)  > 0 ) * (np.array(pred1).reshape(-1,1)  < 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13631157])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mask1)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35386937])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask2 = ((np.array(y_test).reshape(-1,1)  < 0 ) * (np.array(pred1).reshape(-1,1)  > 0)) \n",
    "sum(mask2)/ len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([723.94660247])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mask1 *np.array(y_test).reshape(-1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_df= pd.DataFrame(test_wresp * np.array(pred_action).reshape(len(pred_action),))\n",
    "s_df.cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEQCAYAAABGL0RbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAh0UlEQVR4nO3dd3xVVb7+8c+XEHrvNdQAQiCUgKAzjqA46DhYURzLODqDXtu1j3qtI45l7G0U6ziWQYqACipWioqCEhISQkkooYaWQkg96/dHMvPjctGchJzss8953q8XL5KcnZxnceBxuc/ea5lzDhER8ad6XgcQEZGaU4mLiPiYSlxExMdU4iIiPqYSFxHxMZW4iIiPhazEzexVM9tlZqlBHn+emaWZ2WozeztUuUREIomF6jpxMzsBKADecM4lVHFsPPAuMM45t8/MOjjndoUkmIhIBAnZTNw5twjYe+jXzKyPmX1kZivMbLGZDah86E/Ac865fZXfqwIXEQlCXZ8TnwZc65wbAdwMPF/59X5APzNbambfmtmEOs4lIuJL9evqicysGXAcMMPM/v3lhofkiAdOBLoBi8xssHNuf13lExHxozorcSpm/fudc0OP8Fg2sMw5VwpkmdlaKkr9+zrMJyLiO3V2OsU5l0dFQU8CsAqJlQ/PoWIWjpm1o+L0SmZdZRMR8atQXmL4DvAN0N/Mss3scuBC4HIzSwZWA2dUHv4xsMfM0oAvgFucc3tClU1EJFKE7BJDEREJPd2xKSLiYyF5Y7Ndu3auZ8+eofjRIiIRacWKFbudc+2r+30hKfGePXuyfPnyUPxoEZGIZGabavJ9Op0iIuJjKnERER9TiYuI+JhKXETEx1TiIiI+phIXEfExlbiIiI+pxEVEasGKTft48asNdf68KnERkaMQCDheXpzJ+S9+w9vfbeZAcVmdPn9dricuIhJR9h0o4cZ3V/JFRg7jB3bk0XMTadqwbmtVJS4iUgMfr97B/7yXQt7BMu4/YxAXje7BIbuW1RmVuIhINeQXlfLAh+n86/stJHRtwRuXJTKwSwvP8lRZ4mbWH5h+yJd6A3c7554MVSgRkXD0ZcYu7pidwo68Iq78VR9uGB9Pw/oxnmaqssSdcxnAUAAziwG2Au+FNpaISPjILSzl/g/TmLkim74dmjHjyuMY0aO117GA6p9OOQnY4Jyr0ZKJIiJ+88nqHfzPnFT2Hijh6rF9uHZcPI1ivZ19H6q6JT4ZeOdID5jZFGAKQFxc3FHGEhHx1p6CYu6Zt5oPVm3nmM4teO3SkSR0bel1rP8j6D02zawBsA0Y5Jzb+XPHJiUlOW0KISJ+NS95G/fOW01+USnXjYvnyhP7EBsT2ttqzGyFcy6put9XnZn4qcAPVRW4iIhf5RaWcve8VOau3MbQ7q14+Jwh9O/U3OtYP6s6JX4BP3EqRUTE75as283NM5LJKSjmhpP7cfXYPtQP8ey7NgRV4mbWFBgPXBHaOCIidas84Hjk4zW8+FUmfdo3ZdolxzGkWyuvYwUtqBJ3zh0A2oY4i4hIndqRW8T103/k28y9nJ/UnXsnDqJxg/C58iQYumNTRKLSZ+k7uWlGMiVlAR45dwiTRnTz5Lb5o6USF5GoEgg4nvtiPY8tXMugLi145oJh9G7fzOtYNaYSF5GosSGngLvmpPL1hj2cObQLD50zJKxu3KkJlbiIRLzisnJeXpzFk5+upVFsDH89azAXjOruy9Mnh1OJi0hES96yn5tmJLN+VwGnJnTiL2ck0L55Q69j1RqVuIhEpPKA441vNvLX+em0bdqQ1/4wkrH9O3gdq9apxEUk4qzfVcAN01eSsjWXsf3b88T5Q2nVpIHXsUJCJS4iEWVe8jbumJ1CbIzxzAXDOH1I54g49/1TVOIiEhGKSsu5e24q7y7PZlhcK56/cDidWzb2OlbIqcRFxPe27T/IlW+uYFV2LteM7cv1J8f7Yt2T2qASFxHfCgQc7y7fwl/np1MecLx0SRLjB3b0OladUomLiC8VlpRxxT9XsHjdbkb1asPD5wyhV7umXseqcypxEfGdlOxcbpmZzNqd+dx/ZgIXjoqjXr3IffPy56jERcQ3SsoCPLYwg5cWZdK2WUNevXQkJ0bgtd/VoRIXEV/YvKeQq9/+gZStuVwwqju3TTiGlk1ivY7lOZW4iIS9BSnbuXNOKmUBx4sXj+DXgzp5HSlsqMRFJGwVlZYz9cM03vx2M0O6teTx84bSt4N/l40NBZW4iISlDTkFXP3WD6zZkc8VJ/Tm5l/3D/mO834U7B6brYCXgQTAAZc5574JYS4RiWKzf8jmzjmpNIqNidiFq2pLsDPxp4CPnHPnmlkDoEkIM4lIlCosKePuuauZuSKbUb3a8PTkYXRq2cjrWGGtyhI3s5bACcClAM65EqAktLFEJNqkb8/j2nd+ZENOAdedFM914/pGza3zRyOYmXgvIAd4zcwSgRXAfzvnDhx6kJlNAaYAxMXF1XZOEYlQJWUBXvhqA09/to7WTRvw5uXHcnzfdl7H8o1g/jNXHxgO/N05Nww4ANx2+EHOuWnOuSTnXFL79u1rOaaIRKKl63dz2tOLeXzhWiYkdOLj609QgVdTMDPxbCDbObes8vOZHKHERUSCVVhSxl1zVjPrh2y6tmrMa5eOZOwAvXlZE1WWuHNuh5ltMbP+zrkM4CQgLfTRRCQSZezI57/eWkHW7gNcN64vV43t6/sd570U7NUp1wJvVV6Zkgn8IXSRRCQSBQKOV5dm8fjCtTRtWF/nvmtJUCXunFsJJIU2iohEqpz8Ym58dyWL1+1m3IAOTD0zgS6tIn/XnbqgOzZFJGScc8xZuZWpH6RTUFzGQ2cP5vyR3SN6z8u6phIXkZDYd6CEW2etYmHaTobFteKhs4fQv1Nzr2NFHJW4iNS6Hzfv45q3f2RXfhH/c9oxXPaLXsRE6aYNoaYSF5FaU1oe4JnP1vHsF+vp1KIRM648jqHdW3kdK6KpxEWkVqRty+PmGcmkbc/jnOHduGfiQFo00qYNoaYSF5GjUloe4IUvN/D05+to2ThWmzbUMZW4iNRYxo58bp6RTMrWXH6b2IX7Jg6iTdMGXseKKipxEam2svIALy7K5KlP19G8UX3+fuFwTh3c2etYUUklLiLVsm5nxew7OTuX0wZ34v4zEmjbrKHXsaKWSlxEglJWHuCVJVk8tnAtTRvE8OzvhnH6kC5ex4p6KnERqdKuvCKueftHvtu4l1MGduSBswbTvrlm3+FAJS4iP8k5x5cZOdwycxWFJWU8fl4iZw3rqtvmw4hKXESOqKw8wAPz03lt6UbiOzTjrT8eq9vmw5BKXET+jw05Bdz4bjLJW/bz+zE9uOM3x9Cwvtb8DkcqcRH5D+ccby3bzNQP02hYP4ZnLhjGbxP15mU4U4mLCAB7Coq59/003k/exi/j2/HopEQ6tmjkdSypgkpcRPgsfSc3TF/JgZJybhrfj6vH9qWeVh30BZW4SBQLBBzPf7mexxeuZWCXFjxx3lDiO+rNSz8JqsTNbCOQD5QDZc45bdUm4nOHbpn228QuPHj2YJo11LzOb6rzio11zu0OWRIRqTOfr9nJn2elkHewVFum+Zz+sysSRQpLyrh9dgpzV26jX8dmvHm5rv32u2BL3AGfmJkDXnTOTQthJhEJgS17C7nmnR9Jyd7P9SfHc9WJfWlQv57XseQoBVviv3DObTWzDsBCM1vjnFt06AFmNgWYAhAXF1fLMUWkpsoDjn99v5mH5q/BAc9fOIIJCdq0IVIEVeLOua2Vv+8ys/eAUcCiw46ZBkwDSEpKcrWcU0RqYENOATdMX8mq7FxG927D385NpHubJl7HklpUZYmbWVOgnnMuv/LjU4C/hDyZiByVLzN2ccP0lZgZT00eysTELnrzMgIFMxPvCLxX+eLXB952zn0U0lQiUmP5RaU8+nEG//hmE/06NuOlS5Lo0bap17EkRKoscedcJpBYB1lE5CgtTNvJXXNS2ZlfxKXH9eS2UwfQKFYLV0UyXWIoEgH2F5Zwz7zVzF25jf4dm/P3i4YzLK6117GkDqjERXxuybrd3DIzmV35xdxwcj+uGtuH2BhdOhgtVOIiPrW7oJipH6QxZ+U2erdryntXHceQbq28jiV1TCUu4kOL1+Vw47vJ5BaWct24vlw1tq/OfUcplbiIj5SUBXjhqw088ela4js0443LRnFM5xZexxIPqcRFfGLznkKuensFqVvzOGNoFx46ewiNG2j2He1U4iI+8FHqdv48KwXnHC9cNJwJCZ29jiRhQiUuEsbyikqZ+kEa7y7PJqFrC5773XDduCP/i0pcJEyt2LSXq9/6kZ35RVw9tg/Xn9xPlw7K/6ESFwkzB0vK+dvHGbz2dRZdWzVmzlXHk9i9ldexJEypxEXCyHdZe7l1ZjIb9xRy8ege3DqhP80bxXodS8KYSlwkDBwsKeeRj9fw+tcb6da6Me/8aTRj+rT1Opb4gEpcxGOL1+Vwz9zVZO4+wCVjevDnCQNoqg2LJUj6myLikeKych75KINXlmTRo20T3rz8WH4R387rWOIzKnERD2TsyOeG6StJ257HJWN6cMdpx+i2eakRlbhIHSotD/DClxt46rN1tGgcy0uXJDF+YEevY4mPqcRF6sj6XQXcNCOZ5C37+W1iF+6bOIg2TRt4HUt8TiUuEmKBgOOlxZk8tnAtTRrE8MwFw/htYhevY0mEUImLhNDOvCJumbmKRWtzOGVgR6aelUCH5o28jiURJOgSN7MYYDmw1Tl3eugiififc455ydu4c04qpeUBpp6ZwIXHxmm3eal11ZmJ/zeQDmjxYpGfsW3/Qe6ck8rna3YxLK4Vj583lF7ttGiVhEZQJW5m3YDfAA8AN4Y0kYhPOeeY/v0Wpn6YTnnAcedvjuEPx/cipp5m3xI6wc7EnwRuBZr/1AFmNgWYAhAXF3fUwUT85EBxGbfPTmFe8jbG9G7Lw+cMIa5tE69jSRSossTN7HRgl3NuhZmd+FPHOeemAdMAkpKSXG0FFAl3P27exy0zV5GZU8DNp/TjqhP7Uk+zb6kjwczEjwcmmtlpQCOghZm96Zy7KLTRRMJbecDx8uJMHv5oDe2bN+TNPx7LcX1027zUrSpL3Dl3O3A7QOVM/GYVuES7TXsOcPOMZL7fuI8Jgzrxt0lDtGSseELXiYtUg3OOt5Zt5q/z04mpZzx+XiJnDeuqSwfFM9Uqcefcl8CXIUkiEuZ25BZx66yKG3d+Gd+Oh88ZQpdWjb2OJVFOM3GRKgQCjpkrspn6YRql5Y77zxjERaN7aPYtYUElLvIzNu8p5M+zVvFN5h6SerTmb5MSdeOOhBWVuMgRlJUHeG3pRh5bmEFsvXo8ePZgJo/srtm3hB2VuMhh0rblcdvsVazKzuXkYzoy9cwEOrXUolUSnlTiIpXKA45nP1/PM5+vo1WTWJ793TB+M7izZt8S1lTiIlRsl3brrFUkb9nPGUO7cO9vB9FaGzaID6jEJaqVlQd4cVEmT326jmaN6vPU5KFMTOyi2bf4hkpcotb6XfncNKNi9n1qQifuPzOBds0aeh1LpFpU4hJ1AgHHK0uy+NsnGf/ZLu30ITr3Lf6kEpeosmVvITfPSGZZ1l7GD+zIA9ouTXxOJS5RIRBwvP71Rh77JIN6Zjxy7hAmjeim2bf4nkpcIl769jxumZlM6tY8TuzfngfOGkxXrXkiEUIlLhErEHD8/asNPPnpWlo2jtWVJxKRVOISkfKKSrlx+ko+Td/Fb4Z05v4zEmij674lAqnEJeIsXpfDbbNS2JlXxH0TB3HJGK04KJFLJS4Ro6i0nEc+yuDVpVn0ad+U6VeMYUSP1l7HEgkplbhEhPTteVz/r5Vk7Mzn92N6cNupx9C4QYzXsURCTiUuvhYIOF5dmsUjH2XQonEsr/1hJGP7d/A6lkidqbLEzawRsAhoWHn8TOfcPaEOJlKVLXsLuWH6SpZv2sf4gR156OzBtNVt8xJlgpmJFwPjnHMFZhYLLDGzBc65b0OcTeQnzV25ldtnp1DPjEcnJXLOcG1WLNGpyhJ3zjmgoPLT2MpfLpShRH7K3gMl3DU3lQ9XbSepR2uenDyUbq2beB1LxDNBnRM3sxhgBdAXeM45t+wIx0wBpgDExcXVZkYRABam7eT22SnkHizhll/354oTelM/pp7XsUQ8FVSJO+fKgaFm1gp4z8wSnHOphx0zDZgGkJSUpJm61Jrcg6X85f00Zv2QzTGdW/DGZaMY2KWF17FEwkK1rk5xzu03sy+ACUBqVceLHK2v1+/mphnJ7Mov5tpxfbl2XDwN6mv2LfJvwVyd0h4orSzwxsB44OGQJ5OodrCknAcXpPPGN5vo2bYJs/7rOIZ2b+V1LJGwE8xMvDPwj8rz4vWAd51zH4Q2lkSz5C37ueHdlWTmHOCy43tx64T+NIrVjTsiRxLM1SmrgGF1kEWiXHnA8drSLB5csIYOzRvy1h+P5fi+7byOJRLWdMemhIXV23K5Y3YKydm5jB/YkUcnJdKycazXsUTCnkpcPBUION5ctompH6bTsnEsT54/lDOGas1vkWCpxMUzG3cf4M+zVrEsay+/6teex89L1G3zItWkEpc655xj5ops7pqbSmxMPR45ZwiTkrTfpUhNqMSlTm3IKeD2WSl8t3EvI3u25pkLhtOppXabF6kplbjUidLyANMWZfLUZ+toVL8eD549mPOTulOvnmbfIkdDJS4hl5Kdy62zVpG+PY/TBnfi3omD6NBcs2+R2qASl5A5WFLOk5+u5eUlWbRp2oAXLhrBhIROXscSiSgqcQmJbzbs4fbZq9i4p5DJI7tz+2nH6LpvkRBQiUutyisq5cH5a3jnu83EtWnC2388luN016VIyKjEpdZ8snoHd81NJSe/mCkn9OaGk/tps2KREFOJy1HLyS/m3vdX8+Gq7Qzo1JyXLkliSLdWXscSiQoqcTkqH6/ewZ9nraKwuJybxvfjyhP7EKvddkTqjEpcauRAcRkPLVjDP7/dxJBuLXn8vET6dmjudSyRqKMSl2opKQswP2U7T366lk17C7Xet4jHVOIStKXrd3P33FQ25BygT/umvP3H0Yzp09brWCJRTSUuVcreV8hDC9bwwart9GjbhJcuSeKkAR10y7xIGFCJy0/KLSzlwQXpzFiRTUw94/qT47nyV3106kQkjASzUXJ34A2gI+CAac65p0IdTLzjnGN+yg7umbeafYUlXDKmB3/6ZW+6tGrsdTQROUwwM/Ey4Cbn3A9m1hxYYWYLnXNpIc4mHtiZV8Rdc1L5JG0nCV1b8PofRpLQtaXXsUTkJwSzUfJ2YHvlx/lmlg50BVTiEcQ5x4wV2dz/QRolZQFuP3UAl/+iF/V1zbdIWKvWOXEz60nFzvfLjvDYFGAKQFxcXG1kkzqyPfcg981L46PVOxjVqw0PnzOEXu2aeh1LRIIQdImbWTNgFnC9cy7v8Medc9OAaQBJSUmu1hJKSH2UuoPbZq/iYEk5t506gCm/7K2rTkR8JKgSN7NYKgr8Lefc7NBGkrpwsKScB+an8ea3mxnYuQXPXThcs28RHwrm6hQDXgHSnXOPhz6ShNqKTXu5ZeYqMnMO8Kdf9uLWCQO03omITwUzEz8euBhIMbOVlV+7wzk3P2SpJCT2FBTzlw/SmLtyG11aNuLNy4/lF/Fa61vEz4K5OmUJoJOkPrcgZTt3zkklv6iM68b15Ypf9aFpQ93rJeJ3+lcc4Q5d63tw15Y8OimR/p202qBIpFCJR6hAwPHO95t5eMEaikoDWutbJEKpxCNQ2rY87pqbyopN+xjduw0PnDWYPu2beR1LREJAJR5BikrLeXB+Ov/8dhMtG8fy6KREzhnelYoLjEQkEqnEI8SyzD3c/l4KmTkH+P2YHtw4vj8tm8R6HUtEQkwl7nOb9xTy6CcZzEveRrfWjfnn5aP4ZXx7r2OJSB1RifvUgeIynvl8PS8tzqR+PeOasX25amwfmjTQSyoSTfQv3oc+X7OT+95PY9OeQs4d0Y1bft2fji0aeR1LRDygEveRzJwCHvgwnc/W7KJ3+6b8a8poRvfWHpci0Uwl7gPlAceLizbwxMK1NKwfwx2nDeDS43rRoL6u+RaJdirxMLchp4A7ZqewLGsvpyZ04r4zBtGhuU6diEgFlXiYOlhSzitLMnn68/U0rF+PR84dwqQR3XTNt4j8LyrxMLO/sIS3lm3m1SVZ7DlQwmmDO3HvRM2+ReTIVOJhIhBwvLt8Cw9/tIZ9haX8ql97rhnXl5E923gdTUTCmErcY6XlAT5YtY0Xv8pkzY58RvVswz0TBzKoi3aYF5GqqcQ9tHzjXu54L4W1Owvo26EZT5yfyJlDtdaJiARPJe6BzJwCHv0kg/kpO+jSshEvXDSCUwZ21AbFIlJtKvE6dKC4jKc/X8cri7NoUL8e150UzxUn9NYOOyJSY8FslPwqcDqwyzmXEPpIkScQcLy/aht/nZ/OzrxiJo3oxq0TBtC+eUOvo4mIzwUzBXwdeBZ4I7RRIlP69jzunFOxQcPgri15/sIRjOjR2utYIhIhgtkoeZGZ9ayDLBFld0ExT3+2jreXbaZ5o/o8cs4QzhnRjRid9xaRWqSTsbUs92ApryzJ4uXFmRSXBZg8sjs3n9Kf1k0beB1NRCJQrZW4mU0BpgDExcXV1o/1jcKSMl5enMVLizLJLy7jtMGduOmU/trbUkRCqtZK3Dk3DZgGkJSU5Grr54Y75xxfZuRw97xUtuw9yCkDO3LdSfEkdNXNOiISejqdchRSt+bylw/S+C5rr9b3FhFPBHOJ4TvAiUA7M8sG7nHOvRLqYOGsoLiMxz7J4PWvN9K2aQPumziIC0bFaX1vEalzwVydckFdBPGD0vIAry7J4tkv1pNfVMbFo3tw86/707KxdpUXEW/odEoQduQWMf37Lby7fAtb9x9k3IAOXHdSPEO7t/I6mohEOZX4z1i/K58Xv8pkzsqtlJY7ju/blqlnJjB2QAevo4mIACrxI/px8z6e+2IDn6bvpFFsPS4YFcflv+hFj7ZNvY4mIvK/qMQPkZlTwJOfrmNe8jZaN4nlupPi+f2YHrRtpjVORCQ8qcSBXXlFTFuUyatLs2hYP4arTuzD1WP7anVBEQl7Ud1SGTvyeXHRBt5P3kZpuWPyyO7cdEp/rS4oIr4RlSWee7CUpz5dx+tfZ9EoNoYLj+3B74/rSa92OuctIv4SVSVeHnDMWL6Fv32cwd7CEn43Ko5bft2fVk20OJWI+FPUlPjyjXu59/3VpG7NI6lHa/4xcZTWNxER34v4Et+ee5CHFqxh7sptdGrRiKcmD2ViYhdtRiwiESFiS3xHbhGvLs3i9a83AnDN2L5cNbYPTRpE7JBFJApFXKPtyi/i6c/WMf37LZQFHGcN7coN4/vRvU0Tr6OJiNS6iCnxfQdKeGVJFq8syaK0PMD5I7tz5a/6qLxFJKL5vsSLSst5/ov1vLp0IwXFZZw+pDM3n9KfnrpcUESigG9LvKi0nHe+28zzX24gJ7+Y3wzuzHUnxdO/U3Ovo4mI1BnflXh5wDFrRTaPLcxgZ14xo3u34dkLhnGsdtQRkSjkmxJ3zvHV2hweWrCGNTvyGdq9FU+cP5QxvdvqckERiVq+KPHUrbk8uCCdpev3ENemCc/9bjinDe6k8haRqBfWJX6guIwH5qfzznebadk4lrtPH8hFo3toL0sRkUpBlbiZTQCeAmKAl51zD4UyVFl5gJkrsnls4Vp2FxRz2fG9uO6keO1lKSJymGB2u48BngPGA9nA92Y2zzmXFopAyzL3cOecVNbtKmBYXCtevHgEw+Nah+KpRER8L5iZ+ChgvXMuE8DM/gWcAdRqiRcUl3HxK8v4cfN+urRsxIsXj+CUgR113ltE5GcEU+JdgS2HfJ4NHHv4QWY2BZgCEBcXV+0gzRrWp2fbpkxM7MLkkXE0bhBT7Z8hIhJtau2NTefcNGAaQFJSkqvJz3ji/KG1FUdEJCoEc5nHVqD7IZ93q/yaiIh4LJgS/x6IN7NeZtYAmAzMC20sEREJRpWnU5xzZWZ2DfAxFZcYvuqcWx3yZCIiUqWgzok75+YD80OcRUREqkm3PoqI+JhKXETEx1TiIiI+phIXEfExc65G9+X8/A81ywE21fDb2wG7azGOn0Tz2EHjj+bxR/PYoWL8TZ1z7av7jSEp8aNhZsudc0le5/BCNI8dNP5oHn80jx2Obvw6nSIi4mMqcRERHwvHEp/mdQAPRfPYQeOP5vFH89jhKMYfdufERUQkeOE4ExcRkSCpxEVEfMyzEjezCWaWYWbrzey2Izze0MymVz6+zMx6ehAzJIIY+6VmlmNmKyt//dGLnKFgZq+a2S4zS/2Jx83Mnq78s1llZsPrOmMoBTH+E80s95DX/u66zhgqZtbdzL4wszQzW21m/32EYyL29Q9y/NV//Z1zdf6LiiVtNwC9gQZAMjDwsGOuAl6o/HgyMN2LrB6N/VLgWa+zhmj8JwDDgdSfePw0YAFgwGhgmdeZ63j8JwIfeJ0zRGPvDAyv/Lg5sPYIf/cj9vUPcvzVfv29mon/Z/Nl51wJ8O/Nlw91BvCPyo9nAidZZOyaHMzYI5ZzbhGw92cOOQN4w1X4FmhlZp3rJl3oBTH+iOWc2+6c+6Hy43wgnYo9fA8Vsa9/kOOvNq9K/EibLx8+mP8c45wrA3KBtnWSLrSCGTvAOZX/OznTzLof4fFIFeyfTyQbY2bJZrbAzAZ5HSYUKk+PDgOWHfZQVLz+PzN+qObrrzc2w9P7QE/n3BBgIf///0gk8v0A9HDOJQLPAHO8jVP7zKwZMAu43jmX53WeulbF+Kv9+ntV4sFsvvyfY8ysPtAS2FMn6UKryrE75/Y454orP30ZGFFH2cJBVG/M7ZzLc84VVH48H4g1s3Yex6o1ZhZLRYG95ZybfYRDIvr1r2r8NXn9vSrxYDZfngf8vvLjc4HPXeWZf5+rcuyHnQOcSMW5s2gxD7ik8iqF0UCuc26716Hqipl1+vd7P2Y2iop/o5EweaFyXK8A6c65x3/isIh9/YMZf01e/6D22Kxt7ic2XzazvwDLnXPzqBjsP81sPRVvBE32ImttC3Ls15nZRKCMirFf6lngWmZm71DxDnw7M8sG7gFiAZxzL1Cxl+tpwHqgEPiDN0lDI4jxnwv8l5mVAQeByREyeQE4HrgYSDGzlZVfuwOIg6h4/YMZf7Vff912LyLiY3pjU0TEx1TiIiI+phIXEfExlbiIiI+pxEVEjlJVC5sd4fjzDlkI6+2jem5dnSIicnTM7ASggIp1XxKqODYeeBcY55zbZ2YdnHO7avrcmomLiBylIy1sZmZ9zOwjM1thZovNbEDlQ38CnnPO7av83hoXOKjERURCZRpwrXNuBHAz8Hzl1/sB/cxsqZl9a2YTjuZJPLljU0QkklUucnUcMOOQFbQbVv5eH4in4s7dbsAiMxvsnNtfk+dSiYuI1L56wH7n3NAjPJZNxWYXpUCWma2lotS/r+kTiYhILapcYjbLzCbBf7adS6x8eA4Vs3AqVyjsB2TW9LlU4iIiR6lyYbNvgP5mlm1mlwMXApebWTKwmv+/g9fHwB4zSwO+AG5xztV4pUpdYigi4mOaiYuI+JhKXETEx1TiIiI+phIXEfExlbiIiI+pxEVEfEwlLiLiY/8Pilb2lgxoAlsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[data['weight']!= 0].loc[:,'weight'].cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[157.23203851211804, 4604.404854004136, -4447.1728154920165]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_test).numpy()\n",
    "\n",
    "pred_action = (preds > 0) * 1\n",
    "\n",
    "score_func.u2(test_weight, test_resp,np.array(pred_action).reshape(len(pred_action),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_MLP():\n",
    "    hidden_units = 300\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(int(hidden_units/3), activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(1),\n",
    "      tf.keras.layers.Activation(\"sigmoid\") \n",
    "    ])    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = (y_train > 0 ) * 1\n",
    "y_test_binary = (y_test > 0 ) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          1\n",
       "3          0\n",
       "4          1\n",
       "          ..\n",
       "1920546    1\n",
       "1920547    0\n",
       "1920548    1\n",
       "1920549    0\n",
       "1920550    0\n",
       "Name: action, Length: 1920551, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_binary.reset_index(drop=True, inplace =True)\n",
    "y_train_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 10ms/step - loss: 0.7386 - auc: 0.5093\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6989 - auc: 0.5277\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6947 - auc: 0.5338\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6922 - auc: 0.5376\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6910 - auc: 0.5400\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6900 - auc: 0.5424\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6894 - auc: 0.5450\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6886 - auc: 0.5467\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6881 - auc: 0.5489\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6873 - auc: 0.5513\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6866 - auc: 0.5542\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6858 - auc: 0.5566\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6849 - auc: 0.5586\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6843 - auc: 0.5605\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6831 - auc: 0.5635\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6821 - auc: 0.5665\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6808 - auc: 0.5698\n",
      "Epoch 18/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6801 - auc: 0.5715\n",
      "Epoch 19/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6787 - auc: 0.5749\n",
      "Epoch 20/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6786 - auc: 0.5744\n",
      "Epoch 21/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6772 - auc: 0.5787\n",
      "Epoch 22/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6760 - auc: 0.5811\n",
      "Epoch 23/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6752 - auc: 0.5817\n",
      "Epoch 24/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6742 - auc: 0.5845\n",
      "Epoch 25/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6731 - auc: 0.5863\n",
      "Epoch 26/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6722 - auc: 0.5886\n",
      "Epoch 27/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6712 - auc: 0.5900\n",
      "Epoch 28/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6707 - auc: 0.5917\n",
      "Epoch 29/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6700 - auc: 0.5930\n",
      "Epoch 30/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6694 - auc: 0.5948\n",
      "Epoch 31/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6686 - auc: 0.5962\n",
      "Epoch 32/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6678 - auc: 0.5977\n",
      "Epoch 33/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6673 - auc: 0.5990\n",
      "Epoch 34/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6662 - auc: 0.6012\n",
      "Epoch 35/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6659 - auc: 0.6010\n",
      "Epoch 36/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6653 - auc: 0.6023\n",
      "Epoch 37/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6646 - auc: 0.6036\n",
      "Epoch 38/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6642 - auc: 0.6042\n",
      "Epoch 39/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6640 - auc: 0.6048\n",
      "Epoch 40/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6628 - auc: 0.6072\n",
      "Epoch 41/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6625 - auc: 0.6078\n",
      "Epoch 42/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6622 - auc: 0.6085\n",
      "Epoch 43/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6615 - auc: 0.6093\n",
      "Epoch 44/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6612 - auc: 0.6102\n",
      "Epoch 45/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6604 - auc: 0.6116\n",
      "Epoch 46/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6607 - auc: 0.6110\n",
      "Epoch 47/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6598 - auc: 0.6129\n",
      "Epoch 48/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6596 - auc: 0.6139\n",
      "Epoch 49/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6589 - auc: 0.6144\n",
      "Epoch 50/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6582 - auc: 0.6158\n",
      "Epoch 1/50\n",
      "190/190 [==============================] - 3s 9ms/step - loss: 0.7386 - auc: 0.5096\n",
      "Epoch 2/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6986 - auc: 0.5259\n",
      "Epoch 3/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6940 - auc: 0.5346\n",
      "Epoch 4/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6924 - auc: 0.5381\n",
      "Epoch 5/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6910 - auc: 0.5402\n",
      "Epoch 6/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6900 - auc: 0.5431\n",
      "Epoch 7/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6892 - auc: 0.5459\n",
      "Epoch 8/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6886 - auc: 0.5473\n",
      "Epoch 9/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6876 - auc: 0.5501\n",
      "Epoch 10/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6869 - auc: 0.5525\n",
      "Epoch 11/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.6860 - auc: 0.5553\n",
      "Epoch 12/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6851 - auc: 0.5574\n",
      "Epoch 13/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6841 - auc: 0.5607\n",
      "Epoch 14/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6829 - auc: 0.5637\n",
      "Epoch 15/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6821 - auc: 0.5653\n",
      "Epoch 16/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6811 - auc: 0.5677\n",
      "Epoch 17/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6800 - auc: 0.5704\n",
      "Epoch 18/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6789 - auc: 0.5733\n",
      "Epoch 19/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6778 - auc: 0.5761\n",
      "Epoch 20/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6772 - auc: 0.5775\n",
      "Epoch 21/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6759 - auc: 0.5802\n",
      "Epoch 22/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6748 - auc: 0.5825\n",
      "Epoch 23/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6740 - auc: 0.5845\n",
      "Epoch 24/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6731 - auc: 0.5863\n",
      "Epoch 25/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6724 - auc: 0.5876\n",
      "Epoch 26/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6715 - auc: 0.5903\n",
      "Epoch 27/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6707 - auc: 0.5915\n",
      "Epoch 28/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.6697 - auc: 0.5937\n",
      "Epoch 29/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6693 - auc: 0.5946\n",
      "Epoch 30/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6683 - auc: 0.5967\n",
      "Epoch 31/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6675 - auc: 0.5983\n",
      "Epoch 32/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6672 - auc: 0.5980\n",
      "Epoch 33/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6663 - auc: 0.6005\n",
      "Epoch 34/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6655 - auc: 0.6023\n",
      "Epoch 35/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6653 - auc: 0.6029\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6647 - auc: 0.6038\n",
      "Epoch 37/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6641 - auc: 0.6050\n",
      "Epoch 38/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6635 - auc: 0.6062\n",
      "Epoch 39/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6632 - auc: 0.6064\n",
      "Epoch 40/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6627 - auc: 0.6082\n",
      "Epoch 41/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6621 - auc: 0.6086\n",
      "Epoch 42/50\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6617 - auc: 0.6097\n",
      "Epoch 43/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.6609 - auc: 0.6106\n",
      "Epoch 44/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.6604 - auc: 0.6123\n",
      "Epoch 45/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.6602 - auc: 0.6125\n",
      "Epoch 46/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.6595 - auc: 0.6134\n",
      "Epoch 47/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.6594 - auc: 0.6140\n",
      "Epoch 48/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.6587 - auc: 0.6152\n",
      "Epoch 49/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.6584 - auc: 0.6159\n",
      "Epoch 50/50\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.6584 - auc: 0.6162\n",
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 9ms/step - loss: 0.7418 - auc: 0.5103\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6986 - auc: 0.5265\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6940 - auc: 0.5346\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6921 - auc: 0.5386\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6910 - auc: 0.5414\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6899 - auc: 0.5436\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6891 - auc: 0.5462\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6882 - auc: 0.5494\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6875 - auc: 0.5511\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6866 - auc: 0.5550\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6858 - auc: 0.5569\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6848 - auc: 0.5605\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6839 - auc: 0.5620\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6827 - auc: 0.5658\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6818 - auc: 0.5674\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6805 - auc: 0.5706\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6795 - auc: 0.5731\n",
      "Epoch 18/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6784 - auc: 0.5748\n",
      "Epoch 19/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6772 - auc: 0.5782\n",
      "Epoch 20/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6764 - auc: 0.5799\n",
      "Epoch 21/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6752 - auc: 0.5831\n",
      "Epoch 22/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6743 - auc: 0.5844\n",
      "Epoch 23/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6730 - auc: 0.5875\n",
      "Epoch 24/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6722 - auc: 0.5890\n",
      "Epoch 25/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6714 - auc: 0.5907\n",
      "Epoch 26/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6705 - auc: 0.5927\n",
      "Epoch 27/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6699 - auc: 0.5935\n",
      "Epoch 28/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6690 - auc: 0.5956\n",
      "Epoch 29/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6683 - auc: 0.5965\n",
      "Epoch 30/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6674 - auc: 0.5986\n",
      "Epoch 31/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6667 - auc: 0.6004\n",
      "Epoch 32/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6660 - auc: 0.6018\n",
      "Epoch 33/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6653 - auc: 0.6027\n",
      "Epoch 34/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6648 - auc: 0.6035\n",
      "Epoch 35/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6644 - auc: 0.6045\n",
      "Epoch 36/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6639 - auc: 0.6054\n",
      "Epoch 37/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6633 - auc: 0.6070\n",
      "Epoch 38/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6625 - auc: 0.6079\n",
      "Epoch 39/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6620 - auc: 0.6091\n",
      "Epoch 40/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6613 - auc: 0.6102\n",
      "Epoch 41/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6611 - auc: 0.6105\n",
      "Epoch 42/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6608 - auc: 0.6114\n",
      "Epoch 43/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6599 - auc: 0.6128\n",
      "Epoch 44/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6595 - auc: 0.6140\n",
      "Epoch 45/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6593 - auc: 0.6149\n",
      "Epoch 46/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6592 - auc: 0.6144\n",
      "Epoch 47/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6586 - auc: 0.6157\n",
      "Epoch 48/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6577 - auc: 0.6171\n",
      "Epoch 49/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6574 - auc: 0.6181\n",
      "Epoch 50/50\n",
      "193/193 [==============================] - 2s 10ms/step - loss: 0.6575 - auc: 0.6180\n",
      "Epoch 1/50\n",
      "191/191 [==============================] - 3s 10ms/step - loss: 0.7366 - auc: 0.5090\n",
      "Epoch 2/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6981 - auc: 0.5283\n",
      "Epoch 3/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6943 - auc: 0.5350\n",
      "Epoch 4/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6924 - auc: 0.5383\n",
      "Epoch 5/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6910 - auc: 0.5405\n",
      "Epoch 6/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6901 - auc: 0.5438\n",
      "Epoch 7/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6894 - auc: 0.5453\n",
      "Epoch 8/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6888 - auc: 0.5475\n",
      "Epoch 9/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6879 - auc: 0.5512\n",
      "Epoch 10/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6873 - auc: 0.5521\n",
      "Epoch 11/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6865 - auc: 0.5551\n",
      "Epoch 12/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6858 - auc: 0.5572\n",
      "Epoch 13/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6849 - auc: 0.5592\n",
      "Epoch 14/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6840 - auc: 0.5619\n",
      "Epoch 15/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6832 - auc: 0.5634\n",
      "Epoch 16/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6822 - auc: 0.5662\n",
      "Epoch 17/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6818 - auc: 0.5672\n",
      "Epoch 18/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6805 - auc: 0.5704\n",
      "Epoch 19/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6793 - auc: 0.5730\n",
      "Epoch 20/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6785 - auc: 0.5757\n",
      "Epoch 21/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6776 - auc: 0.5771\n",
      "Epoch 22/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6769 - auc: 0.5788\n",
      "Epoch 23/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6760 - auc: 0.5809\n",
      "Epoch 24/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6750 - auc: 0.5834\n",
      "Epoch 25/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6742 - auc: 0.5847\n",
      "Epoch 26/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6734 - auc: 0.5868\n",
      "Epoch 27/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6728 - auc: 0.5883\n",
      "Epoch 28/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6721 - auc: 0.5891\n",
      "Epoch 29/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6714 - auc: 0.5917\n",
      "Epoch 30/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6703 - auc: 0.5933\n",
      "Epoch 31/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6696 - auc: 0.5948\n",
      "Epoch 32/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6696 - auc: 0.5945\n",
      "Epoch 33/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6681 - auc: 0.5977\n",
      "Epoch 34/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6675 - auc: 0.5992\n",
      "Epoch 35/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6671 - auc: 0.6000\n",
      "Epoch 36/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6666 - auc: 0.6014\n",
      "Epoch 37/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6661 - auc: 0.6020\n",
      "Epoch 38/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6658 - auc: 0.6027\n",
      "Epoch 39/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6647 - auc: 0.6046\n",
      "Epoch 40/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6648 - auc: 0.6048\n",
      "Epoch 41/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6641 - auc: 0.6059\n",
      "Epoch 42/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6634 - auc: 0.6071\n",
      "Epoch 43/50\n",
      "191/191 [==============================] - 2s 10ms/step - loss: 0.6629 - auc: 0.6087\n",
      "Epoch 44/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.6622 - auc: 0.6089\n",
      "Epoch 45/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.6621 - auc: 0.6101\n",
      "Epoch 46/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.6617 - auc: 0.6108\n",
      "Epoch 47/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.6611 - auc: 0.6112\n",
      "Epoch 48/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.6606 - auc: 0.6122\n",
      "Epoch 49/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.6604 - auc: 0.6129\n",
      "Epoch 50/50\n",
      "191/191 [==============================] - 2s 9ms/step - loss: 0.6597 - auc: 0.6145\n",
      "Epoch 1/50\n",
      "193/193 [==============================] - 4s 9ms/step - loss: 0.7345 - auc: 0.5113\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6979 - auc: 0.5299\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6938 - auc: 0.5363\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6918 - auc: 0.5412\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6907 - auc: 0.5432\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6893 - auc: 0.5466\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6887 - auc: 0.5481\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6878 - auc: 0.5517\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6871 - auc: 0.5538\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6861 - auc: 0.5567\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6852 - auc: 0.5589\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6842 - auc: 0.5620\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6831 - auc: 0.5638\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6818 - auc: 0.5678\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6808 - auc: 0.5703\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6800 - auc: 0.5714\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6786 - auc: 0.5751\n",
      "Epoch 18/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6779 - auc: 0.5770\n",
      "Epoch 19/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6769 - auc: 0.5796\n",
      "Epoch 20/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6756 - auc: 0.5814\n",
      "Epoch 21/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6749 - auc: 0.5835\n",
      "Epoch 22/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6740 - auc: 0.5855\n",
      "Epoch 23/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6731 - auc: 0.5871\n",
      "Epoch 24/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6721 - auc: 0.5891\n",
      "Epoch 25/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6714 - auc: 0.5907\n",
      "Epoch 26/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6703 - auc: 0.5931\n",
      "Epoch 27/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6697 - auc: 0.5944\n",
      "Epoch 28/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6687 - auc: 0.5969\n",
      "Epoch 29/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6681 - auc: 0.5975\n",
      "Epoch 30/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6675 - auc: 0.5992\n",
      "Epoch 31/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6668 - auc: 0.6001\n",
      "Epoch 32/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6662 - auc: 0.6017\n",
      "Epoch 33/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6657 - auc: 0.6028\n",
      "Epoch 34/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6653 - auc: 0.6036\n",
      "Epoch 35/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6645 - auc: 0.6043\n",
      "Epoch 36/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6636 - auc: 0.6060\n",
      "Epoch 37/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6629 - auc: 0.6077\n",
      "Epoch 38/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6623 - auc: 0.6087\n",
      "Epoch 39/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6624 - auc: 0.6089\n",
      "Epoch 40/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6617 - auc: 0.6103\n",
      "Epoch 41/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6610 - auc: 0.6111\n",
      "Epoch 42/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6606 - auc: 0.6125\n",
      "Epoch 43/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6602 - auc: 0.6129\n",
      "Epoch 44/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6597 - auc: 0.6139\n",
      "Epoch 45/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6591 - auc: 0.6152\n",
      "Epoch 46/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6590 - auc: 0.6153\n",
      "Epoch 47/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6585 - auc: 0.6163\n",
      "Epoch 48/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6583 - auc: 0.6166\n",
      "Epoch 49/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6579 - auc: 0.6171\n",
      "Epoch 50/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.6578 - auc: 0.6175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[24.794042670876483, 7264.7613597246345, -7239.967317053757],\n",
       " [47.02955462027041, 7857.603320014103, -7810.573765393834],\n",
       " [-99.09501055762343, 7182.200775473806, -7281.295786031427],\n",
       " [-523.7950032236475, 7738.790907038392, -8262.585910262038],\n",
       " [57.072563847940486, 7226.4505280095245, -7169.377964161585]]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = 0 \n",
    "binary_scores = [] \n",
    "for fold in range(0,5):\n",
    "    features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "\n",
    "    target = 'action'\n",
    "    mask = ret_kfold.ret_fold_mask(fold)\n",
    "\n",
    "    x_train, y_train, x_test, y_test, test_weight, test_resp = ret_kfold.kfold_split(data,mask,features,target)\n",
    "\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "\n",
    "    model = binary_MLP()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['AUC'])\n",
    "    model.fit(x_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1 ,callbacks= [earlystopping])\n",
    "    #model.evaluate(x_test, y_test_binary)\n",
    "    preds = model(x_test.values).numpy()\n",
    "    preds = (preds > 0 ) * 1\n",
    "    binary_scores.append(score_func.u2(test_weight, test_resp,preds.reshape(len(preds),) ))\n",
    "    \n",
    "binary_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(x_test.values).numpy()\n",
    "preds = (preds > 0 ) * 1\n",
    "score_func.u2(test_weight, test_resp,preds.reshape(len(preds),) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ret_kfold' from '/home/x99e/gitgood/legendary-robot/kaggle_jane_street/ret_kfold.py'>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(score_func)\n",
    "imp.reload(ret_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.794042670876483, 7264.7613597246345, -7239.967317053757]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.u2(test_weight, test_resp,preds.reshape(len(preds),) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.40521674795502"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "pred_train = probability_model(x_train.values)\n",
    "pred_test = probability_model(x_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_bins = [] \n",
    "for n_pred in pred_test:\n",
    "    pred_bins.append(np.argmax(n_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.821427</td>\n",
       "      <td>1</td>\n",
       "      <td>5.285973</td>\n",
       "      <td>3.011525</td>\n",
       "      <td>-1.393305</td>\n",
       "      <td>-0.735402</td>\n",
       "      <td>-0.908020</td>\n",
       "      <td>-0.471151</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>4.728494</td>\n",
       "      <td>5.309723</td>\n",
       "      <td>3.199164</td>\n",
       "      <td>4.911131</td>\n",
       "      <td>4.783926</td>\n",
       "      <td>4.397508</td>\n",
       "      <td>5.122683</td>\n",
       "      <td>4.998204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.838150</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>1.068607</td>\n",
       "      <td>0.506972</td>\n",
       "      <td>1.252115</td>\n",
       "      <td>0.665742</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>1.998274</td>\n",
       "      <td>3.805928</td>\n",
       "      <td>3.327486</td>\n",
       "      <td>8.298933</td>\n",
       "      <td>4.331993</td>\n",
       "      <td>6.636454</td>\n",
       "      <td>2.742866</td>\n",
       "      <td>4.421242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.115654</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.995780</td>\n",
       "      <td>-2.397085</td>\n",
       "      <td>0.631661</td>\n",
       "      <td>0.260796</td>\n",
       "      <td>-0.606878</td>\n",
       "      <td>-0.318215</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.571110</td>\n",
       "      <td>1.961126</td>\n",
       "      <td>0.115093</td>\n",
       "      <td>1.678828</td>\n",
       "      <td>0.269419</td>\n",
       "      <td>1.380691</td>\n",
       "      <td>0.414362</td>\n",
       "      <td>1.482215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.787618</td>\n",
       "      <td>-2.193015</td>\n",
       "      <td>-0.158265</td>\n",
       "      <td>-0.120893</td>\n",
       "      <td>-0.255401</td>\n",
       "      <td>-0.169363</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>2.002705</td>\n",
       "      <td>4.953593</td>\n",
       "      <td>1.173338</td>\n",
       "      <td>4.815103</td>\n",
       "      <td>1.706985</td>\n",
       "      <td>3.908004</td>\n",
       "      <td>1.612412</td>\n",
       "      <td>3.702609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.712978</td>\n",
       "      <td>-2.196925</td>\n",
       "      <td>0.280446</td>\n",
       "      <td>0.115972</td>\n",
       "      <td>0.567565</td>\n",
       "      <td>0.311161</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.029607</td>\n",
       "      <td>3.311418</td>\n",
       "      <td>0.615229</td>\n",
       "      <td>6.732394</td>\n",
       "      <td>0.774046</td>\n",
       "      <td>5.262576</td>\n",
       "      <td>0.132370</td>\n",
       "      <td>3.440806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920546</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.649365</td>\n",
       "      <td>-1.169996</td>\n",
       "      <td>-0.889129</td>\n",
       "      <td>-1.256179</td>\n",
       "      <td>-0.265419</td>\n",
       "      <td>-0.383478</td>\n",
       "      <td>0.526201</td>\n",
       "      <td>1.162469</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.421753</td>\n",
       "      <td>-1.896874</td>\n",
       "      <td>-1.260055</td>\n",
       "      <td>1.947725</td>\n",
       "      <td>-1.994399</td>\n",
       "      <td>-1.685163</td>\n",
       "      <td>-2.866165</td>\n",
       "      <td>-0.216130</td>\n",
       "      <td>-1.892048</td>\n",
       "      <td>0.901585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920547</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.432943</td>\n",
       "      <td>5.284504</td>\n",
       "      <td>-0.337469</td>\n",
       "      <td>-0.494263</td>\n",
       "      <td>-0.442409</td>\n",
       "      <td>-0.739016</td>\n",
       "      <td>-0.064645</td>\n",
       "      <td>0.163244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.677511</td>\n",
       "      <td>-0.936553</td>\n",
       "      <td>1.064936</td>\n",
       "      <td>3.119762</td>\n",
       "      <td>-0.419796</td>\n",
       "      <td>-0.208975</td>\n",
       "      <td>-0.146749</td>\n",
       "      <td>0.730166</td>\n",
       "      <td>0.648452</td>\n",
       "      <td>2.068737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920548</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.622475</td>\n",
       "      <td>-0.963682</td>\n",
       "      <td>0.532835</td>\n",
       "      <td>0.392287</td>\n",
       "      <td>0.977046</td>\n",
       "      <td>0.819693</td>\n",
       "      <td>0.140248</td>\n",
       "      <td>0.039213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459167</td>\n",
       "      <td>-2.956745</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>-2.279663</td>\n",
       "      <td>-0.950259</td>\n",
       "      <td>-4.388417</td>\n",
       "      <td>-1.669922</td>\n",
       "      <td>-3.288939</td>\n",
       "      <td>-1.336142</td>\n",
       "      <td>-2.814239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920549</th>\n",
       "      <td>0.283405</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.463757</td>\n",
       "      <td>-1.107228</td>\n",
       "      <td>-2.286985</td>\n",
       "      <td>-3.156451</td>\n",
       "      <td>-1.690676</td>\n",
       "      <td>-2.348199</td>\n",
       "      <td>-0.683812</td>\n",
       "      <td>-0.939522</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.651236</td>\n",
       "      <td>-2.035894</td>\n",
       "      <td>-1.780962</td>\n",
       "      <td>0.881246</td>\n",
       "      <td>-2.202140</td>\n",
       "      <td>-1.912601</td>\n",
       "      <td>-3.341684</td>\n",
       "      <td>-0.571188</td>\n",
       "      <td>-2.185795</td>\n",
       "      <td>0.627452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920550</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.817184</td>\n",
       "      <td>-1.131577</td>\n",
       "      <td>0.541893</td>\n",
       "      <td>0.998988</td>\n",
       "      <td>0.412844</td>\n",
       "      <td>0.798855</td>\n",
       "      <td>0.198607</td>\n",
       "      <td>0.752480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.983979</td>\n",
       "      <td>-0.571013</td>\n",
       "      <td>2.483421</td>\n",
       "      <td>8.284037</td>\n",
       "      <td>-0.698486</td>\n",
       "      <td>0.199953</td>\n",
       "      <td>-0.168395</td>\n",
       "      <td>2.051091</td>\n",
       "      <td>1.726072</td>\n",
       "      <td>5.823676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920551 rows Ã— 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           weight  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0        9.821427          1   5.285973   3.011525  -1.393305  -0.735402   \n",
       "1        0.838150          1  -3.172026  -3.093182   1.068607   0.506972   \n",
       "2        0.115654         -1  -1.995780  -2.397085   0.631661   0.260796   \n",
       "3        0.000000          1  -1.787618  -2.193015  -0.158265  -0.120893   \n",
       "4        0.000000         -1  -1.712978  -2.196925   0.280446   0.115972   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "1920546  0.000000          1  -1.649365  -1.169996  -0.889129  -1.256179   \n",
       "1920547  0.000000          1   2.432943   5.284504  -0.337469  -0.494263   \n",
       "1920548  0.000000          1  -0.622475  -0.963682   0.532835   0.392287   \n",
       "1920549  0.283405         -1  -1.463757  -1.107228  -2.286985  -3.156451   \n",
       "1920550  0.000000         -1  -1.817184  -1.131577   0.541893   0.998988   \n",
       "\n",
       "         feature_5  feature_6  feature_7  feature_8  ...  feature_120  \\\n",
       "0        -0.908020  -0.471151   0.051777   0.026828  ...     0.335127   \n",
       "1         1.252115   0.665742   0.051777   0.026828  ...     0.335127   \n",
       "2        -0.606878  -0.318215   0.051777   0.026828  ...     0.335127   \n",
       "3        -0.255401  -0.169363   0.051777   0.026828  ...     0.335127   \n",
       "4         0.567565   0.311161   0.051777   0.026828  ...     0.335127   \n",
       "...            ...        ...        ...        ...  ...          ...   \n",
       "1920546  -0.265419  -0.383478   0.526201   1.162469  ...    -2.421753   \n",
       "1920547  -0.442409  -0.739016  -0.064645   0.163244  ...    -0.677511   \n",
       "1920548   0.977046   0.819693   0.140248   0.039213  ...    -0.459167   \n",
       "1920549  -1.690676  -2.348199  -0.683812  -0.939522  ...    -2.651236   \n",
       "1920550   0.412844   0.798855   0.198607   0.752480  ...    -0.983979   \n",
       "\n",
       "         feature_121  feature_122  feature_123  feature_124  feature_125  \\\n",
       "0           0.268776     4.728494     5.309723     3.199164     4.911131   \n",
       "1           0.268776     1.998274     3.805928     3.327486     8.298933   \n",
       "2           0.268776     0.571110     1.961126     0.115093     1.678828   \n",
       "3           0.268776     2.002705     4.953593     1.173338     4.815103   \n",
       "4           0.268776     0.029607     3.311418     0.615229     6.732394   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "1920546    -1.896874    -1.260055     1.947725    -1.994399    -1.685163   \n",
       "1920547    -0.936553     1.064936     3.119762    -0.419796    -0.208975   \n",
       "1920548    -2.956745    -0.640334    -2.279663    -0.950259    -4.388417   \n",
       "1920549    -2.035894    -1.780962     0.881246    -2.202140    -1.912601   \n",
       "1920550    -0.571013     2.483421     8.284037    -0.698486     0.199953   \n",
       "\n",
       "         feature_126  feature_127  feature_128  feature_129  \n",
       "0           4.783926     4.397508     5.122683     4.998204  \n",
       "1           4.331993     6.636454     2.742866     4.421242  \n",
       "2           0.269419     1.380691     0.414362     1.482215  \n",
       "3           1.706985     3.908004     1.612412     3.702609  \n",
       "4           0.774046     5.262576     0.132370     3.440806  \n",
       "...              ...          ...          ...          ...  \n",
       "1920546    -2.866165    -0.216130    -1.892048     0.901585  \n",
       "1920547    -0.146749     0.730166     0.648452     2.068737  \n",
       "1920548    -1.669922    -3.288939    -1.336142    -2.814239  \n",
       "1920549    -3.341684    -0.571188    -2.185795     0.627452  \n",
       "1920550    -0.168395     2.051091     1.726072     5.823676  \n",
       "\n",
       "[1920551 rows x 131 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 9ms/step - loss: 2.1146 - accuracy: 0.2657\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.3526 - accuracy: 0.3455\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.3177 - accuracy: 0.3572\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2993 - accuracy: 0.3663\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2864 - accuracy: 0.3744\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2754 - accuracy: 0.3816\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2642 - accuracy: 0.3881\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2556 - accuracy: 0.3937\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2477 - accuracy: 0.3989\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2393 - accuracy: 0.4031\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2321 - accuracy: 0.4077\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2250 - accuracy: 0.4119\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2185 - accuracy: 0.4157\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2131 - accuracy: 0.4191\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2077 - accuracy: 0.4222\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.2031 - accuracy: 0.4247\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1972 - accuracy: 0.4280\n",
      "Epoch 18/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1925 - accuracy: 0.4309\n",
      "Epoch 19/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1889 - accuracy: 0.4333\n",
      "Epoch 20/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1845 - accuracy: 0.4357\n",
      "Epoch 21/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1826 - accuracy: 0.4364\n",
      "Epoch 22/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1777 - accuracy: 0.4397\n",
      "Epoch 23/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1769 - accuracy: 0.4398\n",
      "Epoch 24/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1724 - accuracy: 0.4426\n",
      "Epoch 25/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1698 - accuracy: 0.4440\n",
      "Epoch 26/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1674 - accuracy: 0.4459\n",
      "Epoch 27/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1658 - accuracy: 0.4460\n",
      "Epoch 28/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1624 - accuracy: 0.4489\n",
      "Epoch 29/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1596 - accuracy: 0.4502\n",
      "Epoch 30/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1583 - accuracy: 0.4509\n",
      "Epoch 31/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1569 - accuracy: 0.4520\n",
      "Epoch 32/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1552 - accuracy: 0.4527\n",
      "Epoch 33/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1539 - accuracy: 0.4534\n",
      "Epoch 34/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1515 - accuracy: 0.4548\n",
      "Epoch 35/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1495 - accuracy: 0.4554\n",
      "Epoch 36/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1483 - accuracy: 0.4569\n",
      "Epoch 37/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1470 - accuracy: 0.4578\n",
      "Epoch 38/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1450 - accuracy: 0.4588\n",
      "Epoch 39/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1429 - accuracy: 0.4593\n",
      "Epoch 40/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1421 - accuracy: 0.4597\n",
      "Epoch 41/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1404 - accuracy: 0.4617\n",
      "Epoch 42/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1398 - accuracy: 0.4608\n",
      "Epoch 43/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1378 - accuracy: 0.4628\n",
      "Epoch 44/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1369 - accuracy: 0.4633\n",
      "Epoch 45/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1351 - accuracy: 0.4642\n",
      "Epoch 46/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1354 - accuracy: 0.4637\n",
      "Epoch 47/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1337 - accuracy: 0.4652\n",
      "Epoch 48/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1318 - accuracy: 0.4665\n",
      "Epoch 49/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1300 - accuracy: 0.4672\n",
      "Epoch 50/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 1.1307 - accuracy: 0.4666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7141a23250>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seq_MLP():\n",
    "    hidden_units = 300\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(int(hidden_units/3), activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(10)\n",
    "    ])    \n",
    "    \n",
    "    return model\n",
    "\n",
    "fold = 0 \n",
    "features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "\n",
    "target = 'vol_bin'\n",
    "mask = ret_kfold.ret_fold_mask(fold)\n",
    "\n",
    "x_train, y_train, x_test, y_test, test_weight, test_resp = ret_kfold.kfold_split(data,mask,features,target)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vol_model = seq_MLP()\n",
    "vol_model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "vol_model.fit(x_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1 ,callbacks= [earlystopping])\n",
    "#model.evaluate(x_test, y_test_binary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vol_embed = vol_model(x_train.values).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fold = 0 \n",
    "features = ['weight']+[c for c in data.columns if \"feature\" in c]\n",
    "\n",
    "target = 'resp'\n",
    "mask = ret_kfold.ret_fold_mask(fold)\n",
    "\n",
    "x_train, y_train, x_test, y_test, test_weight, test_resp = ret_kfold.kfold_split(data,mask,features,target)\n",
    "\n",
    "\n",
    "vol_embed = vol_model(x_train.values).numpy()\n",
    "x_train = pd.concat([x_train,pd.DataFrame(vol_embed)], axis = 1)\n",
    "\n",
    "vol_embed = vol_model(x_test.values).numpy()\n",
    "x_test = pd.concat([x_test,pd.DataFrame(vol_embed)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.872746</td>\n",
       "      <td>-2.191242</td>\n",
       "      <td>-0.474163</td>\n",
       "      <td>-0.323046</td>\n",
       "      <td>0.014688</td>\n",
       "      <td>-0.002484</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.729607</td>\n",
       "      <td>3.403493</td>\n",
       "      <td>4.486810</td>\n",
       "      <td>5.528064</td>\n",
       "      <td>-16.217875</td>\n",
       "      <td>-16.603025</td>\n",
       "      <td>-16.146721</td>\n",
       "      <td>-15.583063</td>\n",
       "      <td>-15.994396</td>\n",
       "      <td>-16.221869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.673515</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.349537</td>\n",
       "      <td>-1.704709</td>\n",
       "      <td>0.068058</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.193794</td>\n",
       "      <td>0.138212</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.214633</td>\n",
       "      <td>4.076755</td>\n",
       "      <td>4.531205</td>\n",
       "      <td>4.637632</td>\n",
       "      <td>-11.385442</td>\n",
       "      <td>-11.425260</td>\n",
       "      <td>-11.386423</td>\n",
       "      <td>-11.858445</td>\n",
       "      <td>-11.746490</td>\n",
       "      <td>-11.593060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.812780</td>\n",
       "      <td>-0.256156</td>\n",
       "      <td>0.806463</td>\n",
       "      <td>0.400221</td>\n",
       "      <td>-0.614188</td>\n",
       "      <td>-0.354800</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.557886</td>\n",
       "      <td>2.472301</td>\n",
       "      <td>2.713197</td>\n",
       "      <td>3.681982</td>\n",
       "      <td>-9.799013</td>\n",
       "      <td>-10.127969</td>\n",
       "      <td>-9.970935</td>\n",
       "      <td>-9.792596</td>\n",
       "      <td>-10.142966</td>\n",
       "      <td>-10.175447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.174378</td>\n",
       "      <td>0.344640</td>\n",
       "      <td>0.066872</td>\n",
       "      <td>0.009357</td>\n",
       "      <td>-1.006373</td>\n",
       "      <td>-0.676458</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.467654</td>\n",
       "      <td>2.517851</td>\n",
       "      <td>2.537681</td>\n",
       "      <td>2.423693</td>\n",
       "      <td>-7.305463</td>\n",
       "      <td>-7.262433</td>\n",
       "      <td>-7.371759</td>\n",
       "      <td>-7.498514</td>\n",
       "      <td>-7.515182</td>\n",
       "      <td>-7.674811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138531</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>-0.161518</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.195006</td>\n",
       "      <td>-0.143780</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.082304</td>\n",
       "      <td>5.234684</td>\n",
       "      <td>5.561175</td>\n",
       "      <td>6.529605</td>\n",
       "      <td>-14.687175</td>\n",
       "      <td>-14.488884</td>\n",
       "      <td>-13.920571</td>\n",
       "      <td>-14.798549</td>\n",
       "      <td>-14.468530</td>\n",
       "      <td>-14.522084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469935</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.953012</td>\n",
       "      <td>-1.825455</td>\n",
       "      <td>1.167282</td>\n",
       "      <td>1.407730</td>\n",
       "      <td>0.604182</td>\n",
       "      <td>0.745740</td>\n",
       "      <td>0.533140</td>\n",
       "      <td>0.860828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.249876</td>\n",
       "      <td>4.889501</td>\n",
       "      <td>5.061232</td>\n",
       "      <td>6.782163</td>\n",
       "      <td>-20.332289</td>\n",
       "      <td>-22.976454</td>\n",
       "      <td>-20.284576</td>\n",
       "      <td>-22.883825</td>\n",
       "      <td>-21.512280</td>\n",
       "      <td>-22.170584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469936</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.093580</td>\n",
       "      <td>-0.308872</td>\n",
       "      <td>-1.175020</td>\n",
       "      <td>-1.802869</td>\n",
       "      <td>-0.266547</td>\n",
       "      <td>-0.411329</td>\n",
       "      <td>-0.622232</td>\n",
       "      <td>-0.924464</td>\n",
       "      <td>...</td>\n",
       "      <td>2.837904</td>\n",
       "      <td>5.756587</td>\n",
       "      <td>1.978915</td>\n",
       "      <td>10.882157</td>\n",
       "      <td>-30.849245</td>\n",
       "      <td>-31.941072</td>\n",
       "      <td>-32.235985</td>\n",
       "      <td>-36.884075</td>\n",
       "      <td>-35.956364</td>\n",
       "      <td>-35.413586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469937</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.140352</td>\n",
       "      <td>-0.681436</td>\n",
       "      <td>-0.155998</td>\n",
       "      <td>-0.173413</td>\n",
       "      <td>1.272839</td>\n",
       "      <td>1.825249</td>\n",
       "      <td>0.102961</td>\n",
       "      <td>0.315304</td>\n",
       "      <td>...</td>\n",
       "      <td>2.105800</td>\n",
       "      <td>4.126677</td>\n",
       "      <td>2.191408</td>\n",
       "      <td>2.860481</td>\n",
       "      <td>-13.266150</td>\n",
       "      <td>-13.483205</td>\n",
       "      <td>-14.025131</td>\n",
       "      <td>-13.090538</td>\n",
       "      <td>-13.426685</td>\n",
       "      <td>-13.231532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469938</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.911317</td>\n",
       "      <td>0.753140</td>\n",
       "      <td>-1.752795</td>\n",
       "      <td>-1.096010</td>\n",
       "      <td>1.310088</td>\n",
       "      <td>0.788032</td>\n",
       "      <td>-0.760948</td>\n",
       "      <td>-0.698112</td>\n",
       "      <td>...</td>\n",
       "      <td>3.261356</td>\n",
       "      <td>5.430039</td>\n",
       "      <td>5.534235</td>\n",
       "      <td>4.937364</td>\n",
       "      <td>-15.544859</td>\n",
       "      <td>-15.852084</td>\n",
       "      <td>-15.213339</td>\n",
       "      <td>-16.537561</td>\n",
       "      <td>-15.398089</td>\n",
       "      <td>-15.408822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469939</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.832427</td>\n",
       "      <td>-0.034935</td>\n",
       "      <td>0.448466</td>\n",
       "      <td>0.228038</td>\n",
       "      <td>1.056665</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>0.445886</td>\n",
       "      <td>0.172523</td>\n",
       "      <td>...</td>\n",
       "      <td>1.025140</td>\n",
       "      <td>3.914076</td>\n",
       "      <td>4.656905</td>\n",
       "      <td>4.280062</td>\n",
       "      <td>-12.711132</td>\n",
       "      <td>-12.018847</td>\n",
       "      <td>-11.518799</td>\n",
       "      <td>-12.658293</td>\n",
       "      <td>-12.787009</td>\n",
       "      <td>-12.214556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469940 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           weight  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0        0.000000          1  -1.872746  -2.191242  -0.474163  -0.323046   \n",
       "1       16.673515         -1  -1.349537  -1.704709   0.068058   0.028432   \n",
       "2        0.000000         -1   0.812780  -0.256156   0.806463   0.400221   \n",
       "3        0.000000         -1   1.174378   0.344640   0.066872   0.009357   \n",
       "4        0.138531          1  -3.172026  -3.093182  -0.161518  -0.128149   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "469935   0.000000          1  -1.953012  -1.825455   1.167282   1.407730   \n",
       "469936   0.000000         -1  -1.093580  -0.308872  -1.175020  -1.802869   \n",
       "469937   0.000000         -1  -1.140352  -0.681436  -0.155998  -0.173413   \n",
       "469938   0.000000         -1   1.911317   0.753140  -1.752795  -1.096010   \n",
       "469939   0.000000         -1   0.832427  -0.034935   0.448466   0.228038   \n",
       "\n",
       "        feature_5  feature_6  feature_7  feature_8  ...         0         1  \\\n",
       "0        0.014688  -0.002484   0.051777   0.026828  ...  1.729607  3.403493   \n",
       "1        0.193794   0.138212   0.051777   0.026828  ...  1.214633  4.076755   \n",
       "2       -0.614188  -0.354800   0.051777   0.026828  ...  1.557886  2.472301   \n",
       "3       -1.006373  -0.676458   0.051777   0.026828  ...  2.467654  2.517851   \n",
       "4       -0.195006  -0.143780   0.051777   0.026828  ...  1.082304  5.234684   \n",
       "...           ...        ...        ...        ...  ...       ...       ...   \n",
       "469935   0.604182   0.745740   0.533140   0.860828  ...  2.249876  4.889501   \n",
       "469936  -0.266547  -0.411329  -0.622232  -0.924464  ...  2.837904  5.756587   \n",
       "469937   1.272839   1.825249   0.102961   0.315304  ...  2.105800  4.126677   \n",
       "469938   1.310088   0.788032  -0.760948  -0.698112  ...  3.261356  5.430039   \n",
       "469939   1.056665   0.671095   0.445886   0.172523  ...  1.025140  3.914076   \n",
       "\n",
       "               2          3          4          5          6          7  \\\n",
       "0       4.486810   5.528064 -16.217875 -16.603025 -16.146721 -15.583063   \n",
       "1       4.531205   4.637632 -11.385442 -11.425260 -11.386423 -11.858445   \n",
       "2       2.713197   3.681982  -9.799013 -10.127969  -9.970935  -9.792596   \n",
       "3       2.537681   2.423693  -7.305463  -7.262433  -7.371759  -7.498514   \n",
       "4       5.561175   6.529605 -14.687175 -14.488884 -13.920571 -14.798549   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "469935  5.061232   6.782163 -20.332289 -22.976454 -20.284576 -22.883825   \n",
       "469936  1.978915  10.882157 -30.849245 -31.941072 -32.235985 -36.884075   \n",
       "469937  2.191408   2.860481 -13.266150 -13.483205 -14.025131 -13.090538   \n",
       "469938  5.534235   4.937364 -15.544859 -15.852084 -15.213339 -16.537561   \n",
       "469939  4.656905   4.280062 -12.711132 -12.018847 -11.518799 -12.658293   \n",
       "\n",
       "                8          9  \n",
       "0      -15.994396 -16.221869  \n",
       "1      -11.746490 -11.593060  \n",
       "2      -10.142966 -10.175447  \n",
       "3       -7.515182  -7.674811  \n",
       "4      -14.468530 -14.522084  \n",
       "...           ...        ...  \n",
       "469935 -21.512280 -22.170584  \n",
       "469936 -35.956364 -35.413586  \n",
       "469937 -13.426685 -13.231532  \n",
       "469938 -15.398089 -15.408822  \n",
       "469939 -12.787009 -12.214556  \n",
       "\n",
       "[469940 rows x 141 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 9ms/step - loss: 0.5769\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0889\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0196\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0046\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 0.0014\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 8.3713e-04\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.5327e-04\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4413e-04\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4432e-04\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4705e-04\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.3737e-04\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 7.4911e-04\n",
      "14686/14686 [==============================] - 25s 2ms/step - loss: 6.5726e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0006572648417204618"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "loss_fn = 'mse'\n",
    "model = resp_MLP()\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "modelcheckpoint  = tf.keras.callbacks.ModelCheckpoint(js_path+'models/resp_model.h5', \n",
    "                                                      monitor='loss', mode='auto', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn)\n",
    "model.fit(x_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1, callbacks= [earlystopping])\n",
    "#model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255.8159205073855, 4201.563320706824, -3945.747400199439]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_test.values).numpy()\n",
    "\n",
    "pred_action = (preds > 0) * 1\n",
    "\n",
    "score_func.u2(test_weight, test_resp,np.array(pred_action).reshape(len(pred_action),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/193 [==============================] - 2s 7ms/step - loss: 0.6999 - auc: 0.5000 - accuracy: 0.5030\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 1s 7ms/step - loss: 0.6932 - auc: 0.5000 - accuracy: 0.5031\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 1s 7ms/step - loss: 0.6931 - auc: 0.5000 - accuracy: 0.5034\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 1s 7ms/step - loss: 0.6932 - auc: 0.5000 - accuracy: 0.5026\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 1s 7ms/step - loss: 0.6932 - auc: 0.5000 - accuracy: 0.5027\n",
      "14686/14686 [==============================] - 43s 3ms/step - loss: 0.6932 - auc: 0.5000 - accuracy: 0.5084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6931966543197632, 0.5, 0.5084436535835266]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = 300\n",
    "dropout_rates = 0.2\n",
    "num_labels = int(len(np.unique(y_train_binary))) - 1\n",
    "\n",
    "inp = tf.keras.layers.Input(shape=(131))\n",
    "\n",
    "for _ in range(3):\n",
    "    x = tf.keras.layers.Dense(hidden_units)(inp)\n",
    "    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates)(x)\n",
    "\n",
    "x= tf.keras.layers.Dense(num_labels)(x)\n",
    "\n",
    "out = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['AUC', 'accuracy'])\n",
    "model.fit(x_train, y_train_binary, batch_size= 10000, epochs=50 ,verbose  = 1 ,callbacks= [earlystopping])\n",
    "#model.evaluate(x_test, y_test_binary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vb_simple(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns))\n",
    "    \n",
    "    for _ in range(3):\n",
    "        x = tf.keras.layers.Dense(hidden_units[0], activation='relu')(inp)\n",
    "        #x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "\n",
    "\n",
    "\n",
    "    x= tf.keras.layers.Dense(num_labels)(x)\n",
    "    \n",
    "    out = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        #optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model_7/activation_31/Softmax:0) = ] [[nan][nan][nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\n\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_93]]\n  (1) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model_7/activation_31/Softmax:0) = ] [[nan][nan][nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_204268]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-61ee8a391884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mkmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model_7/activation_31/Softmax:0) = ] [[nan][nan][nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\n\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_93]]\n  (1) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model_7/activation_31/Softmax:0) = ] [[nan][nan][nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_204268]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "hidden_units = [150, 150, 150]\n",
    "dropout_rates = [0.2, 0.2, 0.2, 0.2]\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "kmodel = vol_bin_classifier(\n",
    "    len(features), 1, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kmodel.fit(x_train, y_train_binary, epochs=50, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model_2/activation_11/Sigmoid:0) = ] [[nan][nan][nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\n\t [[assert_greater_equal/Assert/AssertGuard/branch_executed/_9/_65]]\n  (1) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model_2/activation_11/Sigmoid:0) = ] [[nan][nan][nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_179696]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-3f7176d2ac7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model_2/activation_11/Sigmoid:0) = ] [[nan][nan][nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\n\t [[assert_greater_equal/Assert/AssertGuard/branch_executed/_9/_65]]\n  (1) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model_2/activation_11/Sigmoid:0) = ] [[nan][nan][nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_179696]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_mlp(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns,))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(num_labels)(x)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
    "        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "batch_size = 5000\n",
    "hidden_units = [150, 150, 150]\n",
    "dropout_rates = [0.2, 0.2, 0.2, 0.2]\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "clf = create_mlp(\n",
    "    len(features), 1, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "\n",
    "clf.fit(x_train, y_train_binary, epochs=200, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469940, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469940,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test).reshape(len(y_test),).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5587       9.821427\n",
       "5588       0.838150\n",
       "5589       0.115654\n",
       "5590       0.000000\n",
       "5591       0.000000\n",
       "             ...   \n",
       "2390486    0.000000\n",
       "2390487    0.000000\n",
       "2390488    0.000000\n",
       "2390489    0.283405\n",
       "2390490    0.000000\n",
       "Name: weight, Length: 1920551, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w = data[mask].loc[:,'weight']\n",
    "test_resp = data[mask].loc[:,'resp']\n",
    "test_action = (test_resp > 0) * 1\n",
    "test_size= len(test_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.872746</td>\n",
       "      <td>-2.191242</td>\n",
       "      <td>-0.474163</td>\n",
       "      <td>-0.323046</td>\n",
       "      <td>0.014688</td>\n",
       "      <td>-0.002484</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>1.168391</td>\n",
       "      <td>8.313583</td>\n",
       "      <td>1.782433</td>\n",
       "      <td>14.018213</td>\n",
       "      <td>2.653056</td>\n",
       "      <td>12.600292</td>\n",
       "      <td>2.301488</td>\n",
       "      <td>11.445807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.673515</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.349537</td>\n",
       "      <td>-1.704709</td>\n",
       "      <td>0.068058</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.193794</td>\n",
       "      <td>0.138212</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>-1.178850</td>\n",
       "      <td>1.777472</td>\n",
       "      <td>-0.915458</td>\n",
       "      <td>2.831612</td>\n",
       "      <td>-1.417010</td>\n",
       "      <td>2.297459</td>\n",
       "      <td>-1.304614</td>\n",
       "      <td>1.898684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.812780</td>\n",
       "      <td>-0.256156</td>\n",
       "      <td>0.806463</td>\n",
       "      <td>0.400221</td>\n",
       "      <td>-0.614188</td>\n",
       "      <td>-0.354800</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>6.115747</td>\n",
       "      <td>9.667908</td>\n",
       "      <td>5.542871</td>\n",
       "      <td>11.671595</td>\n",
       "      <td>7.281757</td>\n",
       "      <td>10.060014</td>\n",
       "      <td>6.638248</td>\n",
       "      <td>9.427299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.174378</td>\n",
       "      <td>0.344640</td>\n",
       "      <td>0.066872</td>\n",
       "      <td>0.009357</td>\n",
       "      <td>-1.006373</td>\n",
       "      <td>-0.676458</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>2.838853</td>\n",
       "      <td>0.499251</td>\n",
       "      <td>3.033732</td>\n",
       "      <td>1.513488</td>\n",
       "      <td>4.397532</td>\n",
       "      <td>1.266037</td>\n",
       "      <td>3.856384</td>\n",
       "      <td>1.013469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138531</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>-0.161518</td>\n",
       "      <td>-0.128149</td>\n",
       "      <td>-0.195006</td>\n",
       "      <td>-0.143780</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.344850</td>\n",
       "      <td>4.101145</td>\n",
       "      <td>0.614252</td>\n",
       "      <td>6.623456</td>\n",
       "      <td>0.800129</td>\n",
       "      <td>5.233243</td>\n",
       "      <td>0.362636</td>\n",
       "      <td>3.926633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335939</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.953012</td>\n",
       "      <td>-1.825455</td>\n",
       "      <td>1.167282</td>\n",
       "      <td>1.407730</td>\n",
       "      <td>0.604182</td>\n",
       "      <td>0.745740</td>\n",
       "      <td>0.533140</td>\n",
       "      <td>0.860828</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.821456</td>\n",
       "      <td>-2.384486</td>\n",
       "      <td>-1.125181</td>\n",
       "      <td>0.374372</td>\n",
       "      <td>-1.678955</td>\n",
       "      <td>-2.763350</td>\n",
       "      <td>-2.406755</td>\n",
       "      <td>-1.314956</td>\n",
       "      <td>-1.421907</td>\n",
       "      <td>0.109313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335940</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.093580</td>\n",
       "      <td>-0.308872</td>\n",
       "      <td>-1.175020</td>\n",
       "      <td>-1.802869</td>\n",
       "      <td>-0.266547</td>\n",
       "      <td>-0.411329</td>\n",
       "      <td>-0.622232</td>\n",
       "      <td>-0.924464</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.044894</td>\n",
       "      <td>-0.937643</td>\n",
       "      <td>-1.864301</td>\n",
       "      <td>2.394599</td>\n",
       "      <td>-2.370054</td>\n",
       "      <td>-0.312073</td>\n",
       "      <td>-3.317327</td>\n",
       "      <td>1.023902</td>\n",
       "      <td>-2.057109</td>\n",
       "      <td>2.593925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335941</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.140352</td>\n",
       "      <td>-0.681436</td>\n",
       "      <td>-0.155998</td>\n",
       "      <td>-0.173413</td>\n",
       "      <td>1.272839</td>\n",
       "      <td>1.825249</td>\n",
       "      <td>0.102961</td>\n",
       "      <td>0.315304</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.472322</td>\n",
       "      <td>-2.130481</td>\n",
       "      <td>-1.143322</td>\n",
       "      <td>-0.328897</td>\n",
       "      <td>-1.453351</td>\n",
       "      <td>-2.812128</td>\n",
       "      <td>-2.248055</td>\n",
       "      <td>-1.705126</td>\n",
       "      <td>-1.632931</td>\n",
       "      <td>-0.999637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335942</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.911317</td>\n",
       "      <td>0.753140</td>\n",
       "      <td>-1.752795</td>\n",
       "      <td>-1.096010</td>\n",
       "      <td>1.310088</td>\n",
       "      <td>0.788032</td>\n",
       "      <td>-0.760948</td>\n",
       "      <td>-0.698112</td>\n",
       "      <td>...</td>\n",
       "      <td>1.673762</td>\n",
       "      <td>-1.070076</td>\n",
       "      <td>2.878417</td>\n",
       "      <td>0.354457</td>\n",
       "      <td>1.798903</td>\n",
       "      <td>-0.709118</td>\n",
       "      <td>3.122721</td>\n",
       "      <td>-0.039724</td>\n",
       "      <td>3.135445</td>\n",
       "      <td>0.229055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335943</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.832427</td>\n",
       "      <td>-0.034935</td>\n",
       "      <td>0.448466</td>\n",
       "      <td>0.228038</td>\n",
       "      <td>1.056665</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>0.445886</td>\n",
       "      <td>0.172523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.619985</td>\n",
       "      <td>-2.173323</td>\n",
       "      <td>-0.610736</td>\n",
       "      <td>-1.429112</td>\n",
       "      <td>-0.746617</td>\n",
       "      <td>-2.959842</td>\n",
       "      <td>-1.269910</td>\n",
       "      <td>-2.238614</td>\n",
       "      <td>-1.191447</td>\n",
       "      <td>-2.057664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469940 rows Ã— 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            weight  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0         0.000000          1  -1.872746  -2.191242  -0.474163  -0.323046   \n",
       "1        16.673515         -1  -1.349537  -1.704709   0.068058   0.028432   \n",
       "2         0.000000         -1   0.812780  -0.256156   0.806463   0.400221   \n",
       "3         0.000000         -1   1.174378   0.344640   0.066872   0.009357   \n",
       "4         0.138531          1  -3.172026  -3.093182  -0.161518  -0.128149   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "2335939   0.000000          1  -1.953012  -1.825455   1.167282   1.407730   \n",
       "2335940   0.000000         -1  -1.093580  -0.308872  -1.175020  -1.802869   \n",
       "2335941   0.000000         -1  -1.140352  -0.681436  -0.155998  -0.173413   \n",
       "2335942   0.000000         -1   1.911317   0.753140  -1.752795  -1.096010   \n",
       "2335943   0.000000         -1   0.832427  -0.034935   0.448466   0.228038   \n",
       "\n",
       "         feature_5  feature_6  feature_7  feature_8  ...  feature_120  \\\n",
       "0         0.014688  -0.002484   0.051777   0.026828  ...     0.335127   \n",
       "1         0.193794   0.138212   0.051777   0.026828  ...     0.335127   \n",
       "2        -0.614188  -0.354800   0.051777   0.026828  ...     0.335127   \n",
       "3        -1.006373  -0.676458   0.051777   0.026828  ...     0.335127   \n",
       "4        -0.195006  -0.143780   0.051777   0.026828  ...     0.335127   \n",
       "...            ...        ...        ...        ...  ...          ...   \n",
       "2335939   0.604182   0.745740   0.533140   0.860828  ...    -1.821456   \n",
       "2335940  -0.266547  -0.411329  -0.622232  -0.924464  ...    -3.044894   \n",
       "2335941   1.272839   1.825249   0.102961   0.315304  ...    -1.472322   \n",
       "2335942   1.310088   0.788032  -0.760948  -0.698112  ...     1.673762   \n",
       "2335943   1.056665   0.671095   0.445886   0.172523  ...    -0.619985   \n",
       "\n",
       "         feature_121  feature_122  feature_123  feature_124  feature_125  \\\n",
       "0           0.268776     1.168391     8.313583     1.782433    14.018213   \n",
       "1           0.268776    -1.178850     1.777472    -0.915458     2.831612   \n",
       "2           0.268776     6.115747     9.667908     5.542871    11.671595   \n",
       "3           0.268776     2.838853     0.499251     3.033732     1.513488   \n",
       "4           0.268776     0.344850     4.101145     0.614252     6.623456   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "2335939    -2.384486    -1.125181     0.374372    -1.678955    -2.763350   \n",
       "2335940    -0.937643    -1.864301     2.394599    -2.370054    -0.312073   \n",
       "2335941    -2.130481    -1.143322    -0.328897    -1.453351    -2.812128   \n",
       "2335942    -1.070076     2.878417     0.354457     1.798903    -0.709118   \n",
       "2335943    -2.173323    -0.610736    -1.429112    -0.746617    -2.959842   \n",
       "\n",
       "         feature_126  feature_127  feature_128  feature_129  \n",
       "0           2.653056    12.600292     2.301488    11.445807  \n",
       "1          -1.417010     2.297459    -1.304614     1.898684  \n",
       "2           7.281757    10.060014     6.638248     9.427299  \n",
       "3           4.397532     1.266037     3.856384     1.013469  \n",
       "4           0.800129     5.233243     0.362636     3.926633  \n",
       "...              ...          ...          ...          ...  \n",
       "2335939    -2.406755    -1.314956    -1.421907     0.109313  \n",
       "2335940    -3.317327     1.023902    -2.057109     2.593925  \n",
       "2335941    -2.248055    -1.705126    -1.632931    -0.999637  \n",
       "2335942     3.122721    -0.039724     3.135445     0.229055  \n",
       "2335943    -1.269910    -2.238614    -1.191447    -2.057664  \n",
       "\n",
       "[469940 rows x 131 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[194.42339828374926]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pred_action = (preds > 0) * 1\n",
    "score_func.u(x_test['weight'],test_resp,np.array(pred_action).reshape(len(y_test),)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[194.42339828374926]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.u(test_weight, test_resp,np.array(pred_action).reshape(len(y_test),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_threshold(thd):\n",
    "    pred_action = (preds > thd) * 1\n",
    "    score =  score_func.u(test_w,test_resp,np.array(pred_action).reshape(len(test_w),)) \n",
    "    return np.array(score).astype('double') * -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "sresult = optimize.minimize_scalar(pred_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00090106])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sresult.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'score_func' from '/home/x99e/gitgood/legendary-robot/kaggle_jane_street/score_func.py'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(score_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469940,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0.,  0., ...,  0., -0.,  0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.array(test_resp) * np.array(test_w) * np.array(pred_action).reshape(len(y_test),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12979.074287617968]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.u(test_w,test_resp,test_action) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[309.93456452215366]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.u(test_w,test_resp,np.array(pred_action).reshape(len(y_test),)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008834356337043228, 0.042206384155710534, 0.7557011468372646]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.pos_score(test_w,test_resp,np.array(pred_action).reshape(len(y_test),)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-99.07358774971269]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.u(test_w,test_resp,np.random.randint(0,2,size=len(test_resp))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04021211768006526, 0.12885324404099371, 0.2043161805375402]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.neg_score(test_w,test_resp,np.random.randint(0,2,size=len(test_resp))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010026132783883944, 0.06097105191575509, 0.7955002667632561]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func.pos_score(test_w,test_resp,np.random.randint(0,2,size=len(test_resp))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,2,size=len(test_resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/193 [==============================] - 3s 9ms/step - loss: 2.3094 - accuracy: 0.1647\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1406 - accuracy: 0.1870\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1252 - accuracy: 0.1897\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1177 - accuracy: 0.1923\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1123 - accuracy: 0.1937\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1088 - accuracy: 0.1944\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1060 - accuracy: 0.1955\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1021 - accuracy: 0.1965\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.1001 - accuracy: 0.1974\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0973 - accuracy: 0.1983\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0951 - accuracy: 0.1994\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0925 - accuracy: 0.2006\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0906 - accuracy: 0.2014\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0889 - accuracy: 0.2025\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0863 - accuracy: 0.2030\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0844 - accuracy: 0.2043\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0827 - accuracy: 0.2052\n",
      "Epoch 18/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0799 - accuracy: 0.2063\n",
      "Epoch 19/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0789 - accuracy: 0.2068\n",
      "Epoch 20/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0766 - accuracy: 0.2076\n",
      "Epoch 21/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0753 - accuracy: 0.2083\n",
      "Epoch 22/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0745 - accuracy: 0.2086\n",
      "Epoch 23/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0715 - accuracy: 0.2097\n",
      "Epoch 24/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0705 - accuracy: 0.2100\n",
      "Epoch 25/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0692 - accuracy: 0.2111\n",
      "Epoch 26/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0673 - accuracy: 0.2120\n",
      "Epoch 27/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0650 - accuracy: 0.2127\n",
      "Epoch 28/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0648 - accuracy: 0.2135\n",
      "Epoch 29/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0630 - accuracy: 0.2134\n",
      "Epoch 30/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0622 - accuracy: 0.2139\n",
      "Epoch 31/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0619 - accuracy: 0.2146\n",
      "Epoch 32/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0597 - accuracy: 0.2149\n",
      "Epoch 33/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0583 - accuracy: 0.2157\n",
      "Epoch 34/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0576 - accuracy: 0.2161\n",
      "Epoch 35/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0578 - accuracy: 0.2170\n",
      "Epoch 36/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0556 - accuracy: 0.2168\n",
      "Epoch 37/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0545 - accuracy: 0.2178\n",
      "Epoch 38/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0534 - accuracy: 0.2184\n",
      "Epoch 39/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0525 - accuracy: 0.2185\n",
      "Epoch 40/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0523 - accuracy: 0.2183\n",
      "Epoch 41/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0511 - accuracy: 0.2187\n",
      "Epoch 42/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0507 - accuracy: 0.2191\n",
      "Epoch 43/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0505 - accuracy: 0.2195\n",
      "Epoch 44/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0498 - accuracy: 0.2193\n",
      "Epoch 45/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0491 - accuracy: 0.2201\n",
      "Epoch 46/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0472 - accuracy: 0.2208\n",
      "Epoch 47/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0461 - accuracy: 0.2208\n",
      "Epoch 48/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0469 - accuracy: 0.2212\n",
      "Epoch 49/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0471 - accuracy: 0.2214\n",
      "Epoch 50/50\n",
      "193/193 [==============================] - 2s 9ms/step - loss: 2.0453 - accuracy: 0.2220\n",
      "14679/14679 [==============================] - 32s 2ms/step - loss: 2.1491 - accuracy: 0.1843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.149144172668457, 0.18434061110019684]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seq_MLP():\n",
    "    hidden_units = 300\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(int(hidden_units/3), activation='relu'),\n",
    "      tf.keras.layers.BatchNormalization(),  \n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(10)\n",
    "    ])    \n",
    "    \n",
    "    return model\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model = seq_MLP()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1 )\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "189/189 [==============================] - 3s 8ms/step - loss: 23.4657\n",
      "Epoch 2/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 9.4292\n",
      "Epoch 3/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.9414\n",
      "Epoch 4/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.6872\n",
      "Epoch 5/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.5498\n",
      "Epoch 6/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.4191\n",
      "Epoch 7/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.3277\n",
      "Epoch 8/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.2679\n",
      "Epoch 9/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.2077\n",
      "Epoch 10/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.1614\n",
      "Epoch 11/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.1431\n",
      "Epoch 12/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.1040\n",
      "Epoch 13/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.0801\n",
      "Epoch 14/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.0586\n",
      "Epoch 15/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.0352\n",
      "Epoch 16/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 8.0039\n",
      "Epoch 17/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.9842\n",
      "Epoch 18/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.9579\n",
      "Epoch 19/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.9409\n",
      "Epoch 20/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.9120\n",
      "Epoch 21/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.8861\n",
      "Epoch 22/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.8603\n",
      "Epoch 23/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.8478\n",
      "Epoch 24/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.8287\n",
      "Epoch 25/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.7968\n",
      "Epoch 26/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.7647\n",
      "Epoch 27/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.7497\n",
      "Epoch 28/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.7276\n",
      "Epoch 29/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.7106\n",
      "Epoch 30/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.6949\n",
      "Epoch 31/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.6671\n",
      "Epoch 32/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.6438\n",
      "Epoch 33/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.6229\n",
      "Epoch 34/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.6157\n",
      "Epoch 35/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5973\n",
      "Epoch 36/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5827\n",
      "Epoch 37/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5633\n",
      "Epoch 38/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5486\n",
      "Epoch 39/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5347\n",
      "Epoch 40/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.5219\n",
      "Epoch 41/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.5152\n",
      "Epoch 42/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4972\n",
      "Epoch 43/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.4787\n",
      "Epoch 44/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4594\n",
      "Epoch 45/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4645\n",
      "Epoch 46/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4450\n",
      "Epoch 47/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4280\n",
      "Epoch 48/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4332\n",
      "Epoch 49/50\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 7.4114\n",
      "Epoch 50/50\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 7.4053\n",
      "15905/15905 [==============================] - 29s 2ms/step - loss: 8.8867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.88668441772461"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = seq_MLP()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "model.fit(X_train, y_train, batch_size= 10000, epochs=50 ,verbose  = 1 )\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14620/14620 [==============================] - 31s 2ms/step - loss: 1.5392 - accuracy: 0.2910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5392255783081055, 0.2909947633743286]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "pred_train = probability_model(X_train.values)\n",
    "pred_test = probability_model(X_test.values)\n",
    "\n",
    "pred_bins = [] \n",
    "for n_pred in pred_test:\n",
    "    pred_bins.append(np.argmax(n_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.295488\n",
      "Precision: 0.323100\n",
      "Recall: 0.297252\n",
      "F1 score: 0.292774\n"
     ]
    }
   ],
   "source": [
    "skl_pred_score(pred_bins,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:40:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:40:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'max_depth': 10,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3}  # the number of classes that exist in this datset\n",
    "num_round = 20  # the number of training iterations\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "#print \"Numpy array precision:\", precision_score(y_test, best_preds, average='macro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45103186727225814"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(best_preds)==y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9448906290315314"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((np.array(best_preds)-y_test)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "predictions = probability_model(X_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bins = [] \n",
    "for preds in predictions:\n",
    "    pred_bins.append(np.argmax(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = (pred_bins - data['vol_bin']) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8112875042434053"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mean() ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbm = xgb.XGBClassifier(max_depth=5, n_estimators=300, learning_rate=0.05).fit(X_train, y_train)\n",
    "#gbm_pred = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [300,130]\n",
    "dropout_rates = [0.2,0.2,0.2]\n",
    "\n",
    "num_labels = len(np.unique(y_train))\n",
    "num_labels\n",
    "i = 0\n",
    "\n",
    "test_w\n",
    "\n",
    "inp = tf.keras.layers.Input(shape=(X_train.shape[1]))\n",
    "x = tf.keras.layers.BatchNormalization()(inp)\n",
    "x = tf.keras.layers.Dropout(0.2)(x) \n",
    "\n",
    "\n",
    "x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
    "x = tf.keras.layers.Dropout(dropout_rates[i])(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(num_labels)(x)\n",
    "\n",
    "\n",
    "out = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "k_model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 131)]             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 131)               524       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 131)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 300)               39600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                3010      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 44,334\n",
      "Trainable params: 43,472\n",
      "Non-trainable params: 862\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "191/191 [==============================] - 2s 6ms/step - loss: 2.1827 - accuracy: 0.1855\n",
      "Epoch 2/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1812 - accuracy: 0.1865\n",
      "Epoch 3/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1797 - accuracy: 0.1869\n",
      "Epoch 4/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1782 - accuracy: 0.1879\n",
      "Epoch 5/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1773 - accuracy: 0.1880\n",
      "Epoch 6/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1770 - accuracy: 0.1887\n",
      "Epoch 7/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1760 - accuracy: 0.1893\n",
      "Epoch 8/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1755 - accuracy: 0.1889\n",
      "Epoch 9/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1748 - accuracy: 0.1900\n",
      "Epoch 10/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1741 - accuracy: 0.1895\n",
      "Epoch 11/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1740 - accuracy: 0.1902\n",
      "Epoch 12/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1727 - accuracy: 0.1909\n",
      "Epoch 13/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1722 - accuracy: 0.1910\n",
      "Epoch 14/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1728 - accuracy: 0.1901\n",
      "Epoch 15/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1721 - accuracy: 0.1907\n",
      "Epoch 16/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1719 - accuracy: 0.1908\n",
      "Epoch 17/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1705 - accuracy: 0.1912\n",
      "Epoch 18/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1706 - accuracy: 0.1916\n",
      "Epoch 19/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1695 - accuracy: 0.1928\n",
      "Epoch 20/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1706 - accuracy: 0.1918\n",
      "Epoch 21/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1703 - accuracy: 0.1918\n",
      "Epoch 22/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1696 - accuracy: 0.1916\n",
      "Epoch 23/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1693 - accuracy: 0.1921\n",
      "Epoch 24/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1689 - accuracy: 0.1926\n",
      "Epoch 25/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1682 - accuracy: 0.1920\n",
      "Epoch 26/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1686 - accuracy: 0.1928\n",
      "Epoch 27/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1681 - accuracy: 0.1922\n",
      "Epoch 28/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1678 - accuracy: 0.1922\n",
      "Epoch 29/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1671 - accuracy: 0.1925\n",
      "Epoch 30/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1669 - accuracy: 0.1933\n",
      "Epoch 31/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1663 - accuracy: 0.1935\n",
      "Epoch 32/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1673 - accuracy: 0.1928\n",
      "Epoch 33/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1661 - accuracy: 0.1931\n",
      "Epoch 34/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1666 - accuracy: 0.1937\n",
      "Epoch 35/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1663 - accuracy: 0.1939\n",
      "Epoch 36/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1652 - accuracy: 0.1938\n",
      "Epoch 37/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1661 - accuracy: 0.1935\n",
      "Epoch 38/50\n",
      "191/191 [==============================] - 1s 6ms/step - loss: 2.1644 - accuracy: 0.1941\n",
      "Epoch 39/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1648 - accuracy: 0.1944\n",
      "Epoch 40/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1653 - accuracy: 0.1935\n",
      "Epoch 41/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1645 - accuracy: 0.1942\n",
      "Epoch 42/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1641 - accuracy: 0.1948\n",
      "Epoch 43/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1638 - accuracy: 0.1948\n",
      "Epoch 44/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1646 - accuracy: 0.1946\n",
      "Epoch 45/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1647 - accuracy: 0.1944\n",
      "Epoch 46/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1633 - accuracy: 0.1945\n",
      "Epoch 47/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1644 - accuracy: 0.1940\n",
      "Epoch 48/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1639 - accuracy: 0.1949\n",
      "Epoch 49/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1638 - accuracy: 0.1937\n",
      "Epoch 50/50\n",
      "191/191 [==============================] - 1s 7ms/step - loss: 2.1635 - accuracy: 0.1948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f27600a0550>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_model.summary()\n",
    "\n",
    "k_model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "k_model.fit(X_train, y_train, batch_size= 10000, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_bin_classifier(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x= tf.keras.layers.Dropout(dropout_rates[0])(x) \n",
    "    #x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dense(hidden_units[0])(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x) #(x)\n",
    "    \n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "        x = tf.keras.layers.Concatenate(axis=1)([x, inp])\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(num_labels)(x)\n",
    "    \n",
    "    out = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        #optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vb_simple(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns))\n",
    "    \n",
    "    for _ in range(3):\n",
    "        x = tf.keras.layers.Dense(hidden_units[0], activation='relu')(inp)\n",
    "        #x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "\n",
    "\n",
    "\n",
    "    x= tf.keras.layers.Dense(num_labels)(x)\n",
    "    \n",
    "    out = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        #optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "num_cats = len(np.unique(y_train))\n",
    "batch_size = 5000\n",
    "hidden_units = [300, 300, 300]\n",
    "dropout_rates = [0.2, 0.2, 0.2, 0.2]\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.5179 - accuracy: 0.1255\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.2758 - accuracy: 0.1520\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.2349 - accuracy: 0.1631\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.2173 - accuracy: 0.1696\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.2069 - accuracy: 0.1742\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1981 - accuracy: 0.1784\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1913 - accuracy: 0.1819\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1857 - accuracy: 0.1851\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1800 - accuracy: 0.1879\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1747 - accuracy: 0.1907\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1710 - accuracy: 0.1927\n",
      "Epoch 12/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1668 - accuracy: 0.1946\n",
      "Epoch 13/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1631 - accuracy: 0.1966\n",
      "Epoch 14/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1578 - accuracy: 0.1987\n",
      "Epoch 15/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1549 - accuracy: 0.2003\n",
      "Epoch 16/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1518 - accuracy: 0.2020\n",
      "Epoch 17/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1493 - accuracy: 0.2027\n",
      "Epoch 18/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1467 - accuracy: 0.2039\n",
      "Epoch 19/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1444 - accuracy: 0.2056\n",
      "Epoch 20/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1418 - accuracy: 0.2071\n",
      "Epoch 21/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1396 - accuracy: 0.2075\n",
      "Epoch 22/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1366 - accuracy: 0.2091\n",
      "Epoch 23/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1353 - accuracy: 0.2098\n",
      "Epoch 24/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1326 - accuracy: 0.2108\n",
      "Epoch 25/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1310 - accuracy: 0.2109\n",
      "Epoch 26/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1295 - accuracy: 0.2121\n",
      "Epoch 27/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1279 - accuracy: 0.2131\n",
      "Epoch 28/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1260 - accuracy: 0.2136\n",
      "Epoch 29/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1252 - accuracy: 0.2142\n",
      "Epoch 30/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1232 - accuracy: 0.2149\n",
      "Epoch 31/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1227 - accuracy: 0.2152\n",
      "Epoch 32/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1213 - accuracy: 0.2160\n",
      "Epoch 33/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1186 - accuracy: 0.2173\n",
      "Epoch 34/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1180 - accuracy: 0.2176\n",
      "Epoch 35/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1177 - accuracy: 0.2179\n",
      "Epoch 36/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1147 - accuracy: 0.2191\n",
      "Epoch 37/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1150 - accuracy: 0.2194\n",
      "Epoch 38/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1144 - accuracy: 0.2192\n",
      "Epoch 39/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1133 - accuracy: 0.2196\n",
      "Epoch 40/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1120 - accuracy: 0.2200\n",
      "Epoch 41/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1109 - accuracy: 0.2203\n",
      "Epoch 42/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1107 - accuracy: 0.2209\n",
      "Epoch 43/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1087 - accuracy: 0.2212\n",
      "Epoch 44/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1087 - accuracy: 0.2219\n",
      "Epoch 45/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1080 - accuracy: 0.2226\n",
      "Epoch 46/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1074 - accuracy: 0.2222\n",
      "Epoch 47/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1062 - accuracy: 0.2228\n",
      "Epoch 48/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1055 - accuracy: 0.2233\n",
      "Epoch 49/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1043 - accuracy: 0.2232\n",
      "Epoch 50/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1044 - accuracy: 0.2236\n",
      "Epoch 51/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1033 - accuracy: 0.2245\n",
      "Epoch 52/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1028 - accuracy: 0.2247\n",
      "Epoch 53/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1024 - accuracy: 0.2245\n",
      "Epoch 54/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1014 - accuracy: 0.2255\n",
      "Epoch 55/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1004 - accuracy: 0.2256\n",
      "Epoch 56/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0992 - accuracy: 0.2258\n",
      "Epoch 57/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.1001 - accuracy: 0.2256\n",
      "Epoch 58/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0991 - accuracy: 0.2260\n",
      "Epoch 59/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0980 - accuracy: 0.2270\n",
      "Epoch 60/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0982 - accuracy: 0.2269\n",
      "Epoch 61/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0974 - accuracy: 0.2269\n",
      "Epoch 62/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0970 - accuracy: 0.2276\n",
      "Epoch 63/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0961 - accuracy: 0.2272\n",
      "Epoch 64/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0958 - accuracy: 0.2281\n",
      "Epoch 65/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0952 - accuracy: 0.2284\n",
      "Epoch 66/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0950 - accuracy: 0.2284\n",
      "Epoch 67/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0945 - accuracy: 0.2285\n",
      "Epoch 68/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0931 - accuracy: 0.2291\n",
      "Epoch 69/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0936 - accuracy: 0.2283\n",
      "Epoch 70/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0932 - accuracy: 0.2295\n",
      "Epoch 71/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0927 - accuracy: 0.2295\n",
      "Epoch 72/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0924 - accuracy: 0.2295\n",
      "Epoch 73/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0912 - accuracy: 0.2299\n",
      "Epoch 74/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0918 - accuracy: 0.2298\n",
      "Epoch 75/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0905 - accuracy: 0.2304\n",
      "Epoch 76/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0910 - accuracy: 0.2306\n",
      "Epoch 77/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0902 - accuracy: 0.2307\n",
      "Epoch 78/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0891 - accuracy: 0.2310\n",
      "Epoch 79/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0894 - accuracy: 0.2311\n",
      "Epoch 80/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0899 - accuracy: 0.2310\n",
      "Epoch 81/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0896 - accuracy: 0.2304\n",
      "Epoch 82/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0879 - accuracy: 0.2315\n",
      "Epoch 83/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0883 - accuracy: 0.2311\n",
      "Epoch 84/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0872 - accuracy: 0.2315\n",
      "Epoch 85/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0872 - accuracy: 0.2314\n",
      "Epoch 86/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0879 - accuracy: 0.2312\n",
      "Epoch 87/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0863 - accuracy: 0.2318\n",
      "Epoch 88/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0859 - accuracy: 0.2325\n",
      "Epoch 89/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0854 - accuracy: 0.2328\n",
      "Epoch 90/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0854 - accuracy: 0.2328\n",
      "Epoch 91/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0849 - accuracy: 0.2325\n",
      "Epoch 92/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0850 - accuracy: 0.2327\n",
      "Epoch 93/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0846 - accuracy: 0.2327\n",
      "Epoch 94/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0844 - accuracy: 0.2326\n",
      "Epoch 95/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0835 - accuracy: 0.2333\n",
      "Epoch 96/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0832 - accuracy: 0.2335\n",
      "Epoch 97/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0831 - accuracy: 0.2334\n",
      "Epoch 98/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0832 - accuracy: 0.2335\n",
      "Epoch 99/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0824 - accuracy: 0.2337\n",
      "Epoch 100/100\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 2.0820 - accuracy: 0.2341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f273c4f0370>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel = vb_simple(\n",
    "    inp_shape,num_cats, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "fmodel.fit(X_train, y_train, epochs=100, batch_size=20000)\n",
    "fmodel.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15261/15261 [==============================] - 27s 2ms/step - loss: 2.3678 - accuracy: 0.1339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.367835283279419, 0.13387653231620789]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "96/96 [==============================] - 5s 30ms/step - loss: 2.5185 - accuracy: 0.1269\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.2631 - accuracy: 0.1497\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.2391 - accuracy: 0.1569\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.2205 - accuracy: 0.1645\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.2012 - accuracy: 0.1742\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.1814 - accuracy: 0.1838\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.1649 - accuracy: 0.1916\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.1492 - accuracy: 0.1990\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.1348 - accuracy: 0.2050\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.1222 - accuracy: 0.2109\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.1105 - accuracy: 0.2160\n",
      "Epoch 12/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.0989 - accuracy: 0.2206\n",
      "Epoch 13/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0894 - accuracy: 0.2252\n",
      "Epoch 14/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.0786 - accuracy: 0.2298\n",
      "Epoch 15/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.0699 - accuracy: 0.2331\n",
      "Epoch 16/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.0608 - accuracy: 0.2371\n",
      "Epoch 17/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0526 - accuracy: 0.2406\n",
      "Epoch 18/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 2.0441 - accuracy: 0.2445\n",
      "Epoch 19/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0382 - accuracy: 0.2466\n",
      "Epoch 20/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0325 - accuracy: 0.2494\n",
      "Epoch 21/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0257 - accuracy: 0.2520\n",
      "Epoch 22/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.0216 - accuracy: 0.2537\n",
      "Epoch 23/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.0162 - accuracy: 0.2563\n",
      "Epoch 24/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 2.0109 - accuracy: 0.2587\n",
      "Epoch 25/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.0070 - accuracy: 0.2599\n",
      "Epoch 26/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.0036 - accuracy: 0.2615\n",
      "Epoch 27/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 2.0003 - accuracy: 0.2629\n",
      "Epoch 28/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9960 - accuracy: 0.2646\n",
      "Epoch 29/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9921 - accuracy: 0.2658\n",
      "Epoch 30/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9899 - accuracy: 0.2667\n",
      "Epoch 31/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9870 - accuracy: 0.2681\n",
      "Epoch 32/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9862 - accuracy: 0.2687\n",
      "Epoch 33/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9824 - accuracy: 0.2697\n",
      "Epoch 34/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9798 - accuracy: 0.2714\n",
      "Epoch 35/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9781 - accuracy: 0.2722\n",
      "Epoch 36/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9765 - accuracy: 0.2730\n",
      "Epoch 37/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9742 - accuracy: 0.2735\n",
      "Epoch 38/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9714 - accuracy: 0.2747\n",
      "Epoch 39/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9694 - accuracy: 0.2752\n",
      "Epoch 40/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9673 - accuracy: 0.2767\n",
      "Epoch 41/100\n",
      "96/96 [==============================] - 3s 32ms/step - loss: 1.9672 - accuracy: 0.2764\n",
      "Epoch 42/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9634 - accuracy: 0.2785\n",
      "Epoch 43/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9641 - accuracy: 0.2780\n",
      "Epoch 44/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9628 - accuracy: 0.2781\n",
      "Epoch 45/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9609 - accuracy: 0.2792\n",
      "Epoch 46/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9604 - accuracy: 0.2799\n",
      "Epoch 47/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9592 - accuracy: 0.2799\n",
      "Epoch 48/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9571 - accuracy: 0.2810\n",
      "Epoch 49/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9553 - accuracy: 0.2810\n",
      "Epoch 50/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9542 - accuracy: 0.2823\n",
      "Epoch 51/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9542 - accuracy: 0.2819\n",
      "Epoch 52/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9522 - accuracy: 0.2835\n",
      "Epoch 53/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9502 - accuracy: 0.2839\n",
      "Epoch 54/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9492 - accuracy: 0.2846\n",
      "Epoch 55/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9486 - accuracy: 0.2845\n",
      "Epoch 56/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9481 - accuracy: 0.2845\n",
      "Epoch 57/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9473 - accuracy: 0.2846\n",
      "Epoch 58/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9461 - accuracy: 0.2857\n",
      "Epoch 59/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9451 - accuracy: 0.2862\n",
      "Epoch 60/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9423 - accuracy: 0.2868\n",
      "Epoch 61/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9422 - accuracy: 0.2869\n",
      "Epoch 62/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9433 - accuracy: 0.2862\n",
      "Epoch 63/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9417 - accuracy: 0.2873\n",
      "Epoch 64/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9404 - accuracy: 0.2878\n",
      "Epoch 65/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9409 - accuracy: 0.2874\n",
      "Epoch 66/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9391 - accuracy: 0.2882\n",
      "Epoch 67/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9379 - accuracy: 0.2887\n",
      "Epoch 68/100\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 1.9371 - accuracy: 0.2887\n",
      "Epoch 69/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9368 - accuracy: 0.2895\n",
      "Epoch 70/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9371 - accuracy: 0.2893\n",
      "Epoch 71/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9364 - accuracy: 0.2893\n",
      "Epoch 72/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9343 - accuracy: 0.2905\n",
      "Epoch 73/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9343 - accuracy: 0.2904\n",
      "Epoch 74/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9338 - accuracy: 0.2901\n",
      "Epoch 75/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9327 - accuracy: 0.2912\n",
      "Epoch 76/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9332 - accuracy: 0.2904\n",
      "Epoch 77/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9324 - accuracy: 0.2916\n",
      "Epoch 78/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9306 - accuracy: 0.2923\n",
      "Epoch 79/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9291 - accuracy: 0.2930\n",
      "Epoch 80/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9300 - accuracy: 0.2914\n",
      "Epoch 81/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9289 - accuracy: 0.2923\n",
      "Epoch 82/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9275 - accuracy: 0.2928\n",
      "Epoch 83/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9277 - accuracy: 0.2928\n",
      "Epoch 84/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9277 - accuracy: 0.2929\n",
      "Epoch 85/100\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 1.9279 - accuracy: 0.2930\n",
      "Epoch 86/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9265 - accuracy: 0.2934\n",
      "Epoch 87/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9253 - accuracy: 0.2939\n",
      "Epoch 88/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9260 - accuracy: 0.2932\n",
      "Epoch 89/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9252 - accuracy: 0.2938\n",
      "Epoch 90/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9254 - accuracy: 0.2937\n",
      "Epoch 91/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9242 - accuracy: 0.2946\n",
      "Epoch 92/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9224 - accuracy: 0.2952\n",
      "Epoch 93/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9235 - accuracy: 0.2937\n",
      "Epoch 94/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9222 - accuracy: 0.2949\n",
      "Epoch 95/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9231 - accuracy: 0.2950\n",
      "Epoch 96/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9207 - accuracy: 0.2962\n",
      "Epoch 97/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9217 - accuracy: 0.2957\n",
      "Epoch 98/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9206 - accuracy: 0.2960\n",
      "Epoch 99/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9196 - accuracy: 0.2961\n",
      "Epoch 100/100\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 1.9201 - accuracy: 0.2961\n",
      "15261/15261 [==============================] - 41s 3ms/step - loss: 2.5122 - accuracy: 0.1327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5122361183166504, 0.1327277421951294]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "fmodel = vol_bin_classifier(\n",
    "    inp_shape,num_cats, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "fmodel.fit(X_train, y_train, epochs=100, batch_size=20000)\n",
    "fmodel.evaluate(X_test, y_test)\n",
    "#fmodel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1491 - accuracy: 0.1953\n",
      "Epoch 2/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1471 - accuracy: 0.1965\n",
      "Epoch 3/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1447 - accuracy: 0.1977\n",
      "Epoch 4/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1430 - accuracy: 0.1985\n",
      "Epoch 5/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1413 - accuracy: 0.1993\n",
      "Epoch 6/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1392 - accuracy: 0.2001\n",
      "Epoch 7/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1381 - accuracy: 0.2007\n",
      "Epoch 8/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1361 - accuracy: 0.2017\n",
      "Epoch 9/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1347 - accuracy: 0.2025\n",
      "Epoch 10/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1334 - accuracy: 0.2029\n",
      "Epoch 11/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1317 - accuracy: 0.2038\n",
      "Epoch 12/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1310 - accuracy: 0.2040\n",
      "Epoch 13/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1299 - accuracy: 0.2047\n",
      "Epoch 14/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1278 - accuracy: 0.2057\n",
      "Epoch 15/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1270 - accuracy: 0.2058\n",
      "Epoch 16/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1264 - accuracy: 0.2062\n",
      "Epoch 17/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1255 - accuracy: 0.2064\n",
      "Epoch 18/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1248 - accuracy: 0.2070\n",
      "Epoch 19/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1235 - accuracy: 0.2076\n",
      "Epoch 20/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1226 - accuracy: 0.2081\n",
      "Epoch 21/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1217 - accuracy: 0.2084\n",
      "Epoch 22/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1215 - accuracy: 0.2087\n",
      "Epoch 23/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1205 - accuracy: 0.2088\n",
      "Epoch 24/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1197 - accuracy: 0.2091\n",
      "Epoch 25/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1189 - accuracy: 0.2096\n",
      "Epoch 26/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1180 - accuracy: 0.2099\n",
      "Epoch 27/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1174 - accuracy: 0.2107\n",
      "Epoch 28/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1167 - accuracy: 0.2106\n",
      "Epoch 29/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1164 - accuracy: 0.2109\n",
      "Epoch 30/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1149 - accuracy: 0.2115\n",
      "Epoch 31/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1150 - accuracy: 0.2113\n",
      "Epoch 32/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1141 - accuracy: 0.2120\n",
      "Epoch 33/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1136 - accuracy: 0.2121\n",
      "Epoch 34/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1133 - accuracy: 0.2121\n",
      "Epoch 35/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1127 - accuracy: 0.2122\n",
      "Epoch 36/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1121 - accuracy: 0.2127\n",
      "Epoch 37/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1115 - accuracy: 0.2127\n",
      "Epoch 38/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1111 - accuracy: 0.2132\n",
      "Epoch 39/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1103 - accuracy: 0.2135\n",
      "Epoch 40/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1097 - accuracy: 0.2137\n",
      "Epoch 41/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1099 - accuracy: 0.2136\n",
      "Epoch 42/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1088 - accuracy: 0.2142\n",
      "Epoch 43/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1094 - accuracy: 0.2138\n",
      "Epoch 44/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1084 - accuracy: 0.2144\n",
      "Epoch 45/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1078 - accuracy: 0.2144\n",
      "Epoch 46/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.1076 - accuracy: 0.2149\n",
      "Epoch 47/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1077 - accuracy: 0.2146\n",
      "Epoch 48/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1062 - accuracy: 0.2153\n",
      "Epoch 49/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1067 - accuracy: 0.2153\n",
      "Epoch 50/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1061 - accuracy: 0.2152\n",
      "Epoch 51/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1055 - accuracy: 0.2158\n",
      "Epoch 52/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1052 - accuracy: 0.2155\n",
      "Epoch 53/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1050 - accuracy: 0.2160\n",
      "Epoch 54/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.1046 - accuracy: 0.2162\n",
      "Epoch 55/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1042 - accuracy: 0.2161\n",
      "Epoch 56/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1039 - accuracy: 0.2162\n",
      "Epoch 57/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1032 - accuracy: 0.2167\n",
      "Epoch 58/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1029 - accuracy: 0.2164\n",
      "Epoch 59/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1022 - accuracy: 0.2171\n",
      "Epoch 60/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1021 - accuracy: 0.2173\n",
      "Epoch 61/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1022 - accuracy: 0.2175\n",
      "Epoch 62/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1017 - accuracy: 0.2173\n",
      "Epoch 63/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.1012 - accuracy: 0.2174\n",
      "Epoch 64/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1011 - accuracy: 0.2175\n",
      "Epoch 65/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1004 - accuracy: 0.2183\n",
      "Epoch 66/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1005 - accuracy: 0.2178\n",
      "Epoch 67/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.1000 - accuracy: 0.2181\n",
      "Epoch 68/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0996 - accuracy: 0.2180\n",
      "Epoch 69/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0994 - accuracy: 0.2182\n",
      "Epoch 70/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0993 - accuracy: 0.2186\n",
      "Epoch 71/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0987 - accuracy: 0.2188\n",
      "Epoch 72/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0991 - accuracy: 0.2186\n",
      "Epoch 73/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0985 - accuracy: 0.2189\n",
      "Epoch 74/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0979 - accuracy: 0.2193\n",
      "Epoch 75/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0979 - accuracy: 0.2189\n",
      "Epoch 76/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0975 - accuracy: 0.2191\n",
      "Epoch 77/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0974 - accuracy: 0.2194\n",
      "Epoch 78/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0972 - accuracy: 0.2194\n",
      "Epoch 79/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0969 - accuracy: 0.2191\n",
      "Epoch 80/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0962 - accuracy: 0.2199\n",
      "Epoch 81/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0960 - accuracy: 0.2200\n",
      "Epoch 82/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0960 - accuracy: 0.2197\n",
      "Epoch 83/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0954 - accuracy: 0.2200\n",
      "Epoch 84/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0948 - accuracy: 0.2204\n",
      "Epoch 85/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0953 - accuracy: 0.2202\n",
      "Epoch 86/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0952 - accuracy: 0.2203\n",
      "Epoch 87/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0950 - accuracy: 0.2206\n",
      "Epoch 88/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0944 - accuracy: 0.2206\n",
      "Epoch 89/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0950 - accuracy: 0.2202\n",
      "Epoch 90/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0939 - accuracy: 0.2210\n",
      "Epoch 91/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0937 - accuracy: 0.2207\n",
      "Epoch 92/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0937 - accuracy: 0.2206\n",
      "Epoch 93/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0938 - accuracy: 0.2209\n",
      "Epoch 94/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0930 - accuracy: 0.2211\n",
      "Epoch 95/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0926 - accuracy: 0.2215\n",
      "Epoch 96/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0925 - accuracy: 0.2215\n",
      "Epoch 97/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0925 - accuracy: 0.2212\n",
      "Epoch 98/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0920 - accuracy: 0.2216\n",
      "Epoch 99/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0919 - accuracy: 0.2216\n",
      "Epoch 100/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0921 - accuracy: 0.2213\n",
      "Epoch 101/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0921 - accuracy: 0.2213\n",
      "Epoch 102/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0909 - accuracy: 0.2218\n",
      "Epoch 103/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0911 - accuracy: 0.2220\n",
      "Epoch 104/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0912 - accuracy: 0.2221\n",
      "Epoch 105/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0909 - accuracy: 0.2224\n",
      "Epoch 106/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0907 - accuracy: 0.2223\n",
      "Epoch 107/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0903 - accuracy: 0.2224\n",
      "Epoch 108/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0899 - accuracy: 0.2225\n",
      "Epoch 109/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0900 - accuracy: 0.2226\n",
      "Epoch 110/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0897 - accuracy: 0.2227\n",
      "Epoch 111/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0893 - accuracy: 0.2226\n",
      "Epoch 112/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0896 - accuracy: 0.2228\n",
      "Epoch 113/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0891 - accuracy: 0.2231\n",
      "Epoch 114/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0883 - accuracy: 0.2233\n",
      "Epoch 115/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0892 - accuracy: 0.2228\n",
      "Epoch 116/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0888 - accuracy: 0.2238\n",
      "Epoch 117/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0884 - accuracy: 0.2233\n",
      "Epoch 118/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0880 - accuracy: 0.2234\n",
      "Epoch 119/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0878 - accuracy: 0.2236\n",
      "Epoch 120/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0879 - accuracy: 0.2231\n",
      "Epoch 121/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0873 - accuracy: 0.2235\n",
      "Epoch 122/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0877 - accuracy: 0.2236\n",
      "Epoch 123/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0876 - accuracy: 0.2238\n",
      "Epoch 124/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0874 - accuracy: 0.2235\n",
      "Epoch 125/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0869 - accuracy: 0.2240\n",
      "Epoch 126/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0868 - accuracy: 0.2239\n",
      "Epoch 127/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0867 - accuracy: 0.2238\n",
      "Epoch 128/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0866 - accuracy: 0.2240\n",
      "Epoch 129/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0867 - accuracy: 0.2241\n",
      "Epoch 130/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0866 - accuracy: 0.2241\n",
      "Epoch 131/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0867 - accuracy: 0.2238\n",
      "Epoch 132/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0860 - accuracy: 0.2243\n",
      "Epoch 133/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0859 - accuracy: 0.2244\n",
      "Epoch 134/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0851 - accuracy: 0.2246\n",
      "Epoch 135/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0858 - accuracy: 0.2240\n",
      "Epoch 136/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0849 - accuracy: 0.2247\n",
      "Epoch 137/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0849 - accuracy: 0.2249\n",
      "Epoch 138/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0851 - accuracy: 0.2249\n",
      "Epoch 139/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0847 - accuracy: 0.2249\n",
      "Epoch 140/200\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 2.0847 - accuracy: 0.2250\n",
      "Epoch 141/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0849 - accuracy: 0.2249\n",
      "Epoch 142/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0847 - accuracy: 0.2250\n",
      "Epoch 143/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0839 - accuracy: 0.2257\n",
      "Epoch 144/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0848 - accuracy: 0.2248\n",
      "Epoch 145/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0841 - accuracy: 0.2255\n",
      "Epoch 146/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0840 - accuracy: 0.2250\n",
      "Epoch 147/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0836 - accuracy: 0.2253\n",
      "Epoch 148/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0836 - accuracy: 0.2251\n",
      "Epoch 149/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0839 - accuracy: 0.2252\n",
      "Epoch 150/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0836 - accuracy: 0.2252\n",
      "Epoch 151/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0831 - accuracy: 0.2256\n",
      "Epoch 152/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0835 - accuracy: 0.2255\n",
      "Epoch 153/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0828 - accuracy: 0.2255\n",
      "Epoch 154/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0832 - accuracy: 0.2255\n",
      "Epoch 155/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0828 - accuracy: 0.2257\n",
      "Epoch 156/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0826 - accuracy: 0.2258\n",
      "Epoch 157/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0827 - accuracy: 0.2255\n",
      "Epoch 158/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0825 - accuracy: 0.2259\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0821 - accuracy: 0.2257\n",
      "Epoch 160/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0820 - accuracy: 0.2262\n",
      "Epoch 161/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0823 - accuracy: 0.2260\n",
      "Epoch 162/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0818 - accuracy: 0.2262\n",
      "Epoch 163/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0819 - accuracy: 0.2260\n",
      "Epoch 164/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0818 - accuracy: 0.2261\n",
      "Epoch 165/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0817 - accuracy: 0.2262\n",
      "Epoch 166/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0812 - accuracy: 0.2265\n",
      "Epoch 167/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0811 - accuracy: 0.2267\n",
      "Epoch 168/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0812 - accuracy: 0.2265\n",
      "Epoch 169/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0812 - accuracy: 0.2265\n",
      "Epoch 170/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0810 - accuracy: 0.2264\n",
      "Epoch 171/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0808 - accuracy: 0.2265\n",
      "Epoch 172/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0809 - accuracy: 0.2265\n",
      "Epoch 173/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0809 - accuracy: 0.2267\n",
      "Epoch 174/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0805 - accuracy: 0.2265\n",
      "Epoch 175/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0805 - accuracy: 0.2271\n",
      "Epoch 176/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0800 - accuracy: 0.2268\n",
      "Epoch 177/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0805 - accuracy: 0.2267\n",
      "Epoch 178/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0800 - accuracy: 0.2268\n",
      "Epoch 179/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0803 - accuracy: 0.2267\n",
      "Epoch 180/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0796 - accuracy: 0.2271\n",
      "Epoch 181/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0800 - accuracy: 0.2266\n",
      "Epoch 182/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0796 - accuracy: 0.2270\n",
      "Epoch 183/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0798 - accuracy: 0.2271\n",
      "Epoch 184/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0796 - accuracy: 0.2270\n",
      "Epoch 185/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0794 - accuracy: 0.2274\n",
      "Epoch 186/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0787 - accuracy: 0.2277\n",
      "Epoch 187/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0792 - accuracy: 0.2275\n",
      "Epoch 188/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0789 - accuracy: 0.2273\n",
      "Epoch 189/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0792 - accuracy: 0.2273\n",
      "Epoch 190/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0786 - accuracy: 0.2277\n",
      "Epoch 191/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0786 - accuracy: 0.2272\n",
      "Epoch 192/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0788 - accuracy: 0.2275\n",
      "Epoch 193/200\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 2.0785 - accuracy: 0.2273\n",
      "Epoch 194/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0784 - accuracy: 0.2278\n",
      "Epoch 195/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0782 - accuracy: 0.2276\n",
      "Epoch 196/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0781 - accuracy: 0.2279\n",
      "Epoch 197/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0782 - accuracy: 0.2276\n",
      "Epoch 198/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0776 - accuracy: 0.2280\n",
      "Epoch 199/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0781 - accuracy: 0.2277\n",
      "Epoch 200/200\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 2.0783 - accuracy: 0.2281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f70cc6f13a0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel.fit(X_train, y_train, epochs=20, batch_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from time import time\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.0000 seconds\n"
     ]
    }
   ],
   "source": [
    "import timer\n",
    "t = timer.Timer()\n",
    "t.start()\n",
    "\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimerError",
     "evalue": "Timer is not running. Use .start() to start it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimerError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-83ca768b201b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/gitgood/legendary-robot/kaggle_jane_street/timer.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Timer is running. Use .stop() to stop it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimerError\u001b[0m: Timer is not running. Use .start() to start it"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:02:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Finished training model\n",
      "Elapsed time: 100.4053 seconds\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "NAN_VALUE = -9999\n",
    "#features = [c for c in data.columns if 'feature' in c]\n",
    "#target = 'vol_bin'\n",
    "\n",
    "# Split into features X and target Y\n",
    "#X = data.loc[:, features].fillna(NAN_VALUE)\n",
    "#Y = (data.loc[:, target] > 0).astype(int)\n",
    "\n",
    "# Clear memory\n",
    "#del data\n",
    "#gc.collect()\n",
    "\n",
    "# Train model\n",
    "t.start()\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators = 500,\n",
    "    tree_method='gpu_hist',\n",
    "    nthread=multiprocessing.cpu_count()-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "print('Finished training model')\n",
    "\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softprob',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'gpu_id': 0,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': '',\n",
       " 'learning_rate': 0.300000012,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': '()',\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 23,\n",
       " 'num_parallel_tree': 1,\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': 1,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'validate_parameters': 1,\n",
       " 'verbosity': None,\n",
       " 'nthread': 23}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22440      1\n",
       "22441      1\n",
       "22442      1\n",
       "22443      1\n",
       "22444      1\n",
       "          ..\n",
       "2390486    2\n",
       "2390487    2\n",
       "2390488    2\n",
       "2390489    2\n",
       "2390490    2\n",
       "Name: vol_bin, Length: 479681, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    " # accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_pred, y_test, average = 'macro')\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_pred, y_test,average = 'macro')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_pred, y_test,average = 'macro')\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "def skl_pred_score(y_pred, y_test):\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_pred, y_test, average = 'macro')\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_pred, y_test,average = 'macro')\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_pred, y_test,average = 'macro')\n",
    "    print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.318647\n",
      "Precision: 0.327637\n",
      "Recall: 0.312099\n",
      "F1 score: 0.312950\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_pred, y_test, average = 'macro')\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_pred, y_test,average = 'macro')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_pred, y_test,average = 'macro')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skl_pred_score(y_pred, y_test):\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_pred, y_test, average = 'macro')\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_pred, y_test,average = 'macro')\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_pred, y_test,average = 'macro')\n",
    "    print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.1406872 , -5.133367  ,  0.33794835, -1.5648116 , -1.4330602 ,\n",
       "        -1.9282924 ,  0.29217353,  1.996077  ,  1.6452407 ,  1.560955  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "predictions = probability_model(X_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = probability_model(X_train[0:2].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([2.0905882e-03, 2.8500901e-04, 6.7767411e-02, 1.0107967e-02,\n",
       "       1.1531389e-02, 7.0275711e-03, 6.4735338e-02, 3.5574403e-01,\n",
       "       2.5047842e-01, 2.3023234e-01], dtype=float32)>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          7\n",
       "1          7\n",
       "2          7\n",
       "3          7\n",
       "4          7\n",
       "          ..\n",
       "2390486    6\n",
       "2390487    6\n",
       "2390488    6\n",
       "2390489    6\n",
       "2390490    6\n",
       "Name: vol_bin, Length: 2390491, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['vol_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def utility_score(df,action_vec):\n",
    "    \"\"\"Calculate utility score of a dataframe according to formulas defined at\n",
    "    https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    p = df['weight']  * df['resp'] * action_vec\n",
    "    df['p'] = p\n",
    "    p_i = df.set_index('date')['p'].groupby('date').sum()\n",
    "    t = (p_i.sum() / sqrt((p_i**2).sum())) * (sqrt(250 / p_i.index.size))\n",
    "    return min(max(t, 0), 6) * p_i.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = train[['date', 'resp','weight']]\n",
    "pred_df['actual'] = ((train['resp'].values) > 0).astype(int)\n",
    "preds = clf(X_train.values, training=False)\n",
    "pred_df['preds'] = preds.numpy()\n",
    "pred_df['action'] = (pred_df['preds'] >= 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score(pred_df, pred_df['action']) / utility_score(pred_df, pred_df['actual'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 131)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0271607 , -0.78438675,  0.7532629 ,  0.9322199 ,  0.18562594,\n",
       "         0.58452153,  0.14282605,  0.5791917 ,  0.18594626, -0.60336185]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07061819, 0.0331175 , 0.15411688, 0.18431908, 0.08736322,\n",
       "        0.13018675, 0.08370297, 0.12949471, 0.08739121, 0.0396895 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0387852"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 1.3377 - accuracy: 0.6045\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3775 - accuracy: 0.8934\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2971 - accuracy: 0.9158\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9301\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9397\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1895 - accuracy: 0.9448\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1760 - accuracy: 0.9504\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.9561\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1450 - accuracy: 0.9590\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1346 - accuracy: 0.9616\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1231 - accuracy: 0.9637\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1180 - accuracy: 0.9654\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1112 - accuracy: 0.9667\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1048 - accuracy: 0.9693\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9721\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9724\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.9725\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9752\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0817 - accuracy: 0.9753\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7137a24a00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size= 1000, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0815 - accuracy: 0.9747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08148473501205444, 0.9746999740600586]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.3066444 ,  -7.0519342 ,   0.6971853 ,   3.110398  ,\n",
       "         -9.046184  ,  -2.5873153 , -10.296194  ,  10.261124  ,\n",
       "         -2.562631  ,  -0.04820664]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "predictions = probability_model(x_test[:1])\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = probability_model(x_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390491, 131)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resLSTM(i_shape, multiplier = 1):\n",
    "    \n",
    "    inputs =  Input(shape = i_shape)\n",
    "    n_timesteps, n_features = i_shape[0], i_shape[1]\n",
    "\n",
    "    node_factor = int(n_features * multiplier)\n",
    "\n",
    "    ######### resnet LSTM model \n",
    "    stackLSTM0 = LSTM(node_factor * 10, activation='relu', input_shape = i_shape, return_sequences=True)(inputs)\n",
    "    stackLSTM0 = BatchNormalization()(stackLSTM0)\n",
    "    merge0 = concatenate([stackLSTM0, inputs])\n",
    "    \n",
    "    \n",
    "    stackLSTM1 = LSTM(node_factor * 5, activation='relu', return_sequences=True)(merge0)\n",
    "    stackLSTM1 = BatchNormalization()(stackLSTM1)\n",
    "    merge1 = concatenate([stackLSTM1, inputs])\n",
    "    \n",
    "        \n",
    "    stackLSTM2 = LSTM(node_factor * 2, activation='relu', return_sequences=True)(merge1)\n",
    "    stackLSTM2 = BatchNormalization()(stackLSTM2)\n",
    "    merge2 = concatenate([stackLSTM2, inputs])\n",
    "    \n",
    "    \n",
    "    resLSTM = LSTM(node_factor * 10, activation='relu', return_sequences=False)(merge2)\n",
    "    resLSTM = BatchNormalization()(resLSTM)\n",
    "    resLSTM = Dropout(rate = 0.3)(resLSTM)\n",
    "    # resLSTM = Flatten()(resLSTM)\n",
    "    resLSTM = Dense(node_factor * 10, activation='relu')(resLSTM)\n",
    "    resLSTM = Dropout(rate = 0.3)(resLSTM)\n",
    "    resLSTM = BatchNormalization()(resLSTM)\n",
    "    \n",
    "    \n",
    "\n",
    "    # output\n",
    "    output = Dense(node_factor * 1, activation='relu',kernel_regularizer='l1')(resLSTM)\n",
    "    # output = BatchNormalization()(output)\n",
    "    output = Dense(node_factor * 1, activation='relu')(output)\n",
    "    output = Dense(1)(output)\n",
    "    model = Model(inputs=inputs, outputs=output, name = 'resnet')\n",
    "    # summarize layers\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "    return model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle_tf] *",
   "language": "python",
   "name": "conda-env-kaggle_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
